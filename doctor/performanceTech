性能优化
	
	1.总述
			1.背景
					1.大规模（大数据、用户多等）和高要求（低延迟、高吞吐等）。这 样的特点也就要求服务的高性能和容量的高效率。
					2.程序员在开发软件时会写出高性能的代码；运维人员会懂得如何监测和提 高系统的性能；软件测试人员会通览软件测试的分类和方法；管理人员可以了解如何进行容 量管理，提升服务效率并降低运营成本
					3.性能和容量领域的工作特点是需要多方面 的知识和技能，以及实际的经验积累。这种学习和积累需要相当长的时间，不太可能一蹴而 就。
					4.帮助公司提升业务 性能和容量效率，*** 节省运营成本 ***
					5.性能分析1.png
					6.靠谱的程序 - 靠谱的人 - 不要挖坑埋雷
					7.一门艺术，它需要知识，需要经验，也需要天分

			2.指标
					服务延迟（Service latency）：客户请求的处理时间。											(快)
					吞吐量（Throughput）：单位时间处理请求的数量。												(多)
					可靠性 （Reliability）可靠性注重的是在极端情况下能不能持续处理正常的服务请求
					扩展性（Scalability）：系统在高压的情况下能不能正常处理请求。								   （好）
					资源使用效率（Resource Utilization）：单位请求处理所需要的资源量（比如 CPU，内存等）。		   （省）

			3.示例
					1.新手: 判断两个日期之间,每天进行循环判断是否满足节假日   ===> 判断两个日期间有几个星期 * 5 + 特殊处理边界问题
					2.普通： 嵌套循环 x[j][i] ==> x[i][j]		因为计算机通常都会有数量不大的缓存。数组在内存里是连续存放的，所以，如果访问数组 元素的时候能够按照顺序来，缓存可以起到极大的加速作用。
					3.资深： std::unordered_map ==> Google：：unordered_map 通过修改合适的数据结构和算法
					4.专家:  提高cpu利用率 线程池线程数等于逻辑cpu数 ==> 逻辑cpu一半 逻辑cpu底层共享了物理资源
					5.架构师：指令级别的提前获取优化。具体来讲，就是用 GCC 的 __builtin_prefetch指令来预先提取关键指令，从而降低缓存的缺失比例，也就提高了 CPU 的使用效率。

					整个的代码改进只需要几行代码的改动，真真切切是"一字万金"。 性能优化的方法和最终解决方案或许看起来很简单直白，但 是要知道在哪里做优化和做什么样的优化，却需要很多的测试和分析的工作经验
					逻辑= 清晰与精准。

			4.整体
					1.模块 - 系统 - 硬件 -机房 - 部门 - 组织结构 - 公司战略
					2.协调、配合、沟通、唇亡齿寒
					3.更进一步到设计层面来讲，从我们负责的模块和应用程序角度来看，对下层的软硬件构件越 是了解，就越有可能设计出性能优越的模块和应用程序。
					4.能科学地管理容量，准确地预 测未来需求，并逐步提升容量的效率，就能把公司这方面的成本管理和节省好


			5.技能要求
					1.知识面要广，并且软硬结合
					2.理论联系实际  性能分析的过程需要做些 实验来使真正的问题暴露出来，也就是需要进行验证，所以对动手能力的要求也比较高 , 多维度确认
					3.不但要会性能测试，还要会性能分析和性能优化

					性能分析技能树.png

					跨层交互，更会产生各种复杂的性能测试；并且实际场景是复杂的，当前的上下文，业务模型

			6.法律法规
					1.帕累托法则
							1.在很多场景下，大约 20% 的因素操控着 80% 的局面。
							2.八二原则.png
					2.阿姆达尔定律
							1.改进程序本身的串行算法可能比用多核处理器并行更有效
							2.优先加速占用时间最多的模块，因为这样可以最大限度地提升加速比
							  对一个性能优化的计划可以做出准确的效果预估和整个系统的性能预测
							同一个要素增加到一定程度后，效果就越来越不明显，所以你要拆分、分别优化使其最大化
					3.利特尔法则
							1. N=XW  从不能参数上变量要进行优化
			7.数理基础
					1.概率统计
							1.概率
								对两个事件 A 和 B 而言，“发生事件 A”在“事件 B 发生”的条件下的概率， 与“发生事件 B”在“事件 A 发生”的条件下的概率是不一样的。

							2.置信度
								对产生样本的 总体参数分布（Parametric Distribution）中的某一个未知参数值，以区间形式给出的估计。置信区间蕴含了估计精 确度的信息。
								置信区间是对分布（尤其是正态分布）的一种深入研究。通过对样本的计算，得到对某个总 体参数的区间估计，展现为总体参数的真实值有多少概率落在所计算的区间里
							3.不同指标
								平均值：（Mean，或称均值，平均数）是最常用测度值，它的目的是确定一组数据的均衡 点。
										但不足之处是它容易受极端值影响。
								中位数（Median，又称中值），将数值集合划分为相等的上下两部分，一般是把数据以升 序或降序排列后，处于最中间的数。它的优点是不受极端值的影响，但是如果数据呈现一些 特殊的分布，比如二向分布，中位数的表达会受很大的负面影响		
								四分位数
								百分位数 ： 将一组数据从小到大排序，某一百 分位所对应数据的值就称为这一百分位的百分位数，以 Pk 表示第 k 个百分位数
										   几个特殊的百分位数也很有意思，比如 P50 其实就是中位数，P0 其实就是最小值，P100 其实就是最大值。
								方差 / 标准差: 变量的离散程度，也就是该变 量离其期望值的距离。

								分布：
										1.泊松分布（Poisson distribution）适合于描述单位时间内随机事件发生的次数的概率分 布。如某一服务设施在一定时间内收到的服务请求的次数等
										2.二项分布（Binomial distribution），是 n 个独立的是 / 非试验中成功的次数的离散概率 分布。
										3.正态分布（Normal distribution），也叫高斯分布（Gaussian distribution）。经常用来 代表一个不明的随机变量。


					2.排队论
							1.网络数据发送和接收、CPU 的调度、存储 IO、数据库查询 处理等等，都是用队列来缓冲请求的，因此排队理论经常被用来做各种性能的建模分析。
							2.主要的输入参数是到达速度、顾客到达分布、排队的规则、服务机构处理速度和处理模型 等。
							  排队系统的输出也有很多的参数，比较重要的是排队长度、等待时间、系统负载水平和空闲 率等。所有这些输入、输出参数和我们进行的性能测试和优化都息息相关

				8.复杂度
							常数时间，O(1)：判断一个数字是奇数还是偶数。1.
							对数时间，O(Log(N))：你很熟悉的对排序数组的二分查找。2.
							线性时间，O(N)：对一个无序数组的搜索来查找某个值。3.
							线性对数时间，O(N Log(N))：最快的排序算法，比如希尔排序，就是这个复杂度。4.
							二次时间，O(N^2)：最直观也最慢的排序算法，比如冒泡，就是这个复杂度。5.
							指数时间，O(2^N)： 比如使用动态规划解决旅行推销员问题。这种复杂度的解决方案 一般不好

				9.性能数据展示
							1.每一种场景下，数 据展示要根据你的具体目的、听众的特点和内容的特点而采用合适的图表。然后用这些图表 做支持，把一个精美的数据分析的“故事”讲出来。
							2.挑战
									1.数据量大		 多个指标
									2.数据复杂 		 几十或几百的性能指标互相交互和影响，需要考虑它们之间的复杂关系
									3.性能问题复杂	 性能分析不是简单地死扣数据，还需要考虑互联网系统的协议，计算机 的设计，和软硬知识的结合
									4.牵扯的模块多    很多性能问题都不是孤立的，都和其他模块和子系统，甚至客户请求有 关系。

							3.如何讲好故事
									1.有啥菜吃啥饭：  要根据手头数据的特点来决定展示方法。比如你有好几组相关的数据， 有很有趣的趋势，那就要用线图来展示趋势和相关性。
									2.给足上下文：    很多工程人员容易犯的错误，就是想当然地以为别人也都了解问题的背 景，然后一上来就展示很多细节。其实除了你自己，没有第二个人更了解你要解决的问 题和要展示的数据，所以一定要给足背景介绍和上下文信息。
									3.用图讲故事：    人人喜欢听故事，而且是有趣的故事。如果你能把整个分析和推理的过 程，变成一个引人入胜和图文并茂的故事来讲，那我保证你的展示会非常地成功。
									4.和听众交互：    尽量鼓励听众参与讲故事的过程。有两种类型的数据叙事：叙述型和探索 型。叙述型是通过描述告诉观众具体结论；而探索型是鼓励观众探索数据以得出结论。 虽然探索型叙事稍微多花一点时间，但若能成功运用，听众更容易被说服；因为结论是 他们自己得出的，而不是你告诉他们的。
									5.总结重要点：    在数据展示的最后，一定要简洁明了地总结你的展示。比如你希望听众最 后只记住三句话，是哪三句呢？根据你的展示目的，这三句或是相关数字，或是趋势， 或是问题本质，或是解决方案。

							4.形式
									1.表格（Table）
									2.线图（Line Chart)
									3.PDF 和 CDF 图
										pdf: 概率密度函数（Probability Density Function）是连续型随机变量的概率密度函数 （或简称为密度函数）。就是一个描述这个随机变量的输出值，在某个确定的取值点附近的 可能性的函数。
										cdf：累积分布函数 (Cumulative Distribution Function)，又叫分布函数，是概率密度函 数 PDF 的积分。换句话说，PDF 就是 CDF 的导数。CDF 能完整描述一个实随机变量 X 的 概率分布。
									4.面积图（Area Charts）
									5.柱状图和条形图（Bar Charts）
									6.散点图（Scatter Plots）和气泡图（Bubble Charts）
									7.饼图和圆环图（Pie Charts， Donut Charts
									8.树形图（Treemaps
									9.热图（Heatmaps）

									性能数据展示.png

				10.容量设计
						1.根据业务、市场、硬件、扩展、数据中心进行总体设计

				11.情商、沟通能力


	2.设计

			1.服务重试机制 vs 指数退避机制（Exponential Backoff ）vs 熔断
						1.串联的服务模块中，上游模块必须摒弃这样雪上加霜的服务异常 通过快速地降低请求速度来帮助 下游模块恢复（上游模块对下游资源进行重试请求的时间间隔，要随着失败次数的增加而指 数加长）。

	3.实现
			1.优化原则
					1.查最大性能瓶颈  - 四个方面 / 最大性能提升 / 
					2.确诊问题根因	 - 考虑工作的投入与产出比例，即考虑成本，也考虑带来的好处 / 多方面的性能测试、假设分析并验证 / 特殊场景 边缘场景
					3.考虑多种情况	 - 不要苛求所有场景下的最优解决方案，不现实的。 / 在不同的性能指标间权衡，以找到一个最优解能达到  ‘ 总体和整体 ’ 最优
				
				综合以上因素，在实际的优化过程中，我们经常会反复权衡利弊和取舍来做最终决定

					1.过度反常态优化
								1.增加系统复杂度和维护成本，开发测试周期加长
								2.预估产品的要求并按照其进行测试
								3.类似于过度设计

					2.过早不成熟优化
								1.不要过早优化 ，但这并不代表写程序时不考虑性能，应该在性能基础上不要去扣研究，最求极致
								2.快速 “ 推出产品 ”，并小步快跑进行迭代
					3.表面的肤浅优化
								1.不要头痛医头，脚痛医脚，从 “底层”“全局”把握，达到最好的解决
								2.你发现一个应用程序的 CPU 使用率并不高，但是吞吐率上不去，表面的优化方 式可能是增大线程池来提升 CPU 使用率。
								  需不需要根据底层硬件和上层请求的变化而对线程池 的大小调优呢？
								  对这样的场景，正确的优化方式，是彻底了解线程的特性，以优化线程为主。
								  至于线程池的 大小，最好能够 “ 自动调整 " ==> 自适应。千万别动不动就手工调优。如果这样手工调整的参数多了，就会 做出一个有很多可调参数的复杂系统，类似jvm

					针锋相对的提出最好的解决方案，而这个最优解，往往是考虑各种情况之后权衡取舍决定，所以说脱离场景没有意义

			2.优化策略	== 有层次的优化性能问题		
					1.时空转换
								1.空间换时间
										1.缓存
										2.数据copy，使其离用户更近，cdn转发
										3.集群/负载均衡

								2.时间换空间
										1.改变应用程序本身的数据结构或者数据格式，减少需要存储的数据的大小；
										2.想方设法压缩存在内存中的数据，比如采用某种压缩算法，真正使用时再解压缩；
										3.把一些内存数据，存放到外部的、更加便宜的存储系统里面，到需要时再取回来。
										4.降低数据的大小来方便网络传输和外部存储

										ZStandard（ZSTD）和 LZ4。这些算法之间有 空间和时间的取舍。 
										压缩比例、压缩速度以及使用内存 - 如果系统的瓶颈在网络传输速度或者存储空间大小上，那就尽量采取高压缩比的算法

					2.并行/异步
								1.并行操作
										1.并行操作是一种物理上把一条流水线分成好几条的策略。直观上说，一个人干不完的活，那 就多找几个人来干
										2.服务器的粒度（所谓的横向扩展），还是在多线程的粒度，甚至 是在指令级别的粒度。
										3.io阻塞的地方

								2.异步操作
										1.
					3.预先/延后处理
								1.预先/提前处理
										1.页面/文件系统会对数据从磁盘额外读取，为下次上层应用读取竹内
										2.cpu/内存同样预读
										3.硬件预取/软件预取 ，后者需要插入prelocad /prefetch指令


								2.延后/惰性处理
										1.cow 写时复制 多线程间/多cpu间共享数据，只有修改时候需要复制/失效来保证一致性 ；2.并不是一开始就复制，而是写时才复制
												1.unix fork
												2.Java copyonwrite容器 并发问题 
												3.string类 写时复制
										2.同步和异步的区别在于一个函数调用之 后，是否直接返回结果。
					4.缓存/批量合并
								1.缓存数据和结果
										1.CPU、内存、文件系统、存储系统、内容分布、数据库通过缓存加速
										2. Web 的应用服务，前端会有浏览器缓存，有 CDN 存放在边缘服务器上，有 反向代理提供的静态内容缓存；后端则还会有服务器本地缓存
										3.程序设计中，对于可能重复创建和销毁，且创建销毁代价很大的对象（比如套接字和线 程），也可以缓存，对应的缓存形式，就是连接池和线程池等。
										4.对于消耗较大的计算，也可以将计算结果缓存起来，下次可以直接读取结果。比如对递归代 码的一个有效优化手段，就是缓存中间结果。
										   ===> 图搜 向量计算

								2.合并和批处理
										1.在有 IO（比如网络 IO 和磁盘 IO）的时候，合并操作和批量操作往往能提升吞吐量
										2.GFS 写文件的时候，尽量批量写，以减少 IO 开销。
										3.对数据库的读写操作，也可以尽量合并。比如，对键值数据库的查询，最好一次查询多个 键，而不要分成多次
										4.网络请求的时候，网络传输的时间可能远大于请求的处理时间，因此合并网络请求也 很有必要

					5.算法设计和数据结构
								1.更快的算法设计
										1.对每一种具体的场景（包括输入集合大小、时间空间的要求、数据的大小分布等），总会有 一种算法是最适合的。
								2.更优化的数据结构
										1.没有一个数据结构是在所有情况下都是最好的，比如你可能经常用到的 Java 里面列表的各 种实现，包括各种口味的 List、Vector、LinkedList，它们孰优孰劣，取决于很多个指标： 添加元素、删除元素、查询元素、遍历耗时等等。我们同样要权衡取舍，找出实际场合下最 适合的高效的数据结构。

			3.优化角度 	=== 系统性能核心关注点.png
					0.背景
								1.程序员所做的工作，就是把现实世界中的问题，用数据、模型来抽象，再用计算机的计算能力把问题解决掉
								2.
					1.应用
								1.
								2.
								3.
					2.基础设施	- 这些基础技术被分布式集群巨大的主机规模放大，从而带来整体上非常可观的收益。
								1.命中cpu缓存
										1.在我的 Linux 系统上，离 CPU 最近的一级缓存是32KB，二级缓存是 256KB，最大的三级缓存则是 20MB

											cat /sys/devices/system/cpu/cpu0/cache/index0/size ..
										2.1/2级缓存cpu独享，三级缓存cpu共享
										3.cpu访问内存==100个时钟周期、一级=4-5周期 二级=12周期 三级=30周期
										4.CPU 会区别对待指令与数据。比如，“1+1=2”这个运算，“+”就是指令，


										现.array[i][j]
										  array[0][0] array[0][1] array[1][0] array[1][1]
										  顺序调换后，导致 ‘ 跳跃访问 ’

										问：为什么两者的执行时间有约 7、8 倍的差距呢？、
										问：载入 array[0][0]元素时，缓存一次性会载入多少元素呢？

										答：与 CPU Cache Line 相关，它定义了缓存一次载入数据的大小
											cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size 作者64byte
											1.当载入 array[0][0]时，若它们占用的内存不足 64 字节，CPU 就会顺序地补足后续元素。顺序访问的 array[i][j]因为利用了这一特点，所以就会比 array[j][i]要快。
											2.也正因为这样，当元素类型是 4 个字节的整数时，性能就会比 8 个字节的高精度浮点数时速度更快，因为缓存一次载入的元素会更多。

										问：为什么是8倍？
										答：由于 64 位操作系统的地址占用 8 个字节。因此，每批 Cache Line 最多也就能载入不到 8 个二维数组元素，所以性能差距大约接近 8 倍。
										扩： Nginx，会发现它是用哈希表来存放域名、HTTP 头部等数据的，这样访问速度非常快，而哈希表里桶的大小如server_names_hash_bucket_size，它默认就等于 CPU Cache Line 的值 。或者整数倍
										工：perf stat 查看命中情况

										现：一个0-255随机数组，操作遍历小于128赋值为0，将数组排序。

										问:先执行哪个操作效率高？
										答：我：先赋值-后排序，减少了一半的排序操作，因为排序意味着挪动位置
										    正：先排序-后赋值，因为循环中有大量的 if 条件分支，而 CPU含有分支预测器
										    	如果分支预测器可以预测接下来要在哪段代码执行（比如 if 还是 else 中的指令），就可以提前把这些指令放在缓存中，CPU 执行时就会很快。当数组中的元素完全随机时，分支预测器无法有效工作，而当 array 数组有序时，分支预测器会动态地根据历史命中数据对未来进行预测，命中率就会非常高
										工：perf -e 查看branch-loads / branch-load-miss
										扩：底层的预判加速，动态--类似自动宏

										答：为了避免时间片切换导致的一二级缓存切换重新加载问题，线程绑定能力
										工：perf工具cpu-migrations事件，显示在不同cpu核心上切换迁移次数

										扩：从cpu读取64k这里设计变量读入，非常热的数据最好cache line对齐

							
								2.如何提升内存分配效率
										问：在 Linux 系统中，用 Xmx 设置 JVM 的最大堆内存为 8GB，但在近百个并发线程下，观察到 Java 进程占用了 14GB 的内存
										答：绝大部分高级语言都是用 C 语言编写的，包括 Java，申请内存必须经过 C 库，而 C 库通过预分配更大的空间作为内存池，来加快后续申请内存的速度。这样，预分配的6GB 的 C 库内存池就与 JVM 中预分配的 8G 内存池叠加在一起，造成了 Java 进程的内存占用超出了预期
											除了 JVM 负责管理的堆内存外，Java 还拥有一些堆外内存，由于它不使用JVM 的垃圾回收机制，所以更稳定、持久，处理 IO 的速度也更快。这些堆外内存就会由C 库内存池负责分配，这是 Java 受到 C 库内存池影响的原因。 nio 

										问：	内存问题影响
										答：垃圾回收耗时 -stw / 频繁 / 无法分配大对象给当前请求 -oom /杀死后有需要使用加载造成耗时 						

										内存申请过程：业务代码 - 应用层内存池 - c库内存池 - 操作系统内核
										主进程申请1字节的内存时，C库内存分配ptmalloc2申请132k字节内存；当释放时，不会直接释放归还操作系统，当再次申请1字节时直接复用

										问：上面是132k,为什么说java进程会被分配几个GB的内存池呢？
										答：多线程和单线程分配策略不同
											每个子线程预分配64m,64*256(cpu核数8倍)16G被ptmalloc2占用。然而，Java 中的 JVM 内存池已经管理了绝大部分内存，确实不能接受莫名多出来 6GB 的内存，那该怎么办呢？既然我们知道了 Ptmalloc2 内存池的存在，就有两种解决办法
												1.通过设置 MALLOC_ARENA_MAX 环境变量，可以限制线程内存池的最大数量，当然，线程内存池的数量减少后，会影响 Ptmalloc2 分配内存的速度。不过由于 Java 主要使用 JVM 内存池来管理对象，这点影响并不重要。
												2.跟换分配解决方案。 Google 出品的 TCMalloc 性能更好，而是在特定的场景中的选择不同。而且，盲目地选择 TCMalloc 很可能会降低性能，否则 Linux 系统早把默认的内存池改为TCMalloc 了。复制代码12# cat /proc/2891/maps | grep heap01643000-01664000 rw-p 0000000000:000     [heap]

										问：选择 Ptmalloc2 还是 TCMalloc？
											1.后者支持线程间复用
											2.前者每次分配都要加锁，解决互斥问题；后者每个线程独立分配内存，无需加锁。前者线程越多，竞争就越激烈
											3.前者更擅长分配

										论：如果主要分配 256KB 以下的内存，特别是在多线程环境下，应当选择 TCMalloc；否则应使用 Ptmalloc2，它的通用性更好。

										问：栈分配 vs 堆分配
											1.每个线程都有独立的栈，所以分配内存时不需要加锁保护
											2.栈上对象的尺寸在编译阶段就已经写入可执行文件了，执行效率更高

											1.栈内存生命周期有限，它会随着函数调用结束后自动释放。在堆中分配的内存，并不随着分配时所在函数调用的结束而释放，它的生命周期足够使用；
											2.栈的容量有限，如 CentOS 7 中是 8MB 字节，如果你申请的内存超过限制会造成栈溢出错误（比如，递归函数调用很容易造成这种问题），而堆则没有容量限制。

											所以，当我们分配内存时，如果在满足功能的情况下，可以在栈中分配的话，就选择栈。
					
											内存池中可以利用享元模式将常用的对象一直保留着，减少重复申请导致的性能的顺耗
											Java堆的内存空间是通过C库内存池申请！


									3.索引：如何用哈希表管理亿级对象？
											1.背景
												1.索引有很多，哈希表、红黑树、B 树都可以在内存中使用，如果我们需要数据规模上亿后还能提供微秒级的访问速度，那么作为最快的索引，哈希表是第一选择。为什么选择哈希表？为什么说哈希表是最快的索引呢？我们怎么定量评价索引快慢呢？实地运行程序统计时间不是个好主意，因为它不只受数据特性、数据规模的影响，而且难以跨环境比较。巴菲特说过：“近似的正确好过精确的错误。”用近似的时间复杂度描述运行时间，好过实地运行得出的精确时间。“时间复杂度”经过了详细的数学运算，它的运算过程我就不详细展开讲了。时间复杂度可以很好地反映运行时间随数据规模的变化趋势，就如下图中，横轴是数据规模，纵轴是运行时间，随着数据规模的增长，水平直线 1 不随之变化，也就是说，运行时间不变，是最好的曲线。用大 O 表示法描述时间复杂度，哈希表就是常量级的 O(1)，数据规模增长不影响它的运行时间，所以 Memcached、Redis 都在用哈希表管理数据。
												2.“近似的正确好过精确的错误。”用近似的时间复杂度描述运行时间，好过实地运行得出的精确时间。 --- 性价比
												3.1-ln -log -nlog - k - 平方 - 幂 -2的n次幂 - n阶乘  同一维度优化比，越往后收益越低，所以要 ‘ 转换维度 ’

											2.为什么hash是O(1)
												1.首先，哈希表基于数组实现，而数组可以根据下标随机访问任意元素。数组之所以可以随机访问，是因为它由连续内存承载，且每个数组元素的大小都相等。于是，当我们知道下标后，把下标乘以元素大小，再加上数组的首地址，就可以获得目标访问地址，直接获取数据。
												2.哈希函数直接把查询关键字转换为数组下标，再通过数组的随机访问特性获取数据。
												3.哈希函数的执行时间是常量，数组的随机访问也是常量，时间复杂度就是 O(1)。

												==>位图是hash变种  	常用于解决缓存穿透的问题，也常用于查找数组中的可用对象，比如下图中通过批量判断位图数组的比特位（对 CPU 缓存也很友好），找到数据数组中的对应元素。当然，logN 也是不错的曲线，随着数据规模的增长，运行时间的增长是急剧放缓的。红黑树的时间复杂度就是 O(logN)。如果需求中需要做范围查询、遍历，由于哈希表没办法找到关键字相邻的下一个元素，所以哈希表不支持这类操作，我们可以选择红黑树作为索引。采用二分法的红黑树，检索 1 万条数据需要做 14 次运算，1 亿条也只需要 27 次而已。
												todo：位图实现细节
												Bloomfilter作为bitmap的改进，牺牲了准确率换来了更强大的存储能力。

											3.树 hash - 红黑树 - b树 - b+树
												1.需求中需要做范围查询、遍历，由于哈希表没办法找到关键字相邻的下一个元素，所以哈希表不支持这类操作，我们可以选择红黑树作为索引。
												2.采用二分法的红黑树，检索 1 万条数据需要做 14 次运算，1 亿条也只需要 27 次而已。
												todo：红黑树特点
												3.如果红黑树过大，内存中放不下时，可以改用 B 树，将部分索引存放在磁盘上。磁盘访问速度要比内存慢很多，但 B 树充分考虑了机械磁盘寻址慢、顺序读写快的特点，通过多分支降低了树高，减少了磁盘读写次数。

											4.使用hash问题
												1.面对上亿条数据，为了保证可靠性，需要做灾备恢复，我们可以结合快照+oplog 方式恢复数据，但内存中的哈希表如何快速地序列化为快照文件？
												2.简单的使用标准库提供的哈希表处理如此规模的数据，会导致内存消耗过大，因为每多使用一个 8 字节的指针（或者叫引用）都会被放大亿万倍，此时该如何实现更节约内存的个性化哈希表？
												3.哈希表频繁发生冲突时，速度会急剧降低，我们该通过哪些手段减少冲突概率？(比如abc /cba) - 链表 / rehash(开放寻址法)

												解决：
												1.生产级存放大量对象的哈希表是需要容灾的，比如每隔一天把哈希表数据定期备份到另一台服务器上。通过操作日志 oplog 把 1 天内的数据载入哈希表，这样就可以最快速的恢复哈希表。所以，为了能够传输，首先必须把哈希表序列化。
												2.链接法虽然实现简单，还允许存放元素个数大于数组的大小（也叫装载因子大于 1），但链接法序列化数据的代价很大，因为使用了指针后，内存是不连续的。
												3.开放寻址法确保所有对象都在数组里，就可以把数组用到的这段连续内存原地映射到文件中
												4.如果能将数据完整的放进数组，那么开放寻址法已经解决了序列化问题，所以我们应该选择开放寻址法
												5.但是，有两个因素使得我们必须把数据放在哈希桶之外：所以，我们要把数据从哈希表中分离出来，提升哈希表的灵活性（灵活调整装载因子）。此时，该如何序列化哈希表以外的数据呢？最快速的序列化方案，还是像开放寻址法的散列表一样，使用定长数组存放对象，通过原地映射文件的方式序列化数据。由于数据未必是定长的，所以又分为两种情况。一、数据的长度是固定的。可以用另一个数组 D 存放数据，其中 D 的大小是待存放元素的最大数量，注意，D 可以远小于哈希数组的大小。如果哈希表是动态的，支持新建与删除元素操作，还需要把数组 D 中空闲的位置构建一个单链表，新建时从链表头取元素，删除时将元素归还至链表头部。每条数据有上百字节；
													1.哈希表中一定会有很多空桶（没有存放数据）。空桶的比例越高（装载因子越小），冲突概率也会但是，有两个因素使得我们必须把数据放在哈希桶之外：所以，我们要把数据从哈希表中分离出来，提升哈希表的灵活性（灵活调整装载因子）。此时，该如何序列化哈希表以外的数据呢？最快速的序列化方案，还是像开放寻址法的散列表一样，使用定长数组存放对象，通过原地映射文件的方式序列化数据。由于数据未必是定长的，所以又分为两种情况。一、数据的长度是固定的。可以用另一个数组 D 存放数据，其中 D 的大小是待存放元素的最大数量，注意，D 可以远小于哈希数组的大小。如果哈希表是动态的，支持新建与删除元素操作，还需要把数组 D 中空闲的位置构建一个单链表，新建时从链表头取元素，删除时将元素归还至链表头部。每条数据有上百字节；1.哈希表中一定会有很多空桶（没有存放数据）。空桶的比例越高（装载因子越小），冲突概率也会
												6.所以，我们要把数据从哈希表中分离出来，提升哈希表的灵活性（灵活调整装载因子）
												该如何序列化哈希表以外的数据呢？最快速的序列化方案，还是像开放寻址法的散列表一样，使用定长数组存放对象，通过原地映射文件的方式序列化数据。由于数据未必是定长的，所以又分为两种情况
												7.一、数据的长度是固定的。、长度不固定  <==== 长度固定和长度不固定.png

											5.降低hash冲突概率
												1.虽然哈希冲突有解决方案，但若是所有元素都发生了冲突，哈希表的时间复杂度就退化成了O(N)，即每查找一次都要遍历所有数据。所以，为了获得与数据规模无关的常量级时间，我们必须减少冲突的概率，而减少冲突概率有两个办法，
														1.调优哈希函数
															1.“abc”和“cba”两个关键字都落在了下标 39 上，造成了哈希冲突，是因为它丢失了字母的位置信息。BKDR 是优秀的哈希算法，但它不能以 2作为基数，这会导致字符串分布不均匀。事实上，我们应当找一个合适的素数作为基数
															2.Java标准库的 BKDR 哈希算法就以它为基数，它的计算量也很小：n*31 可以通过先把 n 左移 5位，再减去 n 的方式替换（n*31 == n<<5 - n）一次位移加一次减法，要比一次乘法快得多。
															3.基数必须是素数

															选多大的基数？
															1.当哈希函数把高信息量的关键字压缩成更小的数组下标时，一定会丢失信息。我们希望只丢失一些无关紧要的信息，尽量多地保留区分度高的信息。
															2.分析关键字的特点、分布规律。比如，对于 11 位手机号，前 3 位接入号区分度最差，中间 4 位表示地域的数字信息量有所增强，最后 4 位个人号信息量最高。如果哈希桶只有 1 万个，那么通过phonenum%10000，最大化保留后 4 位信息就是个不错的选择。(锐化 - db操作中的索引构建区分度)
															  QQ  号似乎不像手机号的数字分布那么有特点，然而，如果静态的统计存量 QQ号，就会发现最后 1 位为 0 的号码特别多（数字更讨人欢喜），区分度很低。这样，哈希函数应当主动降低最后 1 位的信息量，减少它对哈希表位置的影响。比如，QQ 号 %100就放大了最后 1 位的信息，增大了哈希冲突，而用 QQ 号 %101（101 是素数，效果更好）作为哈希函数，就降低了最后 1 位的影响


														2.扩容
															1.扩容前存放在哈希表中的所有元素，它们在扩容后的数组中位置都发生了变化。所以，扩容需要新老哈希表同时存在，通过遍历全部数据，用新的哈希函数把关键字放到合适的新哈希桶中
															2.在耗时以小时计的扩容过程中，如何持续提供正常服务呢？其实，只要把一次性的迁移过程，分为多次后台迁移，且提供服务时能够根据迁移情况选择新老哈希表即可
															3.果单机内存可以存放下新老两张哈希表，那么动态扩容不需要跨主机。反之，扩容过程将涉及新老哈希表所在的两台服务器，

										4.零拷贝：如何高效地传输文件？
											1.背景
													1.磁盘是主机中最慢的硬件之一，常常是性能瓶颈。针对磁盘的优化技术层出不穷，比如零拷贝、直接 IO、异步 IO 等等。这些优化技 术为了降低操作时延、提升系统的吞吐量，围绕着内核中的磁盘高速缓存（也叫 PageCache），去减少 CPU 和磁盘设备的工作量。
													2.
											2.现状
													1.非零拷贝传输.png
														1.假设一个320m文件，内存中分配32k缓冲区，发送1w次
														问题：
															1.经历2w次用户态与内核态的上下文切换。上下文切换的成本并不小，虽然一次切换仅消耗几十纳秒到几微秒，但高并发服务会放大这 类时间的消耗。
															2.4 万次内存拷贝，对 320MB 文件拷贝的字节数也翻了 4 倍，到了 1280MB。很显然，过多的内存拷贝无谓地消耗了 CPU 资源，降低了系统的并发处理能 力。

														解答：
															1.如果想减少上下文切换次数，就一定要减少系统调用的次数。解决方案就是把 read、write 两次系统调用合并成一次，在内核中完成磁盘与网卡的数据交换。
															2.每周期中的 4 次内存拷贝，其中与物理设备相关的 2 次拷贝是必不可少的，包括：把磁盘 内容拷贝到内存，以及把内存拷贝到网卡。但另外 2 次与用户缓冲区相关的拷贝动作都不 是必需的，因为在把磁盘文件发到网络的场景中，用户缓冲区没有必须存在的理由。
																如果内核在读取文件后，直接把 PageCache 中的内容拷贝到 Socket 缓冲区，待到网卡发 送完毕后，再通知进程，这样就只有 2 次上下文切换，和 3 次内存拷贝。
																零拷贝图示.png 
															  如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术，还可以再 去除 Socket 缓冲区的拷贝，这样一共只有 2 次内存拷贝。
																零拷贝图示2.png
															3.减少了用户内存缓冲区，降低用户内存消耗
															  零拷贝使我们不必关心 socket 缓冲区的大小(对于零拷贝图示.png中如果socket缓冲区有1.4m,则可以直接发送1.4m,而不是32k)
														    4.零拷贝使用pageCache技术
														       		1.内核态的pagecache,由于磁盘比内存的速度慢许多，所以我们应该想办法把读写磁盘替换成读写内存，比 如把磁盘中的数据复制到内存中，就可以用读内存替换读磁盘. <== 金字塔原理 越中心走资源越贵
														       				问：哪些数据在pagecache中呢？
														       					1.时间局部性   刚被访问的数据在短时间内再次被访问的概率很高
														       					2.LRU 算法     用 PageCache 缓存近访问的数据，当空间不足时淘汰久 未被访问的缓存
														       		2.预读功能	虽然 read 方法只读取了 0-32KB 的字节，但内核会把其后的 32-64KB 也读取 到 PageCache，这后 32KB 读取的成本很低。如果在 32-64KB 淘汰出 PageCache 前， 进程读取到它了，收益就非常大。这一讲的传输文件场景中这是必然发生的				
														    5.pagecache问题
														    		1.传输大文件时，  有很多 GB 级的文件需要传输，每 当用户访问这些大文件时，内核就会把它们载入到 PageCache 中，这些大文件很快会把有 限的 PageCache 占满。
																		  1.首先，由于 PageCache 长期被大文件占据，热点小文件就无法充分使用 PageCache，它们读起来变慢了
																		  2.其次，PageCache 中的大文件没有享受到缓存的好处， 但却耗费 CPU 多拷贝到 PageCache 一次
																  ==> 所以在大文件不应使用pagecache,进而不适用零拷贝技术
														2.异步io + 直接io
															1.	高并发场景处理大文件时，应当使用异步 IO 和直接 IO 来替换零拷贝技术	  
															问:实际上 read 方法会在磁盘寻址 过程中阻塞等待，导致进程无法并发地处理其他任务
															答：异步 IO（异步 IO 既可以处理网络 IO，也可以处理磁盘 IO，这里我们只关注磁盘 IO）可 以解决阻塞问题。它把读操作分为两部分，前半部分向内核发起读请求，但不等待数据就位 就立刻返回，此时进程可以并发地处理其他任务。当内核将磁盘中的数据拷贝到进程缓冲区 后，进程将接收到内核的通知，再去处理数据，这是异步 IO 的后半部分。
																原始io.png ==> 异步io+直接io.png

															2.异步 IO 并没有拷贝到 PageCache 中，对于磁盘，异步 IO 只支持直 接 IO。

															3.场景
																优点：
																	1.应用程序已经实现了磁盘文件的缓存，不 需要 PageCache 再次缓存，引发额外的性能消耗。比如 MySQL 等数据库就使用直接 IO；
																	2.高并发下传输大文件，我们上文提到过，大文件难以命中 PageCache 缓存，又 带来额外的内存拷贝，同时还挤占了小文件使用 PageCache 时需要的内存，因此，这时应 该使用直接 IO。
																缺点：
																	1.除了缓存外，内核（IO 调度算法）会试图缓存尽量多的 连续 IO 在 PageCache 中，后合并成一个更大的 IO 再发给磁盘，这样可以减少磁盘的 寻址操作；
																	2.内核也会预读后续的 IO 放在 PageCache 中，减少磁盘操作。直接 IO 绕过了 PageCache，所以无法享受这些性能提升
															4.问题：
																1.什么是大文件/小文件呢？

													2.为什么缓冲区是32kb呢?而不是32m/32byte?
															1.在没有零拷贝的情况下，我们希望内存的利用率高。如果用户缓冲区过 大，它就无法一次性把消息全拷贝给 socket 缓冲区；如果用户缓冲区过小，则会导致过多 的 read/write 系统调用。
													3.那用户缓冲区为什么不与 socket 缓冲区大小一致呢	
															1.socket 缓冲区的可用空 间是动态变化的，它既用于 TCP 滑动窗口，也用于应用缓冲区，还受到整个系统内存的影 响


										5.协程：如何快速地实现高并发服务？ 
												1.背景
													1.高并发服务 也是通过降低切换成本实现的
													2.选择一个异步框架，用非阻塞 API 把业务逻辑打乱到多个回调 函数，通过多路复用实现高并发。然而，由于业务代码过度关注并发细节，需要维护很多中 间状态，不但 Bug 率会很高，项目的开发速度也上不去，产品及时上线存在风险。
													3.解决高并发问题的技术一直在变化，从多进程、多线程，到异步化、协程，面对 不同的场景，它们都在用各自不同的方式解决问题
												2.发展
													1.多进程
															问：一颗 CPU、一块磁盘、一张网卡，如何同时服务上百个请求 呢？
															答：内核把 CPU 的执行时间切分成许多时间片 ,每个时间片再分发给不 同的进程，这就实现了请求的 并发执行。
															缺：1.内核的 管理成本高
																2.每个进程的内存空间都是独立。无法简单地通过内存同步数据，很不方便。
													2.多线程
															共享内存对象，解决了上面的两个问题
															缺：任何一个线程 出错时，进程中的所有线程会跟着一起崩溃。这也是如 Nginx 等强调稳定性的服务坚持使 用多进程模式的原因。

															问：无论基于多进程还是多线程，都难以实现高并发
																1.单个线程消耗的内存过多，比如，64 位的 Linux 为每个线程的栈分配了 8MB 的内 存，还预分配了 64MB 的内存作为堆内存池。我们没有足够的内存去开启几万个线程实现并发。
																2.切换请求是内核通过切换线程实现的，什么时候会切换线程呢？不只时间片用尽，当 调用阻塞方法时，内核为了让 CPU 充分工作，也会切换到其他线程执行。一次上下文切换 的成本在几十纳秒到几微秒间，当线程繁忙且数量众多时，这些切换会消耗绝大部分的 CPU 运算能力。
																多线程切换.png
													3.异步化		
															解：多路复用 ：  把上图中本来由内核实现的请求切换工作，交由用户态的代 码来完成就可以了，异步化编程通过应用层代码实现了请求切换，降低了切换成本和内存占 用空间。异步化依赖于 IO 多路复用机制，比如 Linux 的 epoll 或者 Windows 上的 iocp，同时，必须把阻塞方法更改为非阻塞方法，才能避免内核切换带来的巨大消耗。 Nginx、Redis 等高性能服务都依赖异步化实现了百万量级的并发。
																异步 IO 的非阻塞读和异步框架结合.png

															缺：因为所有阻塞函数，都需要通过非阻塞的系统调用拆分成 两个函数。虽然这两个函数共同完成一个功能，但调用方式却不同。
															    第一个函数由你显式调 用，
															    第二个函数则由多路复用机制调用。
															    这种方式违反了软件工程的内聚性原则，函数间同 步数据也更复杂。特别是条件分支众多、涉及大量系统调用时，异步化的改造工作会非常困 难。

												    4.协程  协程可以做到，它在异步化之上包了一层外衣，兼顾了开发效率与运行效率 / 可以在保持异步化运行机 制的同时，用同步方式写代码，这在实现高并发的同时，缩短了开发周期，是高性能服务未 来的发展方向
												    		它们必须使用非阻塞的系统调用与内核交互，把切换请求 的权力牢牢掌握在用户态的代码中。但不同的地方在于，协程把异步化中的两段函数，封装 为一个阻塞的协程函数。这个函数执行时，会使调用它的协程无感知地放弃执行权，由协程 框架切换到其他就绪的协程继续执行。当这个函数的结果满足后，协程框架再选择合适的时 机，切换回它所在的协程继续执行。
												    		协程.png
												    		问：协程不需要什么“回调函数”
												    			它允许用户调用“阻塞的”协程方 法，用同步编程方式写业务逻辑。
												    		问：协程的切换是如何完成的？
												    			内核切换流程：每个线程有独立的栈，而栈既保留了变量的值，也保留了函数的调用关系、参数和返回值， CPU 中的栈寄存器 SP 指向了当前线程的栈，而指令寄存器 IP 保存着下一条要执行的指令 地址。因此，从线程 1 切换到线程 2 时，首先要把 SP、IP 寄存器的值为线程 1 保存下 来，再从内存中找出线程 2 上一次切换前保存好的寄存器值，写入 CPU 的寄存器，这样就 完成了线程切换。（其他寄存器也需要管理、替换，原理与此相同，不再赘述。）
												    			协程切换流程：协程的切换与此相同，只是把内核的工作转移到协程框架实现而已

												    			协程切换过程.png 
															
															需要生态！！！

													5.总结：
															你会发现，协程融合了多线程与异步化编程的优点，既保证了开发效率，也提升了运 行效率。
															有限的硬件资源下，多线程通过微观上时间片的切换，实现了同时服务上百个用户的能力。 多线程的开发成本虽然低，但内存消耗大，切换次数过多，无法实现高并发。
															异步编程方式通过非阻塞系统调用和多路复用，把原本属于内核的请求切换能力，放在用户 态的代码中执行。这样，不仅减少了每个请求的内存消耗，也降低了切换请求的成本，终 实现了高并发。然而，异步编程违反了代码的内聚性，还需要业务代码关注并发细节，开发 成本很高。
															协程参考内核通过 CPU 寄存器切换线程的方法，在用户态代码中实现了协程的切换，既降 低了切换请求的成本，也使得协程中的业务代码不用关注自己何时被挂起，何时被执行。相 比异步编程中要维护一堆数据结构表示中间状态，协程直接用代码表示状态，大大提升了开 发效率。
															在协程中调用的所有 API，都需要做非阻塞的协程化改造。优秀的协程生态下，常用服务都 有对应的协程 SDK，方便业务代码使用。开发高并发服务时，与 IO 多路复用结合的协程框 架可以与这些 SDK 配合，自动挂起、切换协程，进一步提升开发效率。
															协程并不是完全与线程无关，首先线程可以帮助协程充分使用多核 CPU 的计算力，其次， 遇到无法协程化、会导致内核切换的阻塞函数，或者计算太密集从而长时间占用 CPU 的任 务，还是要放在独立的线程中执行，以防止它影响所有协程的执行。

															协程调度和内核调度相比另一个高效的原因，内核是抢占式的调度，协程是非抢占式 的、按需调度，所以协程的调度次数远远小于内核的调度次数。 


										6.锁：如何根据业务场景选择合适的锁
												1.背景
														问：怎样选择最合适的锁呢？
														答：1.我们必须清楚加锁的成本究竟有多大，
														    2.我们要分析业务场 景中访问共享资源的方式
														    3.则要预估并发访问时发生锁冲突的概率
												2.互斥锁与自旋锁：休眠还是“忙等待”？都是悲观锁,都是先锁住,在获取锁,在修改
														1.我们常见的各种锁是有层级的，最底层的两种锁就是互斥锁和自旋锁，其他锁都是基于它们 实现的。
														2.互斥锁的加锁成本更高，但它在加锁失败时会释放 CPU 给其他线程；自旋锁则刚 好相反。

														使用哪种判断依据
														1.当你无法判断锁住的代码会执行多久时，应该首选互斥锁，互斥锁是一种独占锁
														2.对于 99% 的线程级互斥锁而言，阻塞都是由操作系统内核实现的 （比如 Linux 下它通常由内核提供的信号量实现）。
														3.内核完后休眠更改和唤醒，简化了业务代码使用锁的难度

														缺：1.线程获取锁失败时，增加了两次上下文切换的成本。，或许这段时间比 锁住的代码段执行时间还长
														
														1.如果你能确定被锁住的代码执行时间很短，就应该用自旋锁取代互斥锁。通过 CPU 提供的 CAS 函数（全称 Compare And Swap），在用户态代码中完成加锁与解锁操作。
														2.CAS 函数把这 2 个步骤合并为一条硬件级指令。这样，第 1 步比较锁状态和第 2 步锁变量 赋值，将变为不可分割的原子指令 <== 票据业务的加锁解锁擦操作

														什么是自旋？忙等待
														1.	并不意味着一直执行 CAS 函数，生产级的自旋锁在“忙等待”时，会与 CPU 紧密配合 ，它通过 CPU 提供的 PAUSE 指令，减少循环等待时的耗电量；对于单核 CPU，忙等待并没有意义，此时它会主动把线程休眠。 /pause的时间，应当越来越长 。<=== 结合试错进行倒台调整策略

														当取不到锁时，互斥锁用“线程切换”来面对，自旋锁则用“忙等待”来面对。这是两种最 基本的处理方式，更高级别的锁都会选择其中一种来实现，比如读写锁就既可以基于互斥锁 实现，也可以基于自旋锁实现。

												3.读写锁
														1.一旦写锁被持有后，不只读线程必须阻塞在获取读锁的环节，其他获取写锁的写线程也要 被阻塞。写锁就像互斥锁和自旋锁一样，是一种独占锁；而读锁允许并发持有，则是一种共 享锁。
														2.读多写少的场景 , 读线程在并发持有锁，效率更高。
														3.读写锁细分为读优先和写优先
														问:读优先锁并发性更好，但问题也很明显。如果读线程源源不断地获取读锁，写线程将永远获 取不到写锁。写优先锁可以保证写线程不会饿死，但如果新的写线程源源不断地到来，读线 程也可能被饿死
														解:用队列把请求锁的线程排队，按照先来后到的顺序加锁即可，当然读线程仍然可以并发，只 不过不能插队到写线程之前。Java 中的 ReentrantReadWriteLock 读写锁，就支持这种排 队的公平读写锁。

												4.乐观锁：不使用锁也能同步
														1.它认为同时修改资源的概率很高，很容易出现冲突，所以访问共享资源 前，先加上锁，总体效率会更优。然而，如果并发产生冲突的概率很低，就不必使用悲观 锁，而是使用乐观锁。
														2.乐观:“乐观”，就是假定冲突的概率很低，所以它采用的“加锁”方式是，先修改完共享资 源，再验证这段时间内有没有发生冲突。如果没有其他线程在修改资源，那么操作完成。如 果发现其他线程已经修改了这个资源，就放弃本次操作
														
														  ===> 人生也是这样,乐观意味着更多机会,虽然可能失败错误;悲观虽然严谨,却意味着失去机会.
														
														3.乐观 == 无锁 至于放弃后如何重试，则与业务场景相关，虽然重试的成本很高，但出现冲突的概率足够低 的话，还是可以接受的。可见，乐观锁全程并没有加锁，所以它也叫无锁编程。
														4.乐观锁除了应用在 Web 分布式场景，在数据库等单机上也有广泛的应用。只是面向多线程 时，最后的验证步骤是通过 CPU 提供的 CAS 操作完成的。




					3.系统网络
					4.分布式






























	4.测试

			1.概述
					1.目的是确保软件应用程序在一定的负载流量下运行良好。性能测试是性能分析和性能优化的基础，它的目标是发现和性能相关的各种问题和性能瓶颈，从而进一步去消除错误和性能瓶颈。
			2.概念
					1.测试目的
							测量服务速度（Speed）：确定程序是否能够快速地响应用户的请求，这个服务速度一 般包括延迟和吞吐率两个指标。速度通常是应用程序最重要的属性之一，因为运行缓慢 的应用程序容易丢失用户。
							测量可扩展性（Scalability）：确定应用程序是否可以在用户负载和客户流量增大情况下 还能正常地运行。
							测量稳定性（Stability）：确定在各种极端和恶劣环境下，应用程序是否能稳定运行。3.
							测量性能瓶颈（Performance Bottleneck）：性能瓶颈是应用程序和系统中的最影响整 体性能的因素。瓶颈是指某个资源不足而导致某些负载下的性能降低。一些常见的性能 瓶颈是 CPU、内存、网络、存储等
					2.测试环境
					3.负载流量
							小流量，正常流量，还是超大流量。除了大小，负载变化的速 度也需要考虑。

					4.测试对象
							针对一个代码功能，或是整个代码模块，亦或是整个系统。

					5.负载数据
							真实数据 vs 人工模拟
					6.黑盒白盒
					7.性能测试种类 == 测试种类.png 
							负载测试
							容量测试
									：可调节的流量负载、性能的测量、可以接受的性 能指标。
							压力测试
							断点测试
									断点测试也可以用来确定系统将达到其所需规范或服务水平协议的最大容量，并且自动采取 措施来纠正或者缓解。云计算环境中，我们可以设置某种性能断点，用它们来驱动某种 扩展和伸缩策略。

							瓶颈测试
							尖峰测试
							耐力测试
									用例是暴露某些不易重现的问题，如内存问题、系统故障或其他随机 问题。

							基准测试
									确保新的代码不会对整个模块或系统的性能产生任何不好的影响。
							可扩展性测试
									测试用于确定一个程序和系统的非功能性特征能不能在变化的 环境里合理扩展。这里的环境变化包括系统环境的变化、负载量的大小、请求的多样性、数 据量的大小等。

							冒烟测试
									开发环境里执行的简单测试，以确定新的程序代码不出故
			
			3.步骤	==  性能测试步骤.png 
					1.影响性分析，知道测试的点，代码块/模块/系统/...
					2.响应时间是用户关注的 指标，吞吐量是业务关注的指标，资源利用率是系统关注的指标。

			4.工具
					1.Web 测试场景
							1.JMeter 
							2.LoadRunner
							3.Locust 

					2.系统测试场景
							1.UnixBench 
							2.Perf 
					3.数据库测试场景
							1.SysBench
							2.mysqlslap 
					4.文件 IO 和存储测试场景
							1.ioZone

					5.网络测试场景
							1.Netperf 
							2.Iperf 

					工欲善其事，必先利其器


			3.九条性能测试的经验和教训：如何保证测试结果可靠且可重复？ 
					测试可靠且重复.png 
					1.虽然每次调优建议只调整一个参数，但是这样的话可能错过： 最有搭配 
					2.分析要由易到难：
							1.首先从最常见的几种资源和几个指标查起，比如 CPU 使用率、存储 IO 繁忙度、内存大 小、网络发送和接收速度等。
							2.进一步的分析就可以针对不太明显的资源，比如内存带宽，缓存击中率，线程加锁解锁等； 
								从而过渡到应用程序和系统的一些配置参数。这些配置参数包括应用服务器及中间件，操作 系统瓶颈，数据库、WEB 服务器的配置；
							3.还有应用业务瓶颈，比如 SQL 语句、数据库设 计、业务逻辑、算法、数据等。
					3.几种测试最好互相验证
					4.与生产环境对比：
							1.网络
							2.硬件
							3.用户思考时间 停顿

			4. 性能测试的工程集成：如何与产品开发和运维业务有机集成
					1.devops - jeckins : 为了配合敏捷开发（Agile Developement）的速度和效率而产生的，是把源代码管理、代码检查、程序编译、性能 和功能测试、产品部署等一系列操作整合在一起的概念和工具。
					2.集成
						0.持续集成的价值主要有几点：
								降低代码开发风险，
								及早发现集成错误，
								减少手动测试过程， 
								快速生成测试结果，
								提高程序员和开发团队的安全感。
								频繁的提交代码，也会鼓励和 促使开发人员创建模块化和低复杂性的代码。

						1.与产品开发
							代码版本管理、代码检查、编 译连接、功能测试、性能测试、性能分析、代码覆盖率、结果展示等。有些流行的持续集成 服务器可以整合这些工具或功能，比如 Jenkins、CruiseControl、Travis 
								用 JStyle 来分析 Java 源代码
								用 Ant 构建运行 JUnit 来进行简单的功能测试
								用 DbUnit 执行长时间的数据库组件测试
								用 JUnitPerf 来进行负载和压力测试
								用 JProfiler 来进行全功能的代码性能剖析
								用 Selenium 运行基于 Web 的功能测试
								用 Cobertura 测量代码覆盖率
								用 CruiseControl 作为服务器来管理持续集成
						2.与运维业务
								智能化
								自动
								监控报警



	5.问题定位
			1.如何从大量指标中看到信号
					1.收集系统和模块的运行时间、客户访问延迟、客户滞留时间、服 务吞吐率、程序的 CPU 占用时间等
					2.需要判断它们的值到底是正常还是不正常，这就需要经验 和知识才能判断。
							db查询 ： 几百毫秒
					3.如果观测到针对某个指标的一系列性能数据，那就需要判断这个指标有没有随着时间或者其 他变量的变化而变差（Regression）和变好（Improvement
					4.对某个性能指标做预测。根据 情况做些预测分析，比如根据这个指标的历史数据来进行曲线拟合。
					5.指标之间相互影响
					6.对一个时间序列的分析
							1.性能数据的时间序列往往不是均匀平滑的，反而会有各种有规律的峰 值。需要你根据具体的情况来决定要不 要做特殊的考虑。比如节假日,秒杀、办公时点
							2.线性回归分析（Linear Regression）。线性回归是通过拟合自变量 与因变量之间最佳线性关系，来预测目标变量的方法。线性回归往往可以预测未来的数据点
							3.除了研究数据的趋势和未来预测，分类（Classification）、聚 类（Clustering）以及决策树（Decision Tree）。分类是将类别分配给数据集合，帮助更 准确地预测和分析。聚类是把相似的东西分到一组。决策树也叫分类树或回归树，每个叶节 点存放一个类别，每个非叶节点表示一个特征属性上的测试。
					7.对不同时间序列的分析
							1.数据的相关性是指数据之间存在某种关系，可以是正相关，也可以是负相关。两个数据之间 有很多种不同的相关关系

			2.数据分析的教训和陷阱
					1.数据的相关和因果关系
							1.因果关系 却不能判定
							2.容易武断地犯这样的错误，而导致走弯路。
							3.几个指标互为因果，或者构成环形因果，也就 是互相推波助澜。实际分析起来非常有挑战性，这就需要我们对整个系统和各个性能指标了 如指掌。

					2.数据的大小和趋势
							1.面对性能相关的数据并判断它们是“好”还是“坏”是很难的
							2.在很多情况下，比单纯数 字大小更重要的是数据的趋势，比如某个时期是上升还是下降，变化的幅度有多大等等
					3.数据干净与否
							1.是否需要剔除，否则会干扰结论
					
					4.性能数据内在关系的理解

			3.通常的性能标准  == 每次看到一个性能相关的数据的时候，我们立刻就能知道这个性能 数据有没有问题。 == 常用性能数值.png 

					1.存储 -- 存储性能数值.png  
							1.IO 读写延迟。一般是用 4KB 大小的 IO 做基准来测试；
							2.IO 带宽，一般是针对比较大的 IO 而言；
							3.IOPS，就是每秒钟可以读写多少个小的随机 IO

							

							太大可能有问题，太小可能都没有到达存储而是在上游。
							一般传统硬 盘的随机 IO 读写延迟是 8 毫秒的样子，IO 带宽大约 100MB 每秒，而随机 IO 读写一般就 是每秒 100 出头。


					2.CPU 和内存相关  -- cpu性能数值.png
							1. CPU 的时钟频率，也就是主频。 主频反映了 CPU 工作节拍，也就直接决定了 CPU 周期大小。4GHz， 那么每一个时钟周期（Cycle）大约0.25 纳秒（ns）。						
							2.CPU 运行程序时，最基本的执行单位是指令。而每一条指令的执行都需要经过四步：指令 获取、指令解码、指令执行、数据存入。这些操作都是按照 CPU 周期来进行的，一般需要 好几个周期。
							3.CPI 和 IPC
									CPI（cycles per instruction）衡量平均每条指令的平均时钟周期个数。它的反面是 IPC（instructions per cycle）
							4.MIPS


							1.cpu缓存
									1. L1、L2、L3，按这个顺序越来越慢，也越来越大
									2.多核 CPU 的情况下，一般 L1 和 L2 在核上，而 L3 是各个核共享的。

									

					3.操作系统和应用程序相关	
							1. 指令分支延迟	
									1.CPU 通常会采取提前 提取指令这项优化来提高性能，但是如果是指令分支，那么就可能预测错误，预先提取的指 令分支并没有被执行。

							2. 互斥加锁和解锁
									1.互斥锁 Mutex（也叫 Lock）是在多线程中用来同步的，可以保证没有两个线程同时运行在 受保护的关键区域。使用互斥锁的时候需要加锁和解锁，都是时间很昂贵的操作，每个操作 一般需要几十个时钟周期，10 纳秒以上。

							3. 上下文切换
									1.多个进程或线程共享 CPU 的时候，就需要经常做上下文切换（Context switch）。这种切 换在 CPU 时间和缓存上都很大代价；尤其是进程切换。在时间上，上下文切换可能需要几 千个时钟周期，1 微秒（1us）级别。在缓存代价上，多级 CPU 缓存和 TLB 缓存都需要恢 复，所以可能极大地降低程序线程和进程性能。


					4.网络相关	-- 网络性能数值.png
							1.大致 说每 100 公里就需要一毫秒。北京到深圳约 2,000 公里，RTT 就是 20 毫秒；上海到乌鲁 木齐或者美国的东西海岸之间距离差不多 4,000 公里，所以 RTT 是 40 毫秒左右；中国到 美国（比如北京到美国西海岸旧金山）差不多 10,000 公里，RTT 就是 100 毫秒
							2.在数据中心里面，一般的传输 RTT 不超过半毫秒。如果是同一个机柜里面的两台主机之 间，那么延迟就更小了，小于 0.1 毫秒。
							3.数据是通过骨干网光纤网络传播的。 如果光纤网络绕路的话，那么实际的 RTT 会超过以上估算数值。
							4.传输延迟也取决于传输数据的大小，因为各种网络协议都是按照数据包来 传输的，包的大小会有影响。比如一个 20KB 大小的数据，用 1Gbps 的网络传输，仅仅网 卡发送延迟就是 0.2 毫秒。
            
            4.常见的性能瓶颈和任何找到他们(性能分析思路)
            		1.常见的性能瓶颈和任何找到他们.png 
            		2.性能分析能力阶梯.png  ******************************** 整条' 链路 ' <== 普通问题分析也需要在业务模型基础上链路分析
            		3.分析过程
            				1.瓶颈的精准判断
							2.线程递增的策略
							3.性能衰减的过程
							4.响应时间的拆分
							5.构建分析决策树
							6.场景的比对

					1.领域知识,常见的坑来夯实基础


					2.从操作到对监控计数器的理解
							 <17丨CentOS：操作系统级监控及常用计数器解析（上)>
							 <18丨CentOS：操作系统级监控及常用计数器解析（下)>
							 <19丨Java&C++：代码级监控及常用计数器解析（上）>
							 <20丨Java&C++：代码级监控及常用计数器解析（下）>
							 <21丨Tomcat：中间件监控及常用计数器解析>
							 <22丨MySQL：数据库级监控及常用计数器解析（上）>
							 <23丨MySQL：数据库级监控及常用计数器解析（下）>
							 <24丨Kafka：性能监控工具之队列级监控及常用计数器解析>
							 <25丨SkyWalking：性能监控工具之链路级监控及常用计数器解析>

					3.相关性分析和证据链分析的过程
							 <06丨倾囊相授：我毕生所学的性能分析思路都在这里了>

					问题:
						 1.java中对象是动态的,如何确定那些占用
					     2.tomcat等中间件如何处理上万记的并发,哪么是新建的,哪些内容是复用的
					     3.



            5.依据数据和剖析（Profiling）来分析 而不是碰运气
            		1.依据数据和剖析分析.png 






            6.常见的性能问题
            		1.cpu
            				1.CPU 的性能决定因素
            						1.包括有多少处理器、多少个 核、时钟主频是多少、有没有 Turbo 模式、处理器内部的运算架构以及和 CPU 紧密交互 的其他部件的性能。
            				2.CPU 的内部结构
            						1.多处理器和多核、逻辑 CPU 和硬件线 程、超线程，以及 L1/L2/L3 三级缓存
            						2.现在的 CPU 普遍采用多处理器（Socket）来提高 CPU 性能，每个处理器都有自己可以直 接访问的本地内存（Local Memory）。每个处理器也都可以访问其他处理器的内存，这些内存就相当于是外地 / 远程 内存（Remote Memory）。
            						  采用多处理器和 NUMA 架构的主要原因，是提高整个 CPU 的并行处理性能。
            						3.多核结构和多级缓存
            								1.L1 和 L2 一般在核的内部,在同一个处理 器内部的核会共享同一个 L3 缓存。
            								2.除了多个核以及 L3 缓存外，处理器上一般还有非核心处理器（Uncore），里面含有和指 令运行不直接相关的组件，包括 QPI 控制器和存储器一致性监测组件

            						4.超线程
            								1.一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程 度，这一技术就叫超线程
            								2.当处理器在运行一个线程，执行指令代码时，很多时候处理器 并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多 线程来进一步“压榨”处理器。

            				==> 一台计算机有两个处理器，每个处理器有 12 个核，而且采用了 HT 超线程，那 么总的 CPU 数目就是 48，就是 2×12×2。这个数字 48，就是我们平时用监控软件和命令 看到的 CPU 的数量。比如，Linux 的 top 或者 vmstat 命令，显示的 CPU 个数就是这样 算出来的。

            				3.分析
            						1. CPU 架构的复杂性，以及和其他部件的交互，CPU 的使用率和负载的 关系往往不是线性的
            						   如果 10% 的 CPU 使用率可以每秒处理 1 千个请求，那么 80% 的 CPU 使用率 能够处理多少请求呢？不太可能处理每秒 8 千个请求，往往会远远小于这个数字
            						2. MIPS（Millions of Instructions Per Second）来衡量，表示每秒能运行多少个百万指令，MIPS 越高，性能 越高。MIPS 的计算很简单，就是时钟频率×IPC。
            						3.CPU 常见的各种中断包括软中断和硬中断 ， 上下文切换； 均衡
            								如果是 CPU 超载，那么就要分析为什么超载。多数情况下都不一定是合理的超载，
            								比如说 多核之间的负载没有平衡好，或者 CPU 干了很多没用的活，或者应用程序本身的设计需要 优化等等。
            								反之，如果是 CPU 空闲，那就需要了解为什么空闲，或许是指令里面太多内存 数据操作，从而造成 CPU 停顿，也或许是太多的分支预测错误等，这就需要具体分析和对 症下药的优化。

            						4.CPU 对多线程的执行顺序是谁定的呢
            								内核的进程调度来决定的。内核进程调度负责管理和分配 CPU 资源，合理决定哪个进 程该使用 CPU，哪个进程该等待。进程调度给不同的线程和任务分配了不同的优先级，优 先级最高的是硬件中断，其次是内核（系统）进程，最后是用户进程。每个逻辑 CPU 都维 护着一个可运行队列，用来存放可运行的线程来调度。



            		2.内存
            				1.因素
            					缓存命中率
            							1.缓存是 CPU 与内存之间的临时数据交换器
            							2.缓存的策略也用在计算机和互联网服务中很多其他的 地方，比如外部存储、文件系统，以及程序设计上。有人甚至开玩笑说，计算机的各种技术 说到底就是三种——Cache（缓存）、Hash（哈希处理）和 Trash（资源回收
            							3.cpu缓存.png

            							4.为什么要采用多级缓存，并逐级增加缓存大小呢
            								是为了提高各级缓存的命中率，从而最大限度地降低直接访问内存的概率。每 一级缓存的命中率都很重要，尤其是 L1 的命中率。

            					缓存一致性
            							1.在访问缓存和写回内存时遵循一些协议，这样的协议就叫缓存一致性协议。常见的缓存一致性协议有 MSI、MESI 


            					内存带宽内存延迟
            							1.内存带宽：单位时间内，可以并行读取或写入内存的数据量，通常以 字节 / 秒为单位表示。
            							2.实际中我们的程序使用只能达到最大带宽利用率的 60％。如果超出这个百分比，内存的访问延迟会急剧上升
            							3.带宽影响因素：
            									DRAM 时 钟频率
            									每时钟的数据传输次数
            									内存总线带宽（一般是 64 字节）
            									内存通道数量。

            					内存的使用 
            					大小及碎片
            					内存的分配
            							内存池，就是提前申请分配一定数量的、大小仔细考虑的内存块留作备用。当线程有 新的内存需求时，就从内存池中分出一部分内存块。如果已分配的内存块不够，那么可以继 续申请新的内存块。同样，线程释放的内存也暂时不返还给操作系统，而是放在内存池内留 着备用。
            							这样做的一个显著优点是尽量避免了内存碎片，使得内存分配效率和系统的总体内存使用效 率得到提升。

            					回收速度



            		3.存储
            				1.狭义上的存储往往是硬件，比如磁盘、磁带还有固态硬盘。而广义上的存储系统除了指硬件 的硬盘，还包括基于网络的存储系统，比如 SAN（Storage Area Network, 存储区域网 络）和 NAS 存储（Network Attached Storage，网络接入存储）。

            				2.三大指标
            						IOPS
            								1.Input/Output Per Second,即每秒钟能处理的读写请求数量，这是衡量存储性 能的主要指标之一。每个 IO 的请求都有自己的特性，比如读还是写，是顺序读写还是随机 读写，IO 的大小是多少等
            								2.顺序读写
            										就是访问存储设备中相邻位置的数据；随机读写呢，则是访问存储设备 中非相邻位置的数据。对随机读写进行性能衡量时，一般假定 IO 大小是 4KB
            								3.影响要素
            										IOPS 的数值会随这样的参数不同而有很大的不同，这些参数的变化，包括读取和写入的比 例、其中顺序读写及随机读写的比例、读写大小、线程数量及读写队列深度等。此外，系统 配置等因素也会影响 IOPS 的结果，例如操作系统的设置、存储设备的驱动程序特点、操作 系统后台运行的作业等。
            										IO 队列深度增加吞吐率会升高，平均访问延迟会降低。： 这是因为存储系统的 IO 队列处理机制，可以对 IO 进行重新排序，从而获 得好的性能。比如，它可以合并几个相邻的 IO，把随机 IO 重新排序为顺序 IO 

            						访问延迟
            						吞吐率 / 带宽
            								1.衡量的是存储系统的实际数据传输速 率，通常以 MB/s 或 GB/s 为单位。
            				3.磁盘响应时间 = io排队延迟 + 寻址时间（几毫秒） + 旋转时间 + 数据传输时间
            								1.随机 IO 读写延迟就是 8 毫 秒左右，IO 带宽大约每秒 100MB，而随机 IOPS 一般是 100 左右

            				4.SSD
            						1.单元（Cell）、页面（Page）、块（Block）
            								1.SSD 的特点是，对 SSD 单元的每次擦除，都会降低单元的寿命，因此每一个单元只能承受 一定数量的擦除。所以，不同的 SSD 就有这几方面的考虑和平衡。单元存储的位数越多， 制造成本就越少，SSD 的容量也就越大。但是耐久性（擦除次数）也会降低。
            								2.不存在页的覆写，而是直接擦除、重置
            				5.I/O 和垃圾回收
            						1.最小读入一页 page 4k
            						2.ssd问题：写入放大
            								实际写入 SSD 的物理数据量，有可能是应用层写入 数据量的多倍
            									一方面页级别的写入需要移动已有的数据来腾空页面来写入。
            									另一方面，GC 的 操作, 也会移动用户数据来进行块级别的擦除
            							   这么设计是因为：因为一 块 SSD 只能进行有限的擦除次数，也称为编程 / 擦除（P/E）周期，所以写入放大效用会 缩短 SSD 的寿命。
            						3.耗损平衡（Wear Leveling）
            				6.NAS（Network Attached Storage）是网络接入存储。在 NAS 存储结构中，存储系统不 再通过 I/O 总线只属于某个特定的服务器，而是通过网络接口直接与网络相连。NAS 提供 的是文件服务器的功能（比如 NFS 和 CIFS），供客户通过网络访问。


            		4.网络
            				1.指标
            						1.可用性
									2.响应时间（response time）
										端到端的物理距离、所经过网络以及负荷、两端主机的负荷、主机网络队列中堆积情况
									3.网络带宽容量（network bandwidth capacity）
										由设备和网络协议决定的，比如网卡、局域网和 TCP/IP 的特性
									4.网络吞吐量（network throughput）
										网络吞吐量取决于当前的网络负载情况，而且是随着时间不同而不断变化的
									5.网络利用率（network utilization）
										因为 数据传输的突发性，所以实际中的网络利用率一般不会太高。否则的话，那么响应时间就不 能保证。

            				2.论端到端的互联网数据传输		
            						1.机柜交换器
            						2.数据中心网络
            						3.cdn:内容分发网络的基本原理是，利用最靠近每位用户的服务器，更快、更可靠地将（音乐、图 片及其他）文件发送给终端用户
            				3.丢包
            						1.网络 发生了拥塞
            						2.硬件问 题
            						3.软件问题
            					对 TCP 层，可以用比如 netstat 来观察是不是套接字 缓存不够；对操作系统，可以观察 softnet_stat，来判断 CPU 的查询队列；对网卡驱动 层，可以用 ethtool 等来进行分析。

            				4.工具
            						1.Netstat 
            						2.Traceroute 


	6.复现和解决验证
            1.如何提高LLC（最后一级缓存）的命中率？  Cache Hit / miss
            	1.影响
            		1.cpu速度 内存访问延迟时LLC延迟的很多倍
            		2.内存带宽 不命中就要从内存中读取，加大内存带宽
            		3.最近几 年计算机和互联网发展的趋势是，后台系统需要对越来越多的数据进行处理，因此内存带宽 越来越成为性能瓶颈。
            	2.发现定位
            		1.perf : pmu收集各种相关 CPU 硬件事件的数据（例如缓存访问和缓存未命中LLC 读写计数、LLC 不命中计数、LLC 预 先提取计数等指标)

            	2.解决
            		1.紧凑化数据结构
            		2.软件预取数据
            				1.在相当多的情况下，程序对内存的访问模式是随机、不规则的，也就是不连续的。硬件 预取器对于这种随机的访问模式，根本无法做出正确的预测，这就需要使用软件预取。
            				2. __builtin_prefetch ,因为预取的内容会占用缓存空间，所以要谨慎
            							1.软件预取最好只针对绝对必要的情况，就是对会实际严重导致 CPU 停顿的数据进行预 取。
										2.对于很长的循环（就是循环次数比较多），尽量提前预取后面的两到三个循环所需要的 数据。
										3.而对于短些的循环（循环次数比较少），可以试试在进入循环之前，就把数据提前预取 到

            		3.去除伪共享缓存
            				1.避免共享变量，因为底层每次写都要加锁 -- 数据分段整合/数据独立最后整合思想
            						举个具体例子，假如我们要写一个多线程的程序来做分布式的统计工作，为了避免线程对于 同一个变量的竞争，我们一般会定义一个数组，让每个线程修改其中一个元素。当需要总体 统计信息时，再将所有元素相加得到结果。
            						解决:
            							让每个元素单独占用一个缓存行，比如 64 字节，也就是按缓存行 的大小来对齐（Cache Line Alignment）。
            							具体方法怎么实现呢？
            							其实就是插入一些无用 的字节（Padding）。这样的好处，是多个线程可以修改各自的元素和对应的缓存行，不会 存在缓存行竞争，也就避免了“伪共享”问题。

            		
            2.如何提高iTLB（指令地址映射）的命中率？ 
            	1.背景：
            		为了能进行快速的虚拟到物理地址转换，TLB（转换后备缓冲区）这种专门的硬件就 被发明出来了，它可以作为内存页表的缓存。TLB 有两种：数据 TLB（Data）和指令 TLB（Instruction），也就是 iTLB 和 dTLB；因为处理器的大小限制，这两者的大小，也 就是条目数，都不是很大，只能存储为数不多的地址条目。

            	2.解决
            		1.地址映射 - 编译
            				1.重新处理编译二进制，使其hot text 
            		2.使用大页面
            				1.x86_64 上分别为 2MB 和 1GB
            						1.使用较大的页面好处是，减少 了覆盖二进制文件的工作集所需的 TLB 条目数，从而用较少的页面表就可以覆盖所有用到 的地址，也就相应地降低了采用页面表地址转换的成本。
            				2.如果使用“大内存页”，页面变大了（比如 2MB 是 4KB 的 512 倍），自然所需 要的页数就变少了，也就大大减少了由内核加载的映射表的数量。这样就提高了内核级别的 性能，最终提升了应用程序的性能。
            				3.使用大页面来装载程序的热文本区域。通过在大页面上放置热 文本，可以进一步提升 iTLB 命中率。使用大页面 iTLB 条目时，单个 TLB 条目覆盖的代码 是标准 4K 页面的 512 倍。
           
            3.如何降低SSD峰值延迟？
            	1.背景
            		1.写入放大
            				1.当写入 SSD 的物理数据量，大于应用程序打算写入 SSD 的逻辑数据 量时，就会造成“写入放大”。
            				原因:
            					HDD 是可以直接写入覆盖的。和 HDD 不同，SSD 里面的页面只能写入一次， 要重写的话，必须先回收擦除，而且只能在“块”这个级别进行擦除。因此呢，SSD 内部 就需要不断地移动所存储的数据，来清空需要回收的块。也就是说，SSD 内部需要进行块 级别的“垃圾回收”。垃圾收集器必须有效地在 SSD 内部不断地回收块，回收以前使用的 页面空间，然后才能在这个块上写入新数据。
            				缺点:
            					会更快地损耗 SSD 的生命。每个 SSD 都有固定数量的擦除周期，如果在很短时间内写到 SSD 太多数据，就会导致 SSD 损耗太快，有可能过早烧坏 SSD。换句话说，很高的写入速率，可能会导致 SSD 在到 达其预期使用寿命之前就发生故障。
            					一个 SSD 预期寿命是 4 年

            	2.解决
            		1.降低写入放大系数
            				1.是保留一定的空闲存储空间，这是因为写入放大系数是和 SSD 存储空闲率相关的 - 预留空间 / 存储量在80% - 85%
            				2.是使用 Trim
            					SSD 收到 Trim 命令后，SSD 内部的控制器会更新其内部数据页面地图，以便在写入新数 据时不去保留无效页面。
            					并且，在垃圾回收期间不会复制无效页面，这样就实现了更有效的垃圾收集，也就减少了写操作和写入放大系数，同时获得了更高的写吞吐量，延长了驱动器 的使用寿命
            					默认情况下，操作系统一般不启用 Trim。因此，当文件系统删除文件时， 它只是将数据块标记为“未使用”。
            				3.好处是可以减少 SSD 的损耗，延长 SSD 的寿命；坏 处是会造成应用程序的 IO 读写延迟变大。

            4.如何优化程序、OS和存储系统的交互？
            	1.储备
		            	1.问题的表象、问题的" *** 重现 *** " 、性能分析的过程和解决方案
		            	2.从最上层的应用程序，到 中间层 JVM 的机制，再到操作系统和文件系统的特性，最后还涉及到硬件存储的特点。
		            	3.200 毫秒延迟，是多数在线用户可以忍受的 最大延迟。因此，确保低于 200 毫秒（甚至更短）的延迟，已经成为定义的 SLA（服务水 平协议）的一部分。

		        2.现象
		        		1.gc日志
		        			JVM 的堆 4GB,垃圾收集基本不会超过 1 秒,但是
		        			用户和系统时间都可以忽略不计，User 和 Sys 的暂停时间分别 是 0.18 秒钟和 0.01 秒钟，但是实际上 JVM 暂停了 11.45 秒钟

		       	3.解决
		       			1.实验环境搭建--预搭的重要性：出于可控制性和可重复性的考虑，我们使用了自己设计的一个简单 Java 程序。为了方便对 比，我们根据有没有后台背景 IO 活动
		       			2.Java 程序的逻辑也很简单直白，就是不断地分配和删除特定大小的对象
		       			3.为了真实地模拟生产环境，我们在第二种场景中注入后台 IO。 这些 IO 由 bash 脚本生 成，该脚本就是不断复制很大的文件。
		       				在实际的生产过程中，IO 负载可能 来自很多地方，比如操作系统、同一机器上的其他应用程序，或者来自同一 Java 应用程序 的各种 IO 活动。

		       			4.跟踪指标：
		       					1.总暂停时间，即所有 STW  暂停的总暂停时间
		       					2.较大的 STW 暂停计数和

		       					工具监测 分析
		       	4.分析
		       			1.io问题导致，那么此时的swt 需要哪些io操作呢？
		       				 GC 日志记录，write() 调用被阻塞导致的
		       				 虽然以缓冲写入模式（即非阻塞 IO）发出，但由于操作系统有“回写”IO 的机制，所 以仍然可能被操作系统的“回写”IO 阻塞。
		       			2.为什么非阻塞 IO，还会被阻塞？！
		       				非阻塞 IO 写入还是可 能会停留，并被阻塞在内核代码执行过程中。具体原因有好几个，其中包括：
		       				页面写入稳定 （Stable Page Writing）
		       				件系统日志提交（Journal Logging）
		       	5.解决
		       			  修改 JVM；将 GC 日志记录活动，与导致 STW 暂停的关键 JVM GC 进程分开 可能造成数据丢失
						  减少后台 IO；
						  将 GC 日志记录与其他 IO 分开	ssd  是JVM的一个参数 -Xloggc:<gc-log-file-path>。

			5.如何在生产环境中进行真实的容量测试？ 
				1.如何保证服务容量足够呢？首先就必须做好服务容量的预测
						一般是先确定每台单独的服务器可以支撑 多少服务流量；然后用这个单台服务器的数据，来决定这个服务整体需要多少台服务器
				2.测试为什么需要在生产环境中进行？
						1.客户需求会随着时间而变化，例如高峰时段与非高峰时段的流量就很不一样；
						2.用户请求的多样性，例如不同国家的查询类型不同；2.
						3.负载流量的规模，基础架构设施的变化，例如服务的软件版本更新，微服务互相调用的 变化等等。
						由于难以在非生产环境中进行准确的容量测 试，我们经常需要转向真正的生产环境，使用实时而真实的客户流量负载来测试
				3.实施
						1.重定向用户数据
						2.挑战
								1.要使用实时流量以确保准确性。
								2.尽量不影响生产流量，这就需要实时的监测和反馈模块。
								3.可以定制重定向等行为规则；对不同的服务和不同的场景的测试，各种性能指标和阈值 都会不同。
								4.能自动终止测试，并把测试环境复位，尽量减少人工干预。
								5.支持基于日历和事件的自动触发和调度
						3.具体方案
							1.线上测试组件.png
							2.LiveRedliner:核心控制组件是整个系统的神经中枢，负责总体调度容量测试的过程。比如何时发起测试， 何时终止测试，何时需要增加更大的流量等等
							  TrafficRedirector:Redliner 是通过客户请求的属性（例如用户 ID、语言或帐户创建日 期），来决定是否对一个客户请求来进行重定向的
							  					重定向流量可以通过另外一个叫做“资源动态发现和负载均衡”的模块来 实现
							  					权重
							  PerfCollector
							  PerfAnalyzer：指标的值来确定 SUT 是否饱和

			6.怎么规划和控制数据库的复制延迟大小？ 
					1.背景:
							1.当今主要的互联网公司，往往部署一整套的数据处理系统。这一整套系 统通常由几个模块构成，包括数据事件的捕获、数据的存储、数据的复制传输、数据的读 取。

					2.分析
							1.按月/周/天统计信息并绘制图表，进行流量特征
					3.数据传输复制模块 -- 数据复制传输系统.png
							1.除了可以提供高扩展性，也可以提供数据的一致。需要一个整合而一致的数据视图。这样一个目 的，是可以通过传输复制实时数据库事件来得到的。
							2.数据的传输复制模块，其实是一个排队系统，是一个有无限缓冲的先进先出队 列。

					4.历史分析预估未来/sla调整 / 动态 自动化


			7.多任务环境中的Java性能问题，怎样才能不让程序互相干扰？   详见：《27：多任务环境中的Java性能问题，怎样才能不让程序互相干扰》
					1.现象及关联性图表分析
					2.解决方案


			8.网络数据传输慢，问题到底出在哪了？ 

					1.原因
							1.客户端应用程序的原因
							2.网络的原因
							3.服务器应用程序的原因
					2.如何对上述原因进行隔离判断
							1.数据传输涉及多个网络实体，包括两台机器（也就是发送者和接收者）和网络路由， 这与仅涉及一台机器的常见性能问题形成鲜明对比。
							2.诊断涉及多层信息，包括应用程序层和网络传输层。为了找出原因，工程师必须 检查各种数据，包括客户端日志、服务器日志、网络统计信息、CPU 使用情况等。这些检 查需要花费很多时间和精力，并且通常需要性能工程师的经验和专业知识。
							3.日志杂乱
							提出了一种当发生数据传输缓慢的问题时，可以自动 隔离原因的解决方案。毕竟，你只有找出了要对“数据传输慢”负责任的那一部分，才可以 进行后续分析工作，最终确定真正的问题。

					3.流控制功 能，可避免过载； 通过操作 系统来监测当前队列大小。 <=== netstat ss
					4.结论：
							1.网络传输慢原因.png
					5.解决：
							1.基于状态机的解决方案，它是一个针对 HTTP 数据传输问题的，完整 而具体的解决方案。
							  对上满原因.png进行自动监控



			9. 如何彻底发挥SSD的潜力？ 
					1.HDD 最大应用程序吞吐量为 142 个查询 / 秒  ==>  SSD  20,000 QPS
						使用多个并发线程来执行 I/O。这利用了 SSD 的内部并 行性。
						在这个系统中，多个 I/O 线程对 HDD 是毫无益处的。因为 HDD 只 有一个磁头，所以用多个 I/O 线程，并不能提高旧系统的吞吐量

					2.更高效的存储 I/O。
						SSD 上的最小内部 I/O 单元是一页，比如 4KB 大小,必须在页面级进
						ssd寿命:	
								1.SSD 大小
								2.最大擦除周期数
								3.写入放大系数
								4.应用程序写入速
						1.文件系统(数据库系统)		 Ext4 和 Btrfs		基本思想是采用日志结构的数据布局（相对于 B 树或 Htree），来容纳 SSD 的“复制 - 修改 - 写入”属性，比如 NVFS（非易失性文件系统）、FFS / JFFS2 和 F2FS。
					3.数据基础架构
						计算机上的本地磁盘或另一 台计算机上的内存？。这两个来源哪个更快更高效呢？
							Memcached  传统的本地 HDD 访问延迟，大约是好几个毫秒；而远程内存访问的延迟，包 括了 RAM 访问延迟和网络传输延迟，也仅处于微秒级
							而 SSD 的出现，使用 SSD 作为存储设备后，本地 SSD 变得比远程内存访问更为高效。
							SSD 的 I/O 延迟降低到了微秒级，而 I/O 带宽可是比 HDD 高一个数量级

					4.应用层
						1.应用层对ssd友好设计 
							1.数据结构 
									1.避免就地更新优化
										1.传统的 HDD 的寻址延迟很大，因此，使用 HDD 的应用程序通常会进行各种优化，以执行 不需要寻址的就地更新，比如只在一个文件后面写入。
										  不过在设计与 SSD 配合使用的应用程序时，这些考虑就没什么意义了。
										2.SSD 上的随机更新，就不会引起读取和修改步骤（即仅仅“写入”），因此速 度更快。
									2.区分热、冷数据
										1.将 SSD 用作存储设备时，应将热数据与冷数据分开。以不同级 别或不同方式来进行分隔。例如，把它们存在不同的文件、文件的不同部分或不同的表格 里。
									3.采取紧凑的数据结构
										1.在 SSD 的世界中，最小的更新单位是页面（4KB），因此，即使是一个字节的更新，也将 导致至少 4KB SSD 写入。由于写入放大的效果，实际写入 SSD 的字节可能远大于 4KB。 


							2.io处理
									1.避免长而繁重的持续写入
										1.由于后台 GC 是异步发生的（即非阻塞），因此它不会影响应用程序的 I/O 延 迟。但是，如果块的请求速率超过了 GC 速率，并且后台 GC 无法跟上，则将触发前台 GC。
									2.避免 SSD 存储太满
										1.SSD 磁盘存储的满存程度，会影响写入放大系数和 GC 导致的写入性能。在 GC 期间，需 要擦除块以创建空闲块。擦除块前，需要移动并保留有效数据才能获得空闲块。有时为了获 得一个空闲块，需要压缩好几个存储块。而每个空闲块的生产需要压缩的块数，取决于磁盘 的空间使用率。

							3.线程使用
									1.使用多个线程执行小的 I/O
										1.SSD 内部大量使用了并行的设计，这种并行表现在多个层面上。一个 I/O 线程无法充分利 用这些并行性，会导致访问时间更长。而使用多个线程，就可以充分利用 SSD 内部的并行 性了。
										2.这里的“小”IO，到底有多小
											只要是不能充分利用内部并行性的任何 I/O 大小，都被视为“小”。例如，SSD 页面大小为 4KB，内部并行度为 16，则阈值应约为 64KB
									2.使用较少的线程来执行大 I/O
										1.对于大型 I/O，SSD 内部已经充分优化使用了 SSD 的内部并行性，因此，应使用更少的线 程（即小于四个）以实现最大的 I/O 吞吐量。从吞吐量的角度来看，用太多线程不会有太 大益处。更重要的是，使用太多线程可能导致线程之间的资源竞争，以及诸如 OS 级的预读 和回写之类的后台活动。

			10.<26丨案例：手把手带你理解TPS趋势分析>
			11.<27丨案例：带宽消耗以及Swap（上）>
			12.<28丨案例：带宽消耗以及Swap（下）>
			13.<29丨案例：如何应对因网络参数导致的TPS呈锯齿状？>
			14.<30丨案例：为什么参数化数据会导致TPS突然下降？>
			15.<31丨案例：当磁盘参数导致I-O高的时候，应该怎么办？>
			16.<32丨案例：当Postgres磁盘读引起I-O高的时候，应该怎么办？>
