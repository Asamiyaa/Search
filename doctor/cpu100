
问题定位：
	1.20210220 pod一直重启
		0.进来看pod中的java进程是否存在 ps -ef|grep java
		1.desribe pod中看到探针。...status不满足，所以重启了。
		2.拿着探针url进pod测了一下，发现连接异常
		3.网络？
		4.通过kubectl edit deploy xxxx 把副本改为多个发现新创建的pod也是连接异常。对于k8s来说都是新机器，不可能出现端口没开的情况。
			并且通过查看端口命令发现7001端口正常
		5.查看日志，发现插入表因为没有分区一直报错；所以添加分区 。。 这也是为什么刚开始没有数据的时候，服务正常的，但是需要处理的时候有问题了。
		6.多次添加失败，因为没有仔细去读建表sql中，因为开启分区是有三条sql的。而我只是执行了两条
		7.表处理好后可以插入数据了。但还是重启。
		8.查看机器的性能
			1.top 发现top中java cpu 100-300%,avage..2..3..这是为什么呢？
			2.查看网上的教程都是拿cpu对应的进程、拿java线程、查看死锁；但对于当下场景来说，代码是旧的，不可能出现代码问题，环境问题？
		9.log一直在写入，但是cpu居高不下，是不kafka积压造成的系统处理问题，导致探针异常呢?
		10.横向扩展 通过k8s命令kubectl edit deploy xxxx修改副本为多个，加快处理.但是cpu很快又飙升到了300%
		11.cpu资源不足？通过k8s命令kubectl edit deploy xxxx 查看当前是1c1g,直接修改为4c8g。临时解决。
		12.进pod查看cpu还是很高，过了一会就降下来了。解决了，原来是cpu不足导致的。那这个配置从哪里读取的呢？
		13.manager代码中发现是从数据库读取的。查询数据库(另一个)发现配置很高呀。
		14.其实两个库只是同步了部分表，导致这里因为vendor读到的default vendor的配置中的还是1c2g
		15.从values.yaml中读取的?但是values.yaml只是取值，最后还要和数据库的模板融合才是最终的。
		16.查看当前环境的配置，发现真的是低配置，修改后，重启，永久的解决。

	==> 查看topic积压问题
		1.命令
		2.各个指标
	==> 查看端口
		1.lsof -i:8000
		2.netstat -tunlp | grep 端口号

	==> 优化过程
		1.查看系统性能，直接上arthas 全局  thread -n 5 查看消耗cpu的前5个线程
		2.全局来看
		3.定位最有可能的原因，并分析
		4.多维度验证

	==> 指导性文章：
		1.https://www.cnblogs.com/dennyzhangdd/p/11585971.html    <=== 可能原因分析
		2.https://crossoverjie.top/2018/12/17/troubleshoot/cpu-percent-100/

	==> 有问题的环境4c8g，比原来的8c16g的gc日志快
		1.通过ps -ef|grep java 拿到gc日志地址 、 heapdump地址
		2.查看日志现在20s执行一次 而老环境是 

			
			3.系统的内存当前使用率咋样？ 结论：可以通过top中cpu和vmstat中wa共同确认哪里的问题。这些命令解决了内存和cpu

				1.使用top、vmstat内存都大约20g，但是创建pod时候确实8个G，为什么/
					物理机20g -->虚拟机(pod 8g) --> ps-ef|grep java看到java分配了2g
					问题：通常物理内存的多少作为java进程使用？


				2.top命令详解：https://www.cnblogs.com/peida/archive/2012/12/24/2831353.html
					可用内存近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：18537836k +169884k +3612636k = 22GB左右。

					*****对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。

					RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA  **** <=== 这个指标可以和java设定内存比对

				  vmstat详解：https://segmentfault.com/a/1190000018834649
				   	swpd:虚拟内存已使用的大小，如果大于0，表示机器物理内存不足了
				   	si:每秒从磁盘读入虚拟内存的大小，若值大于0，表示物理内存不过或者内存泄漏
					so:每秒虚拟内存写入磁盘的大小，若值大于0，同上
					bo:每秒发送到块设备的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整
					us:用户CPU时间。 us的值比较高时，说明用户进程消耗的CPU时间多
					sy:系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁
					wa:表示等待使用CPU的百分比（该数值大说明CPU不足）
					st:表示被偷走的CPU所占百分比（该值一般为0，不用关注）

				  VIRT 虚拟内存高  -- 无需关注
				  		1.物理内存与虚拟内存  https://www.jianshu.com/p/278cb78aa4f9
				  		2.Java 进程占用 VIRT 虚拟内存超高的问题研究  https://blog.csdn.net/odeng888/article/details/80606013

				  free -h 
				  		1.查看total = used + free + buff/cache 

				  磁盘
				  		1.vmstat si /so 
				  		2.iostat


				3.java进程的内存够用吗？
					0.查看当前java进程现象?
						1.ps -ef|grep java 查看java进程参数
						2.java真正占用内存大于xmx原因：
							JVM内存分配理解 ： https://www.jianshu.com/p/80ae667e1b7f
							JVM进程内存 ≈ JVM程序自身占用内存+Java永久代 ＋ Java堆(新生代和老年代) ＋ 线程栈＋ Java NIO(直接内存)  <==== 估算 并且有具体的查看命令
						3.JVM参数配置详解（包含JDK1.8）： https://blog.csdn.net/yjl33/article/details/78890363

/******/java/bin/java -server -Xmx2G -Xms2G -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m -XX:MaxDirectMemorySize=1g -XX:SurvivorRatio=10 -XX:+UseConcMarkSweepGC -XX:CMSMaxAbortablePrecleanTime=5000 -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly -XX:+ExplicitGCInvokesConcurrent -Dsun.rmi.dgc.server.gcInterval=2592000000 -Dsun.rmi.dgc.client.gcInterval=2592000000 -XX:ParallelGCThreads=40 -Xloggc:/root/logs/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/logs/java.hprof -Djava.awt.headless=true -Dsun.net.client.defaultConnectTimeout=10000 -Dsun.net.client.defaultReadTimeout=30000 -DJM.LOG.PATH=/root/logs -DJM.SNAPSHOT.PATH=/root/snapshots -Dfile.encoding=UTF-8 -Dhsf.publish.delayed=true -Dpandora.boot.wait=true -Dlog4j.defaultInitOverride=true -Daddress.server.domain=diamond..aliyun-inc.com -Drocketmq.namesrv.domain.subgroup=nsaddr -Drocketmq.namesrv.domain=diamond..aliyun-inc.com -Dpandora.location=/home/admin/vcs-dataengine/target/taobao-hsf.sar -classpath /home/adm/ataengine:/home/adme/config -Dapp.location=/home/admi/taengine -Djava.endorsed.dirs= -Djava.io.tmpdir=/home/admin/vcs-dataengine/.default/temp -Djava.util.Arrays.useLegacyMergeSort=true com.er.SarLauncher


					1.gc的频率多少算正常
							1.fullGC 和 majorGc
							1、不管什么GC，都会发送stop the world，区别是发生的时间长短。而这个时间跟垃圾收集器又有关系，Serial、PartNew、Parallel Scavenge收集器无论是串行还是并行，都会挂起用户线程，而CMS和G1在并发标记时，是不会挂起用户线程，但其他时候一样会挂起用户线程，stop the world的时间相对来说小很多了。

							2、major gc很多参考资料指的是等价于full gc，我们也可以发现很多性能监测工具中只有minor gc和full gc。
							一般情况下，一次full gc将会对年轻代、老年代以及元空间、堆外内存进行垃圾回收。而触发Full GC的原因有很多：
							a、当年轻代晋升到老年代的对象大小比目前老年代剩余的空间大小还要大时，此时会触发Full GC；
							b、当老年代的空间使用率超过某阈值时，此时会触发Full GC;
							c、当元空间不足时（JDK1.7永久代不足），也会触发Full GC;
							d、当调用System.gc()也会安排一次Full GC;

							G1与CMS的优势在于以下几点：
								1、并行与并发：G1能够更充分利用多CPU、多核环境运行
								2、分代收集：G1虽然也用了分代概念，但相比其他收集器需要配合不同收集协同工作，但G1收集器能够独立管理整个堆
								3、空间管理：与CMS的标记一清理算法不同，G1从整体上基于标记一整理算法，将整个Java堆划分为多个大小相等的独立区域（Region）,这种算法能够在运行过程中不产生内存碎片
								4、可预测的停顿：降低停顿时间是G1和CMS共同目标，但是G1追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定一个长度为M毫秒的时间片段内，消耗在垃圾收集器上的时间不得超过N毫秒。
							并发收集器 CMS(Concurrent Mark-Sweep)：https://blog.csdn.net/wfh6732/article/details/57490195

					2.gc日志代表什么
						https://blog.csdn.net/renfufei/article/details/49230943



	==> 问题2：cpu 100% 造成原因分析：
		1.内存消耗过大，导致Full GC次数过多
		2.代码中有大量消耗CPU的操作，导致CPU过高，系统运行缓慢； -- cpu本身申请不足(上面内容)
		3.由于锁使用不当，导致死锁。
		4.随机出现大量线程访问接口缓慢。
		"KafkaSubscriberHandler22" #342 daemon prio=5 os_prio=0 tid=0x00007fdf11110000 nid=0x27e waiting on condition [0x00007fdc557da000]  <==== 当前的问题就是这个
		5.某个线程由于某种原因而进入WAITING状态，此时该功能整体不可用，但是无法复现；

	原始机器 cpu排查:https://www.cnblogs.com/dennyzhangdd/p/11585971.html        			
	Arthas使用 : https://www.jianshu.com/p/507f7e0cc3a3

				原始机器	- 直接命令								java - arthas
		   top: 145
		   top -Hp 进程号 : 638/681/352  			---> 			dashboard -n 1  第一块：看到的就是该进程下，线程的消耗 
											  						第二块：memory
		   将线程进程ID转换为十六进制格式: printf 0x%x 638
		   jstack 145 |grep 0x27e KafkaSubscriberHandler是守护线程。
		   jstat -gcutil 145 ： https://www.cnblogs.com/hadoop-dev/p/7209513.html


从实际案例聊聊Java应用的GC优化：https://tech.meituan.com/2017/12/29/jvm-optimize.html

arthas几个命令：
			1.watch
					1.对于日志没有打好，有需要调试的来说  
					watch xx.xxx.xx.xx.xxx.xx.BerlinService view '{params, returnObj}' -x 2
					2.如过需要按照耗时进行过滤，需要加入： '#cost>200' 代表耗时超过200ms的才会打印出来
					3.watch com.example.httpclientdemo.HttpclientDemoApplication send 'params[0].getMethod()'
			2.trace - 耗时定位
					1.trace xx.xxx.xx.xx.xxx.xx.QueryAction queryByReq  -j '#cost>5' -n 5
					2.可以指定深度吗？比如sql层

			3.stack
					1.我们都知道一个方法被执行，但这个方法被执行的路径非常多，或者你根本就不知道这个方法是从那里被执行了
					  用来定位从哪个分支来的

			4.ognl
					1.如何实现类似btrace的热加载类，对某个信息进行统计?
					2.watch com.example.httpclientdemo.HttpclientDemoApplication doSend 'params[0].{? #this.username.endsWith("9")}.size()'  <=== 思考：就像模板引擎、freemaker或者helm、k8s配置一样
					  配合和逻辑在一起。
					  https://segmentfault.com/a/1190000020505575
					3.获取spring相关东西


			5.临时动态加载类到处理逻辑中，可以不重启
					1.https://blog.csdn.net/qq_22912803/article/details/104494018


处理问题：
		0.复现

		1.先看查询条件是否符合
				1.是否出现
				2.时间段是否对
		2.查询超时
				0.后台pod是否正常
				1.直接报timeout grep exception 
				2.后台正常处理，只不过时间较长，前台对其做了超时抛出  <=== 对整体业务流程和系统架构上
				3.gateway
		3.善于使用工具
				1.es构成的日志查看
				2.后天less .../xxx 定位关键字


