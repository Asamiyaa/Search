检索技术
		
		0.全景图
			1....github...
			2.....githbub....  
			3.query.png


		1.检索其实就是将我们所需要的信息，从存储数据的地方高效取出的一种技术。
		  所以，检索效率和数据存储的方式是紧密联系的。具体来说，就是不同的存储方式，会导致不同的检索效率。那么，研究数据结构的存储特点对检索效率的影响就很有必要了。
		  检索的核心思路，其实就是通过合理组织数据，尽可能地快速减少查询范围。

		2.各个数据结构特点
				1.数组
						1.连续的内存空间
						2.可随机访问下标数据
				
				2.链表 

			==> 改进1：让链表每个节点不再只是存储一个元素，而是存储一个小的数组。这样我们就能大幅减少节点的数量，从而减少依次遍历节点带来的“低寻址效率”。注意自己平时的集合嵌套能否优化

			==> 问题1：数据频繁变化，如何快速检索?
			==> 改造2：我们能否结合上面的例子，使用非线性的树状结构来改造有序链表，让链表也具有二分查找的能力呢？ == 使用O(1)而不是O(N/2)定位链表的中间元素
				
				3.树
						1.引入了双向指针来快速遍历左右
						2.直接过滤一半，所以查找时间是O(logn),搜索方面相对于数组多了，相对于链表少了；插入方面相对于数组少了，相对于链表多了(维护二叉平衡)
			==> 问题2：二叉树的搜索耗时一定是O(logn).
					错误，假设，一个二叉树的每一个节点的左指针都是空的，右子树的值都大于根节点。那么它满足二叉检索树的特性，是一颗二叉检索树。但是，如果我们把左边的空指针忽略，你会发现它其实就是一个单链表！单链表的检索效率如何呢？其实是 O(n)  。  所以说，工业级，异常化的处理平衡是重要的。
			==> 改造3：可能地保证二叉检索树的平衡性，让左右子树尽可能差距不要太大。这样无论我们是继续往左边还是右边检索，都可以过滤掉一半左右的数据

					3.1：AVL 树（平衡二叉树）和红黑树
			==> 改造4：链表之所以访问中间节点的效率低，就是因为每个节点只存储了下一个节点的指针，要沿着这个指针遍历每个后续节点才能到达中间节点。那如果我们在节点上增加一个指针，指向更远的节点，比如说跳过后一个节点，直接指向后面第二个节点，那么沿着这个指针遍历，是不是遍历速度就翻倍了
			==> 问题3：当我们要在跳表中插入元素时，节点之间的间隔距离就被改变了。如果要保证理想链表的每隔 2^n 个节点做一次链接的特性，我们就需要重新修改许多节点的后续指针，这会带来很大的开销。
					  从理想 -> 大概率平衡
				
				4.跳表
					确认新加入的节点需要具有几层的指针。我们通过随机函数来生成层数，比如说，我们可以写一个函数 RandomLevel()，以 (1/2)^n 的概率决定是否生成第 n 层。这样，通过简单的随机生成指针层数的方式，我们就可以保证指针的分布，在大概率上是平衡的。

					随机的层级，是用概率的思路来解决跳表指针平衡分布的问题。我可以换一种角度再描述一下。你可以按我的描述，在纸上将图画出来，看看是否会好理解:假设我们有m个节点，由于随机函数是（1/2）^n，因此，每个节点有第0层的概率是1，也就是说，所有的节点都有一个指向下一个节点的指针。（试着画出来）而节点拥有第1层的概率，变成了1/2，也就是只有一半的随机节点会有第1层，那么这一半的节点就会连起来。（试着画出来）在这一半的节点中，拥有第3层的节点数，又是随机的一半。（试着画出来）
					


					当然，由于新加入节点的层数是随机生成的，因此在节点数目较少的情况下，如果指针分布的不合理，检索性能依然可能不高。但是当节点数较多的时候，指针会趋向均匀分布，查找空间会比较平衡，检索性能会趋向于理想跳表的检索效率，接近 O(log n)。


		    ==> 思考：红黑树 vs 跳表
		    		1.是线上后者更简单
		    		2.跳表保持了链表顺序遍历的能力，在需要遍历功能的场景中，跳表会比红黑树用起来更方便

		    		跳表 vs 有序数组
		    		0.后者仅适用于数据量小的，比如几十w呢？如何处理。连续空间内存申请不下来
		    		1.有序数据占用的内存空间小于调表
		    		2.有序数组的读取操作能保持在很稳定的时间复杂度，而跳表并不能
		    		3.因为数组存储空间是连续的，可以利用内存的局部性原理加快查询
		    		4.考虑到范围查找需求，数组的处理效率会更高（内存拷贝+内存局部性原理

		    ==> 问题：根据键（Key）来查询数据。
		    		使用上面知识：具体来说，我们可以将用户 ID 和用户信息作为一个整体的元素，然后以用户 ID 作为 Key 来排序，存入有序数组或者二叉检索树中，这样我们就能通过二分查找算法快速查询到用户信息了

		    ==> 改造：将key/value进行hash排列。所以尝试从list到查找，转换存储结构。hash不仅仅是 表面的，逻辑上的 (K,V)。这种将对象转为有限范围的正整数的表示方法，就叫作 Hash，翻译过来叫散列
		    	
		    	5.hash散列
		    		1.对于相同的hash值，会得到相同的下标，出现hash碰撞/冲突如何处理、
		    				1.寻址法 ： 线性探查”的插入逻辑很简单：在当前位置发现有冲突以后，就顺序去查看数组的下一个位置，看看是否空闲。查询以然
		    ==> 改造：寻址法造成： Key 就会沿着数组一直顺序遍历，直到遇到空位置才会成功插入。查询的时候也一样。但是，顺序遍历的代价是 O(n)，这样的检索性能很差。如果我们在插入 key1 后，先插入 key3 再插入 key2，那 key3 就会抢占key2 的位置，影响 key2 的插入和查询效率
		    						1.1：二次探测：将线性探查的步长从 i 改为 i^2：第一次探查，位置为 Hash(key) + 1^2；第二次探查，位置为 Hash(key) +2^2；
		    						1.2：双散列：就是使用多个 Hash 函数来求下标位置，当第一个 Hash 函数求出来的位置冲突时，启用第二个 Hash 函数，算出第二次探查的位置
		    				2.链表法
		    						将我们前面讲过的数组和链表进行结合，既利用了数组的随机访问特性，又利用了链表的动态修改特性，同时提供了快速查询和动态修改的能力。
		    ==> 改造：如果链表长呢，会影响性能：
		    						JDK1.8 之后，Java 中 HashMap 的实现就是在链表到了一定的长度时，将它转为红黑树；而当红黑树中的节点低于一定阈值时，就将它退化为链表。

		    		2.如何确定一个hash的桶个数？
		    				即hash缺点：O(1)前提条件:哈希表要足够大和有足够的空闲位置，否则就会非常容易发生冲突。我们一般用装载因子（load factor）来表示哈希表的填充率。装载因子 = 哈希表中元素个数 / 哈希表的长度

		    				为了保证哈希表的检索效率，我们需要预估哈希表中的数据量，提前生成足够大的哈希表。按经验来说，我们一般要预留一半以上的空闲位置，哈希表才会有足够优秀的检索效率。

		    		3.HASH缺点：不能有序存储

		    ==> 问题：判断一个是否存在、判断状态需求，不能简单的fore，通过检索学习对于数组、树、hash...等方式对 ’大数量下判断是明显的‘，<== 二分 。 数组o(logn) / hash O(1) 
		    ==> 改造：申请一个大于数量的数组，将下标作为需要判断的参数，通过值的0/1判断该参数是否有值。(特殊处理后的有序数组-下标特定) <== 下标特定化
		    ==> 问题：对于大数据量，即使是数值，每个数组元素都是4byte,造成浪费
		    ==> 改造：使用char/boolean一个byte,仍然有浪费，使用bit
		    	
		    	6.位图
		    		1.以 bit 为单位构建数组的方案，就叫作 Bitmap，翻译为位图。
		    		2.许多系统中并没有以 bit 为单位的数据类型。因此，我们往往需要对其他类型的数组进行一些转换设计
		    			1.申明一个char数组，一个数组元素是8 bit . 当需要定位第11个元素时，则  11/8=1...3 即第二个元素的第三个bit位         O(1)
		    			2.判断第三个位置是0/1 ，先声明一个第三位置是1的00100000，与第二个元素做位运算。                                    O(1)
		    ==> 问题：压缩数组长度 - hash ,并且解决了key可以是任意字符的问题。但出现了hash冲突。。hash压缩之后从位图 ---> bloomFilter
		    		3.Bloom Filter
		    			1.一个对象使用多个哈希函数。
		    				如果我们使用了 k 个哈希函数，就会得到 k 个哈希值，也就是 k 个下标，我们会把数组中对应下标位置的值都置为1。
		    				布隆过滤器和位图最大的区别就在于，我们不再使用一位来表示一个对象，而是使用 k位来表示一个对象。
		    				这样两个对象的 k 位都相同的概率就会大大降低，从而能够解决哈希冲突的问题了。

		    			2.特点
		    				1.布隆过滤器的错误率 --- 因为数组长度压缩到固定的长度，导致不同覆盖问题，即使使用了多个hash，但仍然有错误的
		    					即使任何两个元素的哈希值不冲突，而且我们查询对象的 k 个位置的值都是 1，查询结果为存在，这个结果也可能是错误的。
		    			  	2.不可删除

		    			  	3.bloomfilter来说如果不所在一定不存在，存在不一定存在(因为hash冲突，可能是另外的元素状态)..

		    			在系统不要求结果 100% 准确的情况下使用

		    			3.计算最优哈希函数个数的数学公式: 
		    				哈希函数个数 k = (m/n) * ln(2)。其中 m 为 bit 数组长度，n 为要存入的对象的个数。实际上，如果哈希函数个数为 1，且数组长度足够，****布隆过滤器就可以退化成一个位图*****。所以，我们可以认为“*****位图是只有一个特殊的哈希函数，且没有被压缩长度的布隆过滤器***”。


		    			4.使用场景
		    				布隆过滤器：缓存命中
		    				搜索引擎中，我们也需要使用布隆过滤器快速判断网站是否已经被抓取过，如果一定不存在，我们就直接去抓取；如果可能存在，那我们可以根据需要，直接放弃抓取或者再次确认是否需要抓取。<=== 不准确如何作为判断依据？

		    				你会发现，这种快速预判断的思想，也是提高应用整体检索性能的一种常见设计思路。布隆过滤器初始化均为0，所以：如果不所在一定不存在，存在不一定存在(因为hash冲突，可能是另外的元素状态)..

		    		7.索引倒排
		    			1.正排索引 vs 倒排索引
		    					1.以对象的唯一 ID 为 key 的哈希索引结构，叫作正排索引（Forward Index）   【文章名 ， 文章内容】
		    					2.根据具体内容或属性反过来索引文档标题的结构，我们就叫它倒排索引           【文章中关键字，文章名列表】			--- 全文搜索、诗词大会	
		    							在倒排索引中，key 的集合叫作字典（Dictionary），一个 key 后面对应的记录集合叫作记录列表（Posting List）
		    			2.构建倒排索引
		    					1.给每个文档编号，作为其唯一的标识，并且排好序，然后开始遍历文档（为什么要先排序，然后再遍历文档呢？你可以先想一下，后面我们会解释）。
		    					2.解析当前文档中的每个关键字，生成 < 关键字，文档 ID，关键字位置 > 这样的数据对。为什么要记录关键字位置这个信息呢？因为在许多检索场景中，都需要显示关键字前后的内容，比如，在组合查询时，我们要判断多个关键字之间是否足够近。所以我们需要记录位置信息，以方便提取相应关键字的位置。
		    					3.将关键字作为 key 插入哈希表。如果哈希表中已经有这个 key 了，我们就在对应的 posting list 后面追加节点，记录该文档 ID（关键字的位置信息如果需要如果哈希表中还没有这个 key，我们就直接插入该 key，并创建posting list 和对应节点。
		    					4.复第 2 步和第 3 步，处理完所有文档，完成倒排索引的创建。
		    			3.AND
		    					第 1 步，使用指针 p1 和 p2 分别指向有序链表 A 和 B 的第一个元素。
		    					第 2 步，对比 p1 和 p2 指向的节点是否相同，这时会出现 3 种情况：
		    						两者的 id 相同，说明该节点为公共元素，直接将该节点加入归并结果。然后，p1 和 p2要同时后移，指向下一个元素；p1 元素的 id 小于 p2 元素的 id，p1 后移，指向 A 链表中下一个元素；p1 元素的 id 大于 p2 元素的 id，p2 后移，指向 B 链表中下一个元素。
		    					第 3 步，重复第 2 步，直到 p1 或 p2 移动到链表尾为止。

		    							|  |  | 中间为全量doc , 第一个为p1，关键字key1的列表 ， 第三为p2,关键字为key2的列表。则通过移动，使其指向同一个则这个就是满足AND关系

		    					== 多路归并
		    			4.拉链表
		    			5.数据库中的in(content,"xxx")操作原理，创建对应全文索引？

		    			6.搜索引擎就是典型的应用场景，因为我们只知道我们想找什么关键字，而不知道哪些网页有这些关键字，因此需要倒排索引。
		    			  数据库也一样，很多时候，我们去数据库中查找，也不是直接找id，而是用where去限定一些属性和字段。因此，你会发现，根据我们关心的属性去寻找主体，这种需求其实很常见，这些场景就可以用倒排索引了。

		    		

		    		问题：如果索引特别大，如何边去部分索引边归并呢？
		    				1.通过压缩，全塞进内存；
		    				2.放磁盘上，用b+树或分层跳表处理；---cpu内存、硬盘寻址
		    				3.分布式，分片后全放内存。

		    		问题：若果在已有的倒叙索引上，需要添加作者的信息呢？ 
		    				1.posting list的记录中加域区分。比如用两个比特位，第一个表示是否是作者，第二个表示是否是内容
		    				2.新建一个倒排索引是不错的方案，取并集
		    				区别：相对于要搜索的是从不同维度，前面是关键字现在是作者


		    			7.倒排索引是所有搜索必须用到的，工业界如何利用跳表、哈希表、位图进行加速？(在倒排索引的检索过程中，两个 posting list 求交集是一个最重要、最耗时的操作)
		    		问题：假设 posting list A 中的元素为 <1,2,3,4,5,6,7,8......，1000>，这 1000 个元素是按照从 1到 1000 的顺序递增的。而 posting list B 中的元素，只有 <1,500,1000>3 个。那按照我们之前讲过的归并方法，它们的合并过程就是，在找到相同元素 1 以后，还需要再遍历498 次链表，才能找到第二个相同元素 500。
		    					
		    					1.跳表
		    							1.搜索引擎，还有 Lucene 和 Elasticsearch等应用中，都是使用跳表来实现 posting list 的。
		    							2.O(log(498))
		    		问题：我们只能拿其中一个posting list进行查询吗？
		    							1.互相二分查找
		    							2.如果 posting list 可以都存储在内存中，并且变化不太频繁的话，那我们还可以利用可变长数组来代替链表。 
		    								数组的数据在内存的物理空间中是紧凑的，因此 CPU 还可以利用内存的局部性原理来提高检索效率
		    					
		    					2.hash  - o(1)
		    							1.当两个集合要求交集时，如果一个集合特别大，另一个集合相对比较小，那我们就可以用哈希表来存储大集合。这样，我们就可以拿着小集合中的每一个元素去哈希表中对比：如果查找为存在，那查找的元素就是公共元素；否则，就放弃。
		    							2.针对这一批postting list ,用到哪些就转到hash里面去
		    							3.原始的 posting list 我们也要保留。这是为什么呢？
		    									我们假设有这样一种情况：当我们要给两个 posting list 求交集时，发现这两个 postinglist 都已经转为哈希表了。这个时候，由于哈希表没有遍历能力，反而会导致我们无法合并这两个 posting list。因此，在哈希表法的最终改造中，一个 key 后面会有两个指针，一个指向 posting list，另一个指向哈希表（如果哈希表存在）。除此之外，哈希表法还需要在有很多短 posting list 存在的前提下，才能更好地发挥作用。这是因为哈希表法的查询代价是 O(m)，如果 m 的值很大，那它的性能就不一定会比跳表法更优了。

		    					3.位图- 比较存在与否
		    							1.首先，我们需要为每个 key 生成同样长度的位图，表示所有的对象空间。然后，如果一个元素出现在该 posting list 中，我们就将位图中该元素对应的位置置为 1。这样就完成了posting list 的位图改造。
		    							2. and 位运算（复习一下 and 位运算：0 and 0 = 0； 0 and 1 = 0； 1 and 1 = 1）。由于位图的长度是固定的，因此两个位图的合并运算时间代价也是固定的。并且由于 CPU执行位运算的效率非常快，因此，在位图总长度不是特别长的情况下，位图法的检索效率还是非常高的。
		    							3.局限性：
		    									1.图法仅适用于只存储 ID 的简单的 posting list。如果 posting list 中需要存储复杂的对象，就不适合用位图来表示 posting list 了。
		    									2.位图法仅适用于 posting list 中元素稠密的场景。对于 posting list 中元素稀疏的场景，使用位图的运算和存储开销反而会比使用链表更大。
		    									3.位图法会占用大量的空间。尽管位图仅用 1 个 bit 就能表示元素是否存在，但每个posting list 都需要表示完整的对象空间。如果 ID 范围是用 int32 类型的数组表示的，那一个位图的大小就约为 512M 字节。如果我们有 1 万个 key，每个 key 都存一个这样的位图，那就需要 5120G 的空间了。这是非常可怕的空间开销啊！

		    					在很多成熟的工业界系统中，为了解决位图的空间消耗问题，我们经常会使用一种压缩位图的技术 Roaring Bitmap 来代替位图。在数据库、全文检索 Lucene、数据分析 Druid 等系统中，你都能看到 Roaring Bitmap 的身影。

		    					4.roraing bitmap
		    							1.roraingnBitmap.png
		    			问题：复杂的业务查询（“北京”∪“上海”）∩“安卓”  .. “北京”∩“安卓”∩“学生”

		    					5.调整次序法	-- 具体分析，而不是简单的认为都是简单+
		    							1.有三个集合2，20，40，假设a包含于b包含于c
		    									1. A∩（B∩C）    20+40 =60，得到的结果集是 B，然后 B 再和 A 求交集，时间代价是 2 + 20 = 22。因此，最终一共的时间代价就是 60 + 22 = 82
		    									2.（A∩B）∩C 		 2 + 20 =22，得到的结果集是 A，然后 A 再和 C 求交集，时间代价是 2 + 40 = 42。因此，最终的时间代价就是 22 + 42 = 64。
		    							2.A∩（B∪C）=（A∩B）∪（A∩C）这种分配率前提是B/C的数据量大于A

		    					6.快速多路归并法
		    							1.对于多个posting list数据量差异不大时，利用跳表的性质，快速跳过多个元素，加快多路归并的效率。这种方法，我叫它"快速多路归并法"
		    							2.设置一个max值，对比不同路值和max关系，直接跳跃到大于等于对应的元素；循环完一轮后，如果每个list都命中，则取出，否则取第一个list表中的下一个元素作为max,再次进行煦暖判断
		    							3.skipListFast.png

		    					7.预先组合法
		    							1.以将 key1+key2+key3 这个查询定义为一个新的组合 key，然后对应的 posting list 就是提前计算好的结果
		    							2.依赖预判

		    					8.缓存法
		    							1.最近最少使用替换机制，
		    								一个合适的实现方案是使用双向链表：当一个元素被访问时，将它提到链表头。这个简单的机制能起到的效果是：如果一个元素经常被访问，它就会经常被往前提；如果一个元素长时间未被访问，它渐渐就会被排到链表尾。这样一来，当缓存满时，我们直接删除链表尾的元素即可。

		    							2.“[双向链表-单链表导致断] + 哈希表”是一种常见的实现 LRU 机制的方案。
		    								我们向链表中插入元素时，同时向哈希表中插入该元素的 key，然后这个 key 对应的 value 则是链表中这个节点的地址(add加到队尾并记录当时的size)。这样，我们在查询这个 key 的时候，就可以通过查询哈希表，快速找到链表中的对应节点了。
		    					
		    		8.数据库检索 - 用B+树对 【 海量磁盘数据 】 建立索引
		    			问题：	1.无法将数据全部加载到内存   2.索引无法实施更新
		    					对于复杂的系统和业务场景，我们往往需要对基础的检索技术进行组合和升级。这就需要我们对实际的业务问题和解决方案十分了解。
		    				背景：一般来说，如果是随机读写，会有 10 万到 100 万倍左右的差距。但如果是顺序访问大批量数据的话，磁盘的性能和内存就是一个数量级的。
		    					操作系统一次会读写多个扇区，所以操作系统的最小读写单位是块（Block），也叫作簇（Cluster）。当我们要从磁盘中读取一个数据时，操作系统会一次性将整个块都读出来。因此，

		    					对于大批量的 [ 顺序读写 ] 来说，磁盘的效率会比随机读写高许多。并不是和内存比较 。 
		    					减少磁盘读取次数

		    					hash表不具备范围搜索能力

		    				1. 索引和数据分离
		    						将索引加载到内存中，将数据块放到磁盘；索引根据需求进行过滤筛选后，直接拿着position到磁盘后load数据
		    			问题：  如果索引很大，无法加载到内存呢？
		    						B+ 树给出了将树形索引的所有节点都存在 [ 磁盘 ] 上的高效检索方案，使得索引技术摆脱了内存空间的限制，得到了广泛的应用。

		    								1.让一个节点的大小等于一个块的大小。节点内存储的数据，不是一个元素，而是一个可以装 m 个元素的有序数组。这样一来，我们就可以将磁盘一次读取的数据全部利用起来，使得读取效率最大化。
		    								2.将所有的节点分为内部节点和叶子节点。尽管内部节点和叶子节点的数据结构是一样的，但存储的内容是不同的。
		    										1.内部节点仅存储 key 和维持树形结构的指针
		    										2.叶子节点仅存储 key 和对应数据，不存储维持树形结构的指针
		    										3.B+ 树还将同一层的所有节点串成了有序的双向链表，这样一来，B+ 树就同时具备了良好的范围查询能力和灵活调整的能力

		    						==> 完全平衡的 m 阶多叉树。所谓的 m 阶，指的是每个节点最多有 m 个子节点，并且每个节点里都存了一个紧凑的可包含 m 个元素的数组。

		    			问题：  B+树如何检索
		    							B+ 树可以作为一个完整的文件 [ 全部存储在磁盘 ] 中。当从根节点开始查询时，通过一次磁盘访问，我们就能将文件中的根节点这个数据块读出，然后在根节点的有序数组中进行二分查找
		    							****注意这里，不是说索引在磁盘上就不需要再次读取磁盘信息了。****
		    							因为 B+ 树只有 4 层，这就意味着我们最多只需要读取 4 次磁盘就能到达叶子节点。并且，我们还可以通过将上面几层的内部节点全部读入内存的方式，来降低磁盘读取的次数。

		    							那么真正的节点是在磁盘上吗？ -- 指数级
		    								对于一个 4 层的 B+ 树，每个节点大小为 4K，那么第一层根节点就是 4K，
		    								第二层最多有 400 个节点，一共就是 1.6M；
		    								第三层最多有 400^2，也就是 160000 个节点，
		    								一共就是 640M。对于现在常见的计算机来说，前三层的内部节点其实都可以存储在内存中，只有第四层的叶子节点才需要存储在磁盘中。
		    								这样一来，我们就只需要读取一次磁盘即可。这也是为什么，

		    								B+ 树要将内部节点和叶子节点区分开的原因。通过这种只让内部节点存储索引数据的设计，我们就能更容易地把内部节点全部加载到内存中了

		    			问题：	B+树如何动态调整

		    							1.插入
		    									1.从叶子节点开始，
		    											如果叶子节点未满，则直接插入到数组即可
		    											如果叶子节点满，则分裂；上一层分裂
		    									查看：insert.png

		    							2.删除
		    									如果节点数组较满，直接删除；
		    									如果删除后数组有一半以上的空间为空，那为了提高节点的空间利用率，该节点需要将左右两边兄弟节点的元素转移过来。
		    									可以成功转移的条件是，元素转移后该节点及其兄弟节点的空间必须都能维持在半满以上。
		    									如果无法满足这个条件，就说明兄弟节点其实也足够空闲，那我们直接将该节点的元素并入兄弟节点，然后删除该节点即可。


		    							B+ 树最大的优点在于，它提供了将索引数据存在磁盘中，以及高效检索的方案。这让检索技术摆脱了内存的限制，得到了更广泛地使用。

		    			问题：  范围查找为什么B+树使用先二分找到x,在遍历对比Y，而不是力两次二分，找到x，y取其中的元素呢？
		    							原因大概是涉及磁盘操作，顺序 io 的速度远大于随机 io，因此如果找 y 也使用二分搜索的话，io 成本高，消耗的时间大于顺利遍历。




		    		9.LSM树 Log Structured Merge Trees  === nosql技术
		    			问题：为什么日志系统主要用LSM树而非B+树？
		    						除关系型数据库之外，还有许多常见的大数据应用场景，比如，日志系统、监控系统。这些应用场景有一个共同的特点，
		    							1.数据会持续地大量生成
		    							2.相比于检索操作，它们的写入操作会非常频繁
		    							3.检索操作，往往也不是全范围的随机检索，更多的是针对近期数据的检索。
		    				
		    				1.批量写入代替随机写入
		    						1.磁盘的读写是以块为单位的，我们能否以块为单位写入
		    						2.当数据写入时，延迟写磁盘，将数据先存放在内存中的树里，进行常规的存储和查询。当内存中的树持续变大达到阈值时，再批量地以块为单位写入磁盘的树中
		    						3.LSM 树至少需要由两棵树组成，一棵是存储在内存中较小的 C0 树，另一棵是存储在磁盘中较大的 C1 树 。 这里只是假设
		    							在数据都能加载在内存中的时候，B+ 树并不是最合适的选择，它的效率并不会更高。因此，C0 树我们可以选择其他的数据结构来实现，比如平衡二叉树甚至跳表等
		    							取决于系统需要提供什么样的功能，如果系统需要提供高效的查询不需要范围scanC0用hashmap都可以，如果需要scan那么平衡树或者skiplist比较合适。leveldb是使用skiplist来实现的

		    						4.由于是内存中的叶子节点满之后才批量写入的，所以磁盘上的树无需留空余，都是满的，不存在随机；所以有着高效的利用率

		    				2.WAL 技术（Write AheadLog，预写日志技术）将数据第一时间高效写入磁盘进行备份 避免磁盘中数据因为宕机丢失

		    				3.将磁盘数据和内存数据合并 - rolling merge
		    						1.两个有序列表归并
		    						2.因为磁盘的顺序读写性能和内存是一个数量级的，这使得 LSM 树的性能得到了大幅的提升

		    				4.如何检索
		    						1.C0-C1
		    						2.如果一个数据在c0中没有找到，而在c1中找到分为两种情况
		    									1.数据被删除了，由于批量刷新所以c1没有及时更新；本质这个数据不存在，所以c1搜到的是过期数据。不合理
		    									2.在删除数据后，在c0上添加删除标记，此时如果进来查询，则直接返回null,无需再去c1去判断。<==打标 、提前中断  



		    			10.索引构建 - 搜索引擎如何为万亿级别网站生成索引
		    				问题：在一些超大规模的数据应用场景中，比如搜索引擎，它会对万亿级别的网站进行索引，生成的倒排索引会非常庞大，根本无法存储在内存中。这种情况下，我们能否像 B+ 树或者 LSM 树那样，将数据存入磁盘呢？
		    						
		    						1.如何生成大于内存容量的倒排索引？

		    									1.我们可以将大规模文档均匀划分为多个小的文档集合，并按照之前的方法，为每个小的文档集合在内存中生成倒排索引。
		    									2.将内存中的倒排索引存入磁盘，生成一个临时倒排文件。我们先将内存中的文档列表按照关键词的字符串大小进行排序，然后从小到大，将关键词以及对应的文档列表作为一条记录写入临时倒排文件。这样一来，临时文件中的每条记录就都是有序的了。
		    									3.多路归并  每个临时文件里的每一条记录都是根据关键词有序排列的，因此我们在做多路归并的时候，需要先将所有临时文件当前记录的关键词取出。如果关键词相同的，我们就可以将对应的 postinglist 读出，并且合并了。
		    									4.每个临时文件里的每一条记录都是根据关键词有序排列的，因此我们在做多路归并的时候，需要先将所有临时文件当前记录的关键词取出。如果关键词相同的，我们就可以将对应的 postinglist 读出，并且合并了。 ====> 思考：文件1 文件2 文件3，各自有序，如何全局排？归并排序，取出max,如果从文件1取到的值小于，则继续从文件1中取，并塞入到新的集合(最终集合 ) 直到大于等于max,则替换max，从下一个文件中取.... <=== 相当于有限的空间就像蓄水池一样排了序

		    								map reduce 来构建倒排索引是当时最成功的一个应用。在使用多台机器同时读取操作

		    						2.如何使用磁盘上的倒排文件进行检索？
		    									1.词典在内存中，文档列表在磁盘上；现在词典中定位在从磁盘中加载
		    									2.如果词典过大，无法加载到内存中，则使用B+(部分索引在内存中，部分在节点叶子在磁盘上)；

		    									1.如果查找到的posting list非常大，导致无法加载到内存中，那么如何处理？(热门数据上亿..)
		    											1.问题在本质上和词典无法加载到内存中是一样的。而且，posting list 中的数据也是有序的。因此，我们完全可以对长度过大的 posting list 也进行类似 B+ 树的索引，只读取有用的数据块到内存中，从而降低磁盘访问次数。
		    											包括在 Lucene 中，也是使用类似的思想，用分层跳表来实现 posting list，从而能将 posting list 分层加载到内存中。而对于长度不大的 posting list，我们仍然可以直接加载到内存中。


		    						3.核心原理
		    									1.尽可能使用内存 
		    											1.索引压缩		Lucene 中，就使用了类似于前缀树的技术 FST，来对词典进行前后缀的压缩，使得词典可以加载到内存中。
		    									2.将大数据集合拆成多个小数据集合来处理
		    									3.缓存


		    						问题：在哈希表过大无法存入内存的情况下，我们是否还有可能使用其他占用内存空间更小的数据结构，来将词典完全加载在内存中？有序数组和二叉树是否可行？为什么？
		    									1.hash表为什么占用内存？
		    												HashMap的实例具有两个影响其性能的参数：初始容量和负载因子。容量是哈希表中存储桶的数量，初始容量只是创建哈希表时的容量。负载因子是在自动增加其哈希表容量之前允许哈希表获得的满度的度量。当哈希表中的条目数超过负载因子和当前容量的乘积时，哈希表将被重新哈希（即，内部数据结构将被重建），因此哈希表的存储桶数约为两倍。

															通常，默认负载因子（.75）在时间和空间成本之间提供了一个很好的权衡。较高的值会减少空间开销，但会增加查找成本（在HashMap类的大多数操作中都得到体现，包括get和put）。设置其初始容量时，应考虑映射中的预期条目数及其负载因子，以最大程度地减少重新哈希操作的次数。如果初始容量大于最大条目数除以负载因子，则将不会发生任何哈希操作。


		    									2.有序数组：
		    											  1.将字符串按照字典序在数组中排序好，并且是紧凑存储的(存string|length)
		    												索引数组很简单，就一个元素，存每个词项在字符串数组中的偏移量。比如[0，5，18]这样。
														   二分查找时，从数组中间开始，读出偏移量，然后从str数组中取出这个词项，和查询的词对比，看看是否相等。
														   如果不等，那么就继续二分查找，往左或往右，取出下一个字符串比较。因此，我们使用两个数组，就能实现所有数据的紧凑存储。从而提升了内存的使用率。

														  2.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。

														  3.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。这就是使用数组的方案。

														  4.其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。这就是用非连续的空间，用树来组织和压缩的方案。



		    			11.更新索引
		    				问题：如果在访问的时候有新的文档id更新，能否简单的将id添加到关键字的posting-list中呢？ 不能，造成访问错误 ； 比如分页，删除后仍然可以查到，更新后不是最新内容 <-- 并发
		    						1.锁  											加上“锁”之后会带来频繁的读写锁切换，整个系统的检索效率会比无锁状态有所下降。
		    						2.DoubleBuffer（双缓冲）机制						存中同时保存两份一样的索引，一个是索引 A，一个是索引 B。如果访问A，那么更新B。当B更新完完成后，利用原子操作切换到B上，把B作为只读索引，再去更新A

		    						3.避免切换太频繁，我们并不是每来一条新数据就更新，而是积累一批新数据以后再批量更新

		    				问题：资源开销成倍，解决方案	== 全量索引结合增量索引
		    						1.全量索引作为只读且不加锁进行检索，新接受到的数据单独创建一个增量索引，查询发生时，将全量索引和增量索引合并作为总的结果输出
		    						2.结合doublebuffer将增量索引进行两份存储更新
		    						3.增加删除列表

		    				问题：如果增量索引增长快速，体积膨胀如何处理？
		    						1.完全重建法
		    								1.大部分规模不大的检索系统
		    						2.再合并法
		    								1.将全量旧的索引(只读)和新的增量索引(可读可写)合并
		    						3.滚动合并法
		    								1.如果全量索引和增量索引的量级差距过大，那么再合并法的效率依然不高。
		    								2.两个倒排索引合并的过程中，只有少数词典中的关键词和文档列表会被修改，其他大量的关键词和文档列表都会从旧的全量索引中被原样复制出来，再重写入到新的全量索引中，这会带来非常大的无谓的磁盘读写开销。因此，对于这种量级差距过大的全量索引和增量索引的归并来说，如何避免无谓的数据复制就是一个核心问题。
		    										1.原地更新法
		    										2.滚动合并法		就是先生成多个不同层级的索引，然后逐层合并

		    				问题：为什么在增量索引的方案中，对于删除的数据，我们不是像 LSM 树一样在索引中直接做删除标记，而是额外增加一个删除列表？

		    						  1.倒排索引和kv不一样，posting ist元素很多，每个元素都加标记代价太大。
		    						  2.一个文档可能会影响多个key，因此每个文档都要修改标记的话，读写操作会很频繁，加锁性能下降。
		    						  3.加上标记也没啥用，在posting list求交并的过程中，依然要全部留下来，等着最后和全量索引合并时才能真正删除。这样的话不如直接用一个delete list存着，最后求交集更高效。



		    			12.拆分索引

		    				问题：索引拆分是检索加速的一个重要优化方案，至于索引应该如何拆分，以及拆分后该如何检索，如何合并
		    						   1.业务拆分			方案和业务的耦合性太强，需要根据不同的业务需求灵活调整。
		    						   2.文档拆分			建立起多个倒排索引了，每个倒排索引是一个索引分片，由不同的索引服务器负责。每个索引分片只包含部分文档，所以它们的 posting list 都不会太长，这样单机的检索效率也就得到了提升。
		    						   						分片的数量也不宜过多。这是因为，一个查询请求会被复制到所有的索引分片上，如果分片过多的话，每台加载索引分片的服务器都要返回 n 个检索结果，这会带来成倍的网络传输开销。而且，分片越多，分发服务器需要合并的工作量也会越大，这会使得分发服务器成为瓶颈，造成性能下降。
		    						   						因此，对于索引分片数量，我们需要考虑系统的实际情况进行合理的设置
		    						   3.关键字拆分			将词典划分成多个分片，分别加载到不同的索引服务器上。每台索引服务器上的词典都是不完整的，但是词典中关键词对应的文档列表都是完整的。

		    						   						1.如果查询词很多并且没有被划分到同一个分片中，那么请求依然会被多次复制。
		    						   						2.关键词是高频词，那么对应的文档列表会非常长，检索性能也会急剧下降。
		    						   						3.新增文档的索引修改问题
		    						   						4.系统热点查询负载均衡的问题等。

		    						   	因此，除了少数的高性能检索场景有需求以外，一般我们还是基于文档进行索引拆分。这样，系统的扩展性和可运维性都会更好。水平拆分、垂直拆分
		    						   		  合理的索引拆分是分布式检索加速的重要手段，也是工业界的有效实践经验

		    				问题：KV 和 倒排索引区别？



		    			13.topk 搜索结果打分
		    						1.权重、拉伸、平滑、打分


		    						精准排序
		    							1.相关性 = TF*IDF  TF 是词频（TermFrequency），IDF 是逆文档频率（Inverse Document Frequency）= 词项的重要性和区分度
		    									1.词频：定义的就是一个词项在文档中出现的次数。换一句话说就是，如果一个词项出现了越多次，那这个词在文档中就越重要。
		    									2.文档频率：指的是这个词项出现在了多少个文档中。你也可以理解为，如果一个词出现在越多的文档中，那这个词就越普遍，越没有区分度。一个极端的例子，比如“的”字，它基本上在每个文档中都会出现，所以它的区分度就非常低。
		    							2.BM25
		    									1.词频和相关性的关系并不是线性的。也就是说，随着词频的增加，相关性的增加会越来越不明显，并且还会有一个阈值上限。当词频达到阈值以后，那相关性就不会再增长了
		    									2.引入了可调参数、文档长度、整体文档平均长度

		    							3.机器学习打分
		    									1.Score = w* x+ w* x+ w* x+ ..... 打分因子由机器学习给出
		    									2.逻辑回归模型、支持向量机模型、梯度下降树、深度神经网络模型（DNN）

		    						在实际系统中，我们不需要返回所有结果，只需要返回 Top K 个结果就可以。而不是全局排序 。。***
		    						这就是许多大规模检索系统应用的的 Top K 检索了。而且，我们前面的打分过程都是非常精准的，所以我们今天学习的也叫作精准 Top K 检索。


		    						打分和排序哪个更耗时？如何优化？



		    						非精准排序	- 检索结果的排序过程装上“加速器”   -  取舍
		    							1.高质量的检索结果并不一定要非常精准，我们只需要保证质量足够高的结果，被包含在最终的 Top K 个结果中就够了。这就是非精准 TopK 检索的思路		    										
		    							2.第一阶段，我们会进行非精准的 Top K 检索，将所有的检索结果进行简单的初步筛选，留下 k1 个结果，这样处理代价会小很多（这个阶段也被称为召回阶段）；
		    							  第二个阶段，就是使用精准Top K 检索，也就是使用复杂的打分机制，来对这 k1 个结果进行打分和排序，最终选出k2 个最精准的结果返回（这个阶段也被称为排序阶段）

		    							3.尽可能地将计算放到离线环节，而不是在线环节
		    									1. 根据静态质量得分排序截断 	- 不考虑检索结果和实时检索词的相关性，打分计算仅和结果自身的质量有关
		    											每个网站的质量分，当一个搜索词要返回多个网站时，我们只需要根据网站质量分排序，将质量最好的 Top K 个网站返回 <== 评级
		    									2. 根据词频得分排序截断
		    										    这种根据某种权重将posting list 中的元素进行排序，并提前截取 r 个最优结果的方案，就叫作胜者表。 权重 = 词频 + 静态质量得分

		    									3.使用分层索引
		    											同时考虑相关性和结果质量，用离线计算的方式先给所有文档完成打分，然后将得分最高的m 个文档作为高分文档，单独建立一个高质量索引，其他的文档则作为低质量索引。
		    							4.精准打分前，插入非精准打分 -- 粗排、 精排 


		    			14.空间检索
		    					用Geohash实现“查找附近的人、附近餐馆...”功能
		    				    问题：取出所有的人进行距离计算，并全排序实现的困难性

		    				    1.“查找附近的人”功能
		    				    		1.非精确位置查找 -  查找附近的人”和“检索相关的网页”这两个功能的本质是非常相似的。
		    				    			在这两个功能的实现中，我们都没有明确的检索目标，也就都不需要非常精准的检索结果，只需要 [ 保证质量足够高 ] 的结果包含在 Top K 个结果中就够了

		    				    			1.划分检索空间及编号
		    				    				1.一个空间就会被均匀地划分为四个子空间，这四个子空间，我们可以用两个比特位来编号。00、01、10、11.根据需求更细粒度的缩小范围

		    				    					1.区域有层次关系：如果两个区域的前缀是相同的，说明它们属于同一个大区域；
		    				    					2.区域编码带有分割意义：奇数位的编号代表了垂直切分，偶数位的编号代表了水平切分，这会方便区域编码的计算（奇偶位是从右边以第 0 位开始数起的）。
		    				    					==> 权限角色 000110根据固定位数进行代表不同层次
		    				    			2.如何快速查询同个区域的人
		    				    					1.区域编码能将二维空间的两个维度用一维编码表示，利用这个特点，我们就可以使用一维空间中常见的检索技术快速查找了 <== 数字化、降维、坐标、向量
		    				    	问题：这种非精准检索的方案，会带来一定的误差。
		    				    		 我们找到的所谓“附近的人”，其实只是和你同一区域的人而已，并不一定是离你最近的。比如说，你的位置正好处于一个区域的边缘，那离你最近的人，也可能是在你的邻接区域里。
		    				    		 这种不明确的应用中，误差是可以接受的。

		    				    		2.精确位置查找 - 游戏攻击范围
		    				    					1.以半径为准，划分最近平分区域
		    				    					2.区域编码可以根据奇偶位拆成水平编码和垂直编码这两块，
		    				    						如果一个区域编码是 0110，那它的水平编码就是 01，垂直编码就是 10。
		    				    						那该区域右边一个区域的水平编码的值就比它自己的大 1，垂直编码则相同。
		    				    						因此，我们通过分解出当前区域的水平编码和垂直编码，对对应的编码值进行加 1 或者减 1 的操作，就能得到不同方向上邻接的 8 个区域的编码了

		    				    		3.人将经纬度 [ 39.983429，116.490273 ]   地球的纬度区间是[-90,90]，经度是[-180,180]；按照上面的二分标记01..


		    				    2.将经纬度坐标--01010001110010-- wx4g6y转换为字符串的编码方式，就叫作 Geohash 编码

		    				        问题：每个Geohash增大一位，就会扩大2倍。更灵活地调整自己期望的区域覆盖度了。
		    				        	  在许多系统的底层实现中，虽然都支持以字符串形式输入 Geohash 编码，但是在内存中的存储和计算都是以二进制的方式来进行的。


		    				    3.“查找最近加油站”功能
		    				    		特点：如果当前范围内查不到，系统就需要自动调整查询范围，直到能返回 k 个结果为止。
		    				    			1.最近k个结果
			    				    			1.如果查询不到，每次扩大范围
			    				    			2.将GeoHash去掉一位进行范围扩大查询，这样的话每次查询的区域单位都得到了大范围的提升，因此，查询次数不会太多 <--步长

			    				    			直到查询结果满足topk为止 、 如果要求精准检索，那每次查询就扩展到周围 8 个同样大小的邻接区域即可
			    				    		2.快速返回
			    				    			1.哈希表的倒排检索			每个层级的倒排表中都会出现全部的加油站，数据会被复制多次，这会带来非常大的存储开销
			    				    			2.数组或二叉检索树            由于数组和二叉检索树都可以支持范围查询一份粒度最细的索引就可以了
			    				    					1.在检索完 wx4g6yc8 这个区域编码以后，如果结果数量不够，还要检索wx4g6yc 这个更大范围的区域编码，我们只要将查询改写为“查找区域编码在 wx4g6yc0至 wx4g6ycz 之间的元素”，

			    				    										问题：从头开始进行二分查找，不能充分利用上一次已经查询到的位置信息，这会带来无谓的重复检索的开销
			    				    			3.四叉树
			    				    					1.每四个就是 00、01、10、11
			    				    					2.非满四叉树 - 分裂 <===  实际使用 ***
			    				    		3.用前缀树优化 GeoHash 编码的索引====对于字符串的检索，有一种专门的数据结构，叫作前缀树（Trie 树）

			    				    		4.对于更高维度空间的最近邻检索，我们也可以使用类似的检索方案来划分空间。比如说，在三维空间中，八叉树就是常见的检索方案。那拓展到更高的维度，如 k 维，我们还可以使用 k-d 树（K-Dimensional Tree）来检索

		    			15.最近邻检索
		    				问题：许多文章的内容是非常相似的，它们可能只有一些修饰词不同。如果在搜索结果或者推荐结果中，我们将这些文章不加过滤就全部展现出来，那用户可能在第一页看到的都是几乎相同的内容
		    					1.将关键字在doc中抽象为n维向量
		    					2.对n维向量进行距离计算(余弦、欧式)，即相似度计算
		    				问题：在高阶维度的k-d精确查找中，性能损耗是严重的
		    					1.近似最近邻检索
		    							1.局部敏感哈希（Locality-Sensitive Hashing)而不是完全hash:整体相似度高的两篇文档，通过哈希计算以后得到的值也是相近的.将高维空间中的点映射成低维空间中的一维编码
		    							2.n次划分  一条线分割左右，将靠近的点放在一起；如果多条线呢，这种分割就越接近真实。0-1 00-11....===n次超平面划分
		    				问题：丢失了权重
		    					1.SimHash ： 使用一个普通哈希函数代替了 n 次随机超平面划分，并且这个普通哈希函数的作用对象也不是文档，而是文档中的每一个关键词
		    					2.权重 - 拉伸

		    				===> 要找满足条件的，质量最好的，又不能找相似的(相同的-去重)。

		    				抽屉原理：过滤相似网页
		    					1.如果我们有 3 个苹果要放入 4 个抽屉，就至少有一个抽屉会是空的。那应用到检索上，Google 会将哈希值平均切为 4 段，如果两个哈希值的比特位差异不超过 3个，那这三个差异的比特位最多出现在 3 个段中，也就是说至少有一个段的比特位是完全相同的！因此，我们可以将前面的查询优化为“有一段比特位完全相同的文档会被召回



		    				用矢量积实现拍照识花功能 以图搜图、拍图识物
		    					1.局部敏感哈希由于哈希函数构造相对比较简单，往往更适合计算字面上的相似性（表面特征的相似性），而不是语义上的相似性（本质上的相似性）。
		    						即便是面对同一种花，不同的人在不同的地点拍出来的照片，在角度、背景、花的形状上也会有比较大的差异

		    					2.K-Means 算法（K- 平均算法）--聚类算法 - 类内的点到类中心的距离均值总和最短
		    						1.随机选择 k 个节点，作为初始的 k 个聚类的中心；
		    						2.针对所有的节点，计算它们和 k 个聚类中心的距离，将节点归入离它最近的类中；
		    						3.针对 k 个类，统计每个类内节点的向量均值，作为每个类的新的中心向量；
		    						4.重复第 2 步和第 3 ，重新计算每个节点和新的类中心的距离，将节点再次划分到最近的类中，然后再更新类的中心节点向量。
		    						经过多次迭代，直到节点分类不再变化，或者迭代次数达到上限，我们停止算法。
		    					
		    					3.如何使用矢量积压缩向量
		    						1. 1024 维的向量为例，因为每个向量维度值是一个浮点数（浮点数就是小数，一个浮点数有 4 个字节），所以一个向量就有 4K 个字节
		    						2.用聚类中心的向量代替聚类中的每个向量
		    						3.想要压缩向量，我们往往会使用向量量化（Vector Quantization）技术。其中，我们最常用的是乘积量化（Product Quantization）技术
		    						4.量化指的就是将一个空间划分为多个区域，然后为每个区域编码标识
		    						5.乘积指的是高维空间可以看作是由多个低维空间相乘得到的。我们还是以二维空间<x,y> 为例，它就是由两个一维空间和相乘得到。类似的还有，三维空间 <x,y,z> 是由一个二维空间 <x,y> 和一个一维空间相乘得到
		    						6.那将高维空间分解成多个低维空间的乘积有什么好处呢？它能降低数据的存储量
		    							x 轴上有 4 个点 x1 到 x4，y 轴上有 4 个点 y1到 y4，这四个点的交叉乘积，会在二维空间形成 16 个点。但是，如果我们仅存储一维空间中，x 轴和 y 轴的各 4 个点，一共只需要存储 8 个一维的点，这会比存储 16 个二维的点更节省空间
		    						7. 1024 维的浮点数向量，那我们可以将它分为 4 段，这样每一段就都是一个 256 维的浮点向量。我们可以用 1 至 256 作为 ID，来为这 256 个聚类中心编号。聚类 ID 是从 1 到 256 的，所以我们只需要 8 个比特位就可以表示这个聚类 ID
		    				    
		    				    4.如何计算查询向量和压缩样本向量的距离（相似性）？
		    				    	1.向量查询.png
		    				    	2.你会看到，原本两个高维向量的复杂的距离计算，被 4 次 O(1) 时间代价的查表操作代替之后，就变成了常数级的时间代价。因此，在对压缩后的样本向量进行相似查找的时候，我们即便是使用遍历的方式进行计算，时间代价也会减少许多
		    				    5.如何对乘积量化进行倒排索引？
		    				    	1.乘积量化进行倒排索引.png
		    				    	2.聚类查询.png


		    				  问题 ：乘积量化的方法来压缩样本向量？
		    				         k近邻查询？


		    			16.高性能检索设计
		    					1.todo:<123-检索技术核心> 16之后的特别加餐


		    			17.存储系统    
		    					1.LevelDB 是基于 LSM 树优化而来的存储系统。都做了哪些优化？
		    							1.LSM 树会将索引分为内存和磁盘两部分，并在内存达到阈值时启动树合并
		    							2.数据在内存中如何高效检索？数据是如何高效地从内存转移到磁盘的？以及我们如何在磁盘中对数据进行组织管理？还有数据是如何从磁盘中高效地检索出来的？

		    					2.todo:<123-检索技术核心> 17



		    			18.搜索引擎搜索过程
		    					1.搜索引擎核心架构.png
		    					2.架构组成
		    						 1.爬虫系统				抓取到的网页存放在基于 LSM 树的 HBase 中，以便支持数据的高效读写
		    						 2.索引系统
		    						 			索引拆分     计算和筛选，分别分离出高质量和普通质量的网页集合
		    						 			索引构建     在确认了索引的分片机制以后，我们可以使用 Map Reduce 服务，来为每个索引分片生成对应的任务，然后生成相应的倒排索引文件
		    						 			索引更新     滚动更新
		    						 3.检索系统

		    					3.查询分析
		    						 查询分析.png
		    						 混合粒度检索
		    					4.检索纠错
		    						  1.检索纠错.png
		    					5.检索召回
		    						  1.最小距离窗口



		    			19.广告引擎
		    				 现象：强大的工程和算法让现在的互联网广告能做到千人千面。
		    				 	  最常见的，我们在打开网站的一瞬间，广告系统就会通过实时的分析计算，从百万甚至千万的广告候选集中，为我们这一次的广告请求选出专属的广告，只需要 0.1 秒就能完成。
		    				 	1.广告请求返回
				    				 1.对用户、网站进行kv保存
				    				 2.广告主在投放广告时，为了保证广告的后续效果，往往会进行广告设置，也就是给广告投放加上一些定向投放的条件
				    				 3.收集广告的后续监测数据，比如说是否展现给了用户，以及是否被用户点击等后续行为。那有些后续行为还涉及广告计费，

				    			2.实现
				    				1.标签检索
				    						1.将标签加入过滤列表
				    								1.TF-IDF 算法中计算 IDF 的方式，找出区分度低的标签，不将它们加入倒排索引
				    								2.将这些标签加入“过滤列表”中，然后在倒排索引中检索出结果以后，加上一个过滤环节，也就是对检索结果进行遍历，在遍历过程中使用“过滤列表”中的标签进行检查，这样就完成了标签是否匹配的判断。  --- 量的问题
				    						2.用标签进行索引分片

				    				2.向量检索：提供智能匹配能力
				    						聚类 + 倒排索引 + 乘积量化


				    				3.打分排序：用非精准打分结合深度学习模型的精准打分
				    						我们要返回 Top K个结果，但是在展示广告业务中，广告引擎往往最后只会返回一条广告结果。因此，对于最后选出来的这一条广告，我们希望它和用户的匹配越精准越好
				    						打分排序.png

				    				4.索引精简：在索引构建环节缩小检索空间
				    						相对于搜索添加了过滤审核、排期未到、预算受限....


		    			20.推荐引擎 - 没有搜索词强约束
		    				问题：没有搜索词，所以推荐引擎并不能直接得知用户的意图和喜好

		    					1.用户画像：推荐引擎会收集用户对不同文章的行为数据，包括曝光、点击、阅读、收藏、点赞和评论等等 - 离线 - 用户画像 - 不同标签 - 不同权重，随着时间衰减
		    					2.文章画像：提取文章中的关键词以外，更多的要对文章中的内容做语义分析工作，比如，文章分类、主题词提取、主题提取等等
		    					3.算法召回
		    							1.基于统计的静态召回算法   		
		    									1.提前统计好点击量最大、评论最多、收藏最多、收藏率上升最快的文章等		线上环节将这些热门文章推荐给所有用户。它比较适合作为个性化召回不足时候的补充方案
		    							2.个性化召回算法
		    									1.基于内容的召回（Content Based）	基于标签的召回可能会漏掉许多候选集合。因此，我们可以使用向量空间模型，将标签匹配改为高维向量空间的最近邻检索问题
		    											内容的召回在数据依赖、个性化和冷启动方面的优缺点
		    												优点：不需要其他用户数据，可以针对小众用户给出个性化的推荐，以及可以推荐冷启动的新文章
		    												缺点：依赖于用户画像系统和文章画像系统，无法挖掘出用户的潜在兴趣，以及无法给冷启动的新用户推荐文章



		    									2.基于协同过滤的召回（CollaborativeFiltering  依赖内容本身来进行推荐，而是基于大众用户和这篇文章的互动关系来进行推荐。
		    											一类是传统的基于数据统计的 Memory-based 的协同过滤算法，也叫做基于邻域的算法，
		    												1.基于用户的协同过滤（User CF，UserCollaboration Filter）
		    															1.找到和你最相似的一批用户		
		    																	每个用户对每篇文章的喜爱程度（具体可以通过用户的点击次数、收藏、评论和转发等行为计算出来
		    																	每个用户浏览过的文章形成n维向量，寻找相近用户，就是寻找最近的n维向量
		    																	topk用户取出对应的item并按照item权重得到最终分数
		    																	topk文章

		    																	耗时问题解决
		    																		1.相似计算放在离线环节。
		    																		2.向量检索来近似地完成计算更新		聚类 + 倒排索引 + 乘积量化

		    															2.将这批用户看过，但你没看过的文章推荐给你

		    												2.基于物品的协同过滤（Item CF，Item Collaboration Filter）；

		    									基于物品的协同过滤算法更注重于用户的兴趣传承，
												而基于用户的协同过滤算法会更注重于社会化的推荐。因此，新闻资讯类的 App 更倾向于使用基于用户的协同过滤算法，而电商平台更倾向于使用基于物品的协同过滤算法。


		    											另一类是升级版的基于模型的 Model-based 的协同过滤算法
		    					4.混合推荐
		    							1.混合推荐.png
		    							2.每种召回方案都会返回大量的候选集，这会使得系统难以承受排序计算的代价。为了解决这个问题，推荐引擎中采用了 【 分层打分 】 过滤的排序方式  === 重复呢？









		问题：
			1.刚发布的文章为什么能被搜到
				这就是一个典型的检索实战题，它主要用到的知识就是索引更新。在工业界中实现索引更新的时候，为了追求更高的检索性能，我们一般不会直接对索引加锁，
				而是会利用“双 buffer 机制”来实现索引更新。
				但是像搜索引擎这样万亿级网页的索引规模，无法直接使用“双 buffer 机制”来更新，需要使用“全量索引结合增量索引”方案来更新索引。

			2.当我们要在系统中使用数据库来进行存储和检索时，那么是使用关系型数据库好呢？还是选择 NoSQL 好呢？
		   		这就需要我们对于数据库的检索技术 B+ 树，和 NoSQL 中的LSM 树有一定的了解。比如，在数据被频繁写入、较少查找的日志系统和监控系统中，我们更应该使用 NoSQL 型数据库。

		   	3.对于区间值，取[x,y]有什么快速方法
		   		第一种方法是先二分查找x,然后二分查找y,x和y之间的元素
		   		第二种方法就是只二分查找x或者y，然后去顺序遍历，和另一个去比较。
		   		两种方法对于不同x和y效率应该是不一样的，有些情况第一种方法较快，有些情况第二种方法较快，想问下老师工业界中的产品(redis)是如何实现区间查询的呢？

		   		对于数组怎么范围的问题:对x和y分别做两次二分查找，时间代价为log（n）+log（n）。
		   		而对x做二分查找，再遍历到y，时间代价为log（n）+（y－x）。发现没有，我们完全可以根据log（n）和（y－x）的大小进行预判，哪个更快就选哪个！
		   		当然，除非y－x非常小，否则一般情况下log（n）会更小
		   	4.打删除标记，一方面快速，二方面减少重新申请损耗，但要注意，未清理的空间会很快被新请求占用，而不会造成资源浪费不回收
		   	5.1.对于布隆过滤器的删除问题，的确无法直接删除。但也有带引用计数的布隆过滤器，存的不是0，1，而是一个计数。其实所有的设计都是trade off。应该视具体使用场景而定。比如一个带4个bit位计数器的布隆过滤器，相比于哈希表依然有优势。2.布隆过滤器是否省空间，要看怎么比较。布隆过滤器 vs 原始位图:原始位图要存一个int 32的数，就要先准备好512m的空间的长数组。布隆过滤器不用这么长的数组，因此比原始位图省空间。布隆过滤器 vs 哈希表:假设布隆过滤器数组长度和哈希表一样。但是哈希表存的是一个int 32，而布隆过滤器存的是一个bit，因此比同样长度的哈希表省空间。当然，如果哈希表也改为只存一个bit的数组，那么他们的大小是一样的。这时候就是你说的多个哈希函数的作用场景了。其实，你会发现，只存一个bit的哈希表，其实也可以看做是只有一个哈希函数的布隆过滤器。很多时候，布隆过滤器，哈希表，还有位图，它们的边界是模糊的。我们最重要的是了解清楚他们的特点，知道在什么场景用哪种结构就好了。3.roaring bitmap是一个优秀的设计。我在基础篇的加餐中会和大家分享。在这里，我也说一下它和布隆过滤器的差异:布隆过滤器 vs roaring bitmap:所有的设计都是trade off。roaring bitmap尽管压缩率很高，还支持精准查找，但是它放弃的是速度。高16位是采用二分查找，array container也是二分查找。因此，在这一点上布隆过滤器是有优势的。此外，它还不能保证压缩空间，它的空间会随着元素增多而变大，极端情况下恢复回bitmap。
而布隆过滤器保持了高效的查找能力和空间控制能力，但是放弃了精准查找能力，精准度会随着元素增多而下降。因此，尽管都是对bitmap进行压缩，但是两者的设计思路不一样，使用场景也不同。在不要求精准，但是要求快速和省空间的场景下，布隆过滤器是不错的选择

			5.为什么说分查找还不够快，那么还可以建立倒排索引，直接以编码作为key？
			6.理解各个结构的特点及使用场景
			7.图片为什么是高维的？如何降维处理
