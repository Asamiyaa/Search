

分布式遇到那些问题？
			https://ningg.top/zookeeper-positioning/
zookeeper解决方案：
			1.利用外部共享存储，实现多进程协作，要求共享存储提供有序访问
			2.ZooKeeper 可以保证如下分布式一致性特性：
				顺序一致性：同一个 Client 发起的事务请求，严格按照发起顺序执行
				原子性：事务请求，要么应用到所有节点，要么一个节点都没有应用
				单一视图：Client 无论连接到哪个节点，看到的服务端数据都是一致的（Note：不准确，其实是最终一致性）
				可靠性：事务一旦执行成功，状态永久保留
				实时性：事务一旦执行成功，Client 并不能立即看到最新数据，但 ZooKeeper 保证最终一致性
			3.应用场景
				ZK 目标：简化分布式应用开发中，多进程协作问题。为分布式应用，提供高效、可靠的分布式协调服务（基础服务），例如：
				统一的命名服务
				分布式锁
				进程崩溃检测
				Leader 选举
				配置管理：配置变更时，及时下发到各个 Client。


其他解决方案取舍
能从这些解决中应用到代码中哪些？



zookeeper
1.分布式协调系统
		1.zookeeper  2.consul  3.ectd

2.问题
		1.如何设计一个本地数据节点
		2.分布式环境下节点之间如何通讯
		3.如何从0到1设计一个RPC子系统
		4.如何使用数据一致性协议保证数据的高可用
		5.如何在数据一致性和系统性能之间取舍


3.是什么
	分布式协调系统。设计目标：将那些复杂且容易出错的分布式协同服务封装起来，抽象出一个高效可靠的原语集,并以一系列简单的api提供给用户使用
4.应用场景
		1.配置管理
		2.DNS服务
		3.组成员管理
		4.各种分布式锁
		适用于存储和协同相关的关键数据，不适用于大量的数据存储

5.特点
		1.层次模型和key-value模型是两种主流模型，zookeeper使用前者 
				1.便于表达数据之间层次关系
				2.便于为不同的应用分配独立的命名空间

		2.不同于文件系统，每个系欸但都可以保存数据
		3.每个节点都有一个版本，版本从0开始
		4.提供类似unix文件系统的api
		5.提供接受数据更新机制 event / watch
		6.写操作通过leader/master完成。读操作可以通过任意节点。一致性弱，可能读到旧数据
		7.复制状态机做容错
		8.使用zab数据一致性协议 chubby使用paxos协议

	api特点
		1./a/x表示znode a的子节点x
		2.znode只支持全量写入和读取，不支持部分
		3.所有api都是wait-free,正在执行的api不会影响其他api的完成
		4.利用api完成多种类似锁的复杂的分布式协同机制

	znode特点 
		1.持久性   
		2.临时性		client宕机或者指定timeout时间内没有给zookeeper集群发消息就会消失
		3.顺序持久
		4.顺序临时

6.运维
		1.ls -R /.. 递归显示
		2.ls -w /...监控节点
		3.zookeeper api 

7.分布式锁   1.处理同一个节点  2.冲突
		1. cli1  -- create -e /lock(加锁成功)										-- 退出
		   cli2  							-- create -e /lock   stat -w /lock（监控）      -- 收到watch事件   --再创建成功
		   -e临时节点

		2.api实现
			1.避免羊群效应
				把锁请求者按照后缀数字进行排队，后缀数字小的锁请求者者先获得锁。如果都watch,就会在所删除的时候通知所有造成竞争
			==> 每个锁请求者watch它前面的锁请求者。这样做还让锁分配具有公平性，先到先得原则
			源码中的测试代码示例

		

8.协同 watch
		1.类似于分布式锁
		2.ls -w /works  client1 create /works/w1  clien2 create /works/w2 这样就可以监听到worker变化，完成协同

9.架构
		1.
		2.session:client于任意其中一个节点建立session,当超过时间后自动断开，又重新和另一个节点建立连接
		3.一个leader多个follower,follower只能处理读请求，follower收到写请求会把改请求转发给leader来处理
		4.数据一致性
				1.全局可线性化写入
				2.客户端FIFO顺序

10.count
		  set -s -v 0 --- v1     再执行由于‘ 版本不匹配 ’所以错误 条件机制
		  set -s -v 1 --- v2

11.队列
		1.本质就是调用zk的api

12.选举
		1.

13.curator 
		1.上面的场景需要大量的catch和case处理，cuator进行了封装简化 ， 封装了zookeeper api

14.生产配置
		1.zoo.cfg
				1.clientport
				2.dataDir : 快照
				3.dataLogDir ： 事务日志文件，提交事务之前，保证事务日志落盘

				1.内存 保存data-tree 8g
				2.cpu  消耗一般      2c
				3.存储 影响事务提交，独立的存储设备  独立的SSD


		2.zookeeperObserver跨区域部署
				1.因为observer不参与提交和选举的投票过程，所以往集群中添加observer来增加读性能
				2.普通交互：follower接受写请求 -- forward leader -- leader会发送propose -- flower发送ack -- leader发送comit给f
				   observe交互：..........     ______________________________________________________  -- leader发送inform给f

		3.启动指定配置文件 
				1.bin/zkServer.sh start conf/zoo_reconf.cfg


15.监控
		1.echo ruok | ncat localhost 2181（是否ok）
		2.echo conf | ncat locaohost 2181
		3.echo stat | ncat locaohost 2181
		4.echo dump | ncat locaohost 2181（查看临时节点）
		5.echo wchc | ncat locaohost 2181（查看watch）

		1.JMX		使用jconsole来查看，直接在服务器上的java对应下的bin启动查看 本地  --- 远程访问 export JMXport = xxx

16.高可用
		1.动态配置实现不中断服务的集群成员变更（dynamic reconfiguration）,避免覆盖
				1.
				2.
		2.服务发现		<=== TODO:对比自己当前的网关注册 本质=db + 方法  ===借助curator完成自定义协同服务开发扩展
				1.特点
						1.服务注册
						2.服务实例获取
						3.服务变化通知机制

						curator-x-discovery
								1.basepath
									2.服务
										3.实例

								2.实现
									1.serviceproviderBuilder -- serviceprovider 
												1.是provideStragy和instanceProvider的facade
									2..................................cache    <=== 对于这种不经常变动的配置进行cache

									1.nodeCache  
												 1.listner机制监控znode数据update操作，用户注册监听在cache上监听。
												 2.弥补了watch机制创建一次只监听一次的问题
									2.pathcache
									3.container
												1.下挂子节点挂掉，container就会清理掉
												2.




		3.存储
				1.data-tree在memory中，日志文件（事务文件/快照文件）在磁盘上
				2.zxid:每个data_tree都会作为一个事务执行。每个事务一个zxid。64位整数。zxid有两部分。高4个字节保存epoch,低4保存counter  -- 事务记录

17.工业级应用
			1.kafka与zookeeper交互 -- 选举
				1.broker node registry 
						1.ls可以看到许多kafka对应的节点信息
						2.

			2.分布式锁
				1.通信不确定
				2.节点失败

				单机下的mutex指令完成锁，分布式下除了mutex要有心跳查看持有锁节点的健康。
				使用key为某一个固定前缀如/lock/xxxx的key-value来表示锁请求。锁请求的revision越小，越先获得锁。为了避免羊群效应，每个等待的锁请求watch它前面的锁

			3.分布式下乱序（到达时间 + 处理时间）
				1.sequencer
					  sequencer.png  通过添加sequence和验证sequence来完成消息是否有效，是否执行
				2.lock delay


18.原理
			0.zookeeper源码

			1.Paxos协议	(Asychronous non-Byzantine Model) ==> 拜占庭问题[大多数满足]
				1.一个分布式环境中由若干个agent组成，agent之间通过传递 '消息 '进行通讯.而不是像内存共享等实现
				2.agent以任意的速度运行，agent可能失败和重启。但agent不会出byzantine fault(不符合预期的行为)
				3.消息需要任意长的时间进行传递，消息可能丢失，消息可能重复。但是消息不会corrupt(被劫持篡改)

				==> multiPaxos协议简化
			2.zab协议(zookeeper)
			3.raft协议(etcd系统)
				1.leader概念  处理任何操作之前必须选举一个leader,选举一个leader至少需要一轮的消息交换。但在选取leader之后，处理每个客户端只需要一轮信息交换
				2.描述了日志复制，选举机制和成员变更这些复制状态机的方面，paxos系统需要自己实现这些机制

				raft复制状态机.png
						1.k8s做服务发现和配置管理
						2.openstack做配置管理和分布式锁
						3.ROOK做编排引擎

				etcd vs zookeeper
						1.二者覆盖基本一样的协同服务场景
						2.zookeeper需要把所有数据都加载到内存，约几百兆。 etcd使用bbolt存储引擎，可以处理几个GB的数据

				etcd采用KV模型，所有的key都是扁平的命名空间(zookeeper等是树形,目录形)
				    生成历史版本 整条记录 key createRevision modRevision version value 组成
				    三元组<major,sub,type>

				range

			4.LSM架构 vs b+
				1.LSM存储mvcc的key-value对。每次更新一个key-value就会生成一个新版本，删除也是
				2.mysql的WAL就是这个应用
				3.多层所以要view进行merge
				4.bloom filter加快上面的merge查找操作

			5.本地存储
				1.内存数据结构+wal(事务日志)方案
				2.




endpoint
