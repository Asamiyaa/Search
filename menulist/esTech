参考：https://segmentfault.com/a/1190000039018845
     github上对应的api验证和pdf 
     https://int32.me/blog/2017/03/12/elasticsearch/
     
     手动验证


elasticSearch
	1.入门
		1.是什么
				源的分布式搜索与分析引擎, 提供了近实时搜索和聚合两大功能.
				我们只需要大概认为Index就是一个db，document就是一行数据，field就是table的column，mapping就是table的定义，而document type就是一个table就可以了,type指的是一个index下使用的查询方式可以多个

		2.使用场景
				1.对比关系型数据库 - api
				2.java调用es : https://blog.csdn.net/feinifi/article/details/80799487

				3.es 全功能解决方案  ：  https://segmentfault.com/a/1190000022799288
					
					1.搜索服务
					3.数据分析 日志实时分析
					4.数据监控
					查询服务
					2.后端存储

			   4.什么时候使用
			   		1、典型搜索场景：闭着眼用它！
					2、典型日志分析场景：闭着眼用它！
					3、关系型数据库查询有瓶颈：考虑下用它！为啥是考虑？ES的优点在于查询，然而实践证明，在被作为数据库来使用，即写完马上查询会有延迟。
					4、数据分析场景：考虑下用它！为啥是考虑？简单通用的场景需求可以大规模使用，但在特定业务场景领域，还是要选择更加专业的数据产品，如复杂聚合，ClickHouse相比 Elasticserach 做亿级别数据深度聚合需求会更加合适。

		3.解决的问题，面临的问题，工业级解决方案


				面临问题
					索引，对于需要搜索的数据，如何建立合适的索引，还需要根据特定的语言使用不同的analyzer等。
					搜索，Elasticsearch提供了非常强大的搜索功能，如何写出高效的搜索语句？
					数据源，我们所有的数据是存放到MySQL的，MySQL是唯一数据源，如何将MySQL的数据导入到Elasticsearch？ ==> row based binlog
							github:go-mysql-elasticsearch  首先使用mysqldump获取当前MySQL的数据，然后在通过此时binlog的name和position获取增量数据。

							binlog一定要变成row-based format格式，其实我们并不需要担心这种格式的binlog占用太多的硬盘空间，MySQL 5.6之后GTID模式都推荐使用row-based format了，而且通常我们都会把控SQL语句质量，不允许一次性更改过多行数据的。
							需要同步的table最好是innodb引擎，这样mysqldump的时候才不会阻碍写操作。
							需要同步的table一定要有主键，好吧，如果一个table没有主键，笔者真心会怀疑设计这个table的同学编程水平了。多列主键也是不推荐的，笔者现阶段不打算支持。
							一定别动态更改需要同步的table结构，Elasticsearch只能支持动态增加field，并不支持动态删除和更改field。通常来说，如果涉及到alter table，很多时候已经证明前期设计的不合理以及对于未来扩展的预估不足了。


		4.特点
			es
			  数据层
				Index：Elasticsearch用来存储数据的逻辑区域，它类似于关系型数据库中的db概念。一个index可以在一个或者多个shard上面，同时一个shard也可能会有多个replicas。
				Document：Elasticsearch里面存储的实体数据，类似于关系数据中一个table里面的一行数据。
				document由多个field组成，不同的document里面同名的field一定具有相同的类型。document里面field可以重复出现，也就是一个field会有多个值，即multivalued。
				Document type：为了查询需要，一个index可能会有多种document，也就是document type，但需要注意，不同document里面同名的field一定要是相同类型的。
				Mapping：存储field的相关映射信息，不同document type会有不同的mapping。
			  
			  服务层
			  	Node: 一个server实例。
				Cluster：多个node组成cluster。
				Shard：数据分片，一个index可能会存在于多个shards，不同shards可能在不同nodes。
				Replica：shard的备份，有一个primary shard，其余的叫做replica shards。

			  restful api 
			  	一个url表示一个特定的资源，譬如/blog/article/1，就表示一个index为blog，type为aritcle，id为1的document。



			lucence
				Document：用来索引和搜索的主要数据源，包含一个或者多个Field，而这些Field则包含我们跟Lucene交互的数据。
				Field：Document的一个组成部分，有两个部分组成，name和value。
				Term：不可分割的单词，搜索最小单元。
				Token：一个Term呈现方式，包含这个Term的内容，在文档中的起始位置，以及类型。



			es优势
				1、很简便的横向扩容，分布式的架构，可以轻松地对资源进行横向纵向扩缩容，可以满足不同数据量级及查询场景对硬件资源的需求。能由数百台到万台机器搭建满足PB级的快速搜索，也能搭建单机版服务小公司。
				2、查询速度快：ES底层采用Lucene作为搜索引擎，并在此之上做了多重优化，保证了用户对数据查询数据的需求。可"代替"传统关系型数据库，也可用于复杂数据分析，海量数据的近实时处理等
				3、相关性高：ES内部提供了完善的评分机制，会根据分词出现的频次等信息对文档进行相关性排序，保证相关性越高的文档排序越靠前。另外还提供了包括模糊查询，前缀查询，通配符查询等在内的多种查询手段，帮助用户快速高效地进行检索。
				4、功能点多但使用比较简便，开箱即用，性能优化比较简单
				5、生态圈丰富，社区活跃，适配多种工具。如下图，处理日志和输出到Elasticsearch，您可以使用日志记录工具，如Logstash（www.elastic.co/products/logstash），搜索和可视化界面分析这些日志，你可以使用Kibana（www.elastic.co/产品/ kibana），即传说中的ELK技术栈。另外当前主流的大数据框架也几乎都支持ES，比如Flink和ES就是个完美搭档。




	2.概念
			1.	_index / _type: 文档所属索引和type
			2.GET /_cat/c/kibana*?v&s=index 查看系统指标，并遵循restful  ?后面就是参数也就是where条件  ===> 这个关键字

					GET /_cat/indices?v&s=docs.count:desc
					GET /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt


					query: 用于包含查询使用到的语法
					match_all: 最简单的查询，获取索引所有数据，类似搜索 *。如：”query”:{“match_all”:{}}
					bool: 复合查询，可以包含多个查询条件，主要有(must,must_not,should)
					must: 用于包含逻辑与查询条件，即所有查询条件都满足才行
					must_not: 用于包含逻辑非查询条件，即不包含所有查询的条件数据
					should: 用于包含逻辑或查询条件，即其中有一个条件满足即可
					filter: 与must一样，包含的所有条件都要满足，但不会参与计算分值，查询速度上会提升不少
					match:匹配查询，用于匹配指定属性数据，也可以匹配时间，IP等特殊数据 注意： match匹配不会解析通配符，匹配的效果受到索引属性类型影响，如果索引属性设置了分词，那么match匹配也会分词匹配，他也不解析”“，但可以设置逻辑关系来处理
					operator: 匹配逻辑关系，默认是or，可设置为and，这样可达到精确匹配的效果
					query_string: 使用查询解析器来解析查询内容，如port:80 AND server:http。注意：此类型请求有很多选项属性，可以设置一些特殊的行为
					term: 查找包含在反向索引中指定的确切术语的文档
					terms: 筛选具有匹配任何条件的字段，如”terms” : { “user” : [“kimchy”, “elasticsearch”]}
					range: 将文档与具有特定范围内的字段相匹配。Lucene查询的类型依赖于字段类型，对于字符串字段，即TermRangeQuery，而对于number/date字段，查询是一个数字的范围。如：”range”:{“port”:{“gte”:10,”lte”:20,”boost”:2.0}}
					gte: 大于或等于
					gt: 大于
					lte: 小于或等于
					lt: 小于
					boost: 设置查询的boost值，默认值为1.0
					exists: 返回在原始字段中至少有一个非空值的文档，注意：”“,”-“这些都不算是空值
					prefix: 匹配包含带有指定前缀字段的字段(没有分析)，前缀查询映射到Lucene前缀查询，如：”prefix” : { “user” : “ki” }，查询user数据前缀为ki的doc
					wildcard: 匹配具有匹配通配符表达式的字段(未分析)的文档。支持(*通配符)是匹配任何字符序列(包括空的序列)和(?通配符)它匹配任何一个字符。注意，这个查询可能比较慢，因为它需要迭代多个术语。为了防止非常慢的通配符查询，一个通配符项不应该从通配符开始，或者?通配符查询映射到Lucene通配符查询。如：”wildcard” : { “user” : “ki*y” }
					regexp: regexp查询允许您使用正则表达式术语查询，意味着Elasticsearch将把regexp应用到该字段的标记器所产生的词汇，而不是该字段的原始文本。regexp查询的性能严重依赖于所选择的正则表达式，通配符往往会降低查询性能。如：”regexp”:{ “name.first”:”s.*y” }
					fuzzy: 模糊查询使用基于Levenshtein编辑距离的相似性。如：”fuzzy” : { “user” : “ki” }
					type: 过滤文档匹配所提供的文档/映射类型。如：”type”:{ “value” : “my_type” }
					ids:过滤只具有提供的id的文档。注意：这个查询使用了_uid字段，类型是可选的，可以省略，也可以接受一组值。如果没有指定类型，那么将尝试在索引映射中定义的所有类型。如：”ids”:{ “type” : “my_type”,”values” : [“1”,”4”,”100”] }。
					highlight: 允许在一个或多个字段中突出显示搜索结果，基于lucene plain highlighter。在默认情况下，高亮显示会将高亮显示的文本包装在 and ，可以通过设置pre_tags 与 post_tags来自定义，如：”highlight”:{ “pre_tags” : [““], “post_tags” : [““], “fields” : {“_all”:{}} }
					pre_tags: 自定义包含搜索关键字的前缀
					post_tags: 自定义包含搜索关键字的后缀
					fields: 用于指定要高亮的属性，_all表示所以属性都需要高亮，如：”fields”:{ “_all” : {} }，也可以指定具体属性 “fields”:{ “app” : {} }，也可以给每个属性单独指定设置 “fields”:{ “app” : {“fragment_size” : 150, “number_of_fragments” : 3} }
					highlight_query: 可以通过设置highlight_query来突出显示搜索查询之外的查询，通常，最好将搜索查询包含在highlight_query中。如：”highlight_query”:{ “bool”:{“must”:[{“query_string”:{“query”:app:apache,”analyze_wildcard”:True,”all_fields”:True}}]} }
					fragment_size: 用于指定高亮显示时，碎片的长度，如果过短，高亮内容会被切分为多个碎片。默认情况下，当使用高亮显示的内容时，碎片大小会被忽略，因为它会输出句子，而不管它们的长度
					number_of_fragments 用于指定高亮显示时，碎片的数量，如果指定为0，那么就不会产生任何片段
					from: 可以通过使用from和size参数来对结果进行分页，from参数指定您想要获取的第一个结果的偏移量
					size: 可以通过使用from和size参数来对结果进行分页，size参数指定要返回结果的最大数量
					sort: 允许在特定的字段上添加一个或多个排序，排序是在每个字段级别上定义的，用特殊的字段名来排序，然后根据索引排序进行排序，如”sort”: [ { “date”: { “order”: “desc” } } ],desc降序，asc升序
					aggs: aggs主要用于分类集合，可以将查询的数据按指定属性进行分类集合统计.如：”aggs”:{ “deviceType”:{ “terms”:{ “field”:”deviceType”, “size”:6 } } }
					field: 用于指定要分类的属性名称
					size: 用于指定分类集合的数量，即只集合前N名
			3.PUT mapping_test/_doc/1 只能是_doc或者_create
			4.查看 Mapping文件
				GET mapping_test/_mapping 类似于查看表结构

				   动态表
			5.POST dynamic_mapping_test/_search    ===> 通过 ‘ 查询对象 ’ 来查看而不是 url ,也就对应起来上面的关键字
				{
				  "query":{
				    "match":{
				      "newField":"someValue"
				    }
				  }
				}

			6.显示东西多，是因为相对于db来说，es将db自己的表组织底层、类似于jvm对class的操作信息进行了显示，定位使用有用的即可

			7.Text vs. keyword（移除了string）
				Text：会分词，然后进行索引

				       支持模糊、精确查询

				       不支持聚合

				keyword：不进行分词，直接索引

				       支持模糊、精确查询

				       支持聚合
			8.Analyzer
				Analyzer是由一个tokenizer、零到多个Token Filter、还有零到多个CharFilters构成的，也就是说一个Analyzer就是一个完整的解析模块。
				详见各个分词器：https://blog.csdn.net/i6448038/article/details/51509439
				按照一定规则，切分数据、替换、过滤
				Analyzer = tokenizer +filter ...
									  stop: 过滤on the a s this 等词
									  lowcase

				Analyzer 主要有三部分组成

				Character Filters
				针对原始文档处理, 例如去除 html

				Tokenizer
				按照规则切分为单词

				Token Filter
				将切分的单词进行加工, 小写, 删除 stopwords, 增加同义词等操作.

				Analyzer = CharFilters（0个或多个） + Tokenizer(恰好一个) + TokenFilters(0个或多个)

					- Standard Analyzer - 默认分词器，按词切分，小写处理
					- Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理
					- Stop Analyzer - 小写处理，停止词过滤(the,a,is)
					- Whitespace Analyzer - 按照空格切分，不转小写
					- Keyword Analyzer - 不分词，直接将输入当作输出
					- Patter Analyzer - 正则表达式，默认\W+(非字符分割)
					- Language - 提供了30多种常见语言的分词器
					- Customer Analyzer 自定义分词器

			9.Index Template和Dynamic Template
				设置副本等配置信息、设置索引配置信息、是否类型推断(数值、boolean...)
				setting -副本等deploy特性 	/mapping -类似于表的特性

			10.aggs / term 统计
					term：按照该字段进行分bucket ，类似于分组。
					嵌套 分组后同样可以嵌套aggs进行max/avg/min...等操作
					stats：全部指标
			11.bulk/mget/msearch批量操作
			12.倒排索引主要包含两个部分

				单词词典(Term Dictionary): 记录所有文档的单词, 记录单词到倒排列表的关联关系

				单词词典一般比较大, 可以通过 B+ 树或哈希拉链法实现, 以满足高性能的插入与查询
				倒排列表(Posting List)

					倒排索引项(Posting)

						文档 ID
						词频 TF - 该单词在文档中出现的次数, 用于相关性评分
						位置 Position - 单词在文档中分词的位置. 用于语句搜索 (phrase query)

							注意这是单词的位置, 而不是字节/字符位置.
						偏移 Offset - 记录单词的开始结束位置, 实现高亮显示

						字节/字符位置.
				可以配置有些字段不做倒排索引


			13.search api
					- URI Search
							1.泛查询			GET /movies/_search?q=title:2012
							2.Phrase查询		GET /movies/_search?q=title:"Beautiful Mind"
							3.Bool查询		GET /movies/_search?q=title:(Beautiful Mind)
											GET /movies/_search?q=title:(Beautiful AND Mind)  AND -> NOT %2B
							4.范围查询 ,区间写法		GET /movies/_search?q=title:beautiful AND year:[2002 TO 2018%7D
							5.通配          *
							6.模糊匹配&近似度匹配  GET /movies/_search?q=title:beautifl~1

					- Request Body Search
							1.分页
									POST /kibana_sample_data_ecommerce/_search
									{
									  "from":10,
									  "size":20,
									  "query":{
									    "match_all": {}
									  }
									}
							2.排序
							POST kibana_sample_data_ecommerce/_search
									{
									  "sort":[{"order_date":"desc"}],
									  "query":{
									    "match_all": {}
									  }

									}
							3.返回值过滤
							POST kibana_sample_data_ecommerce/_search
									{
									  "_source":["order_date"],
									  "query":{
									    "match_all": {}
									  }
									}
							4.操作函数
							GET kibana_sample_data_ecommerce/_search
									{
									  "script_fields": {
									    "new_field": {
									      "script": {
									        "lang": "painless",
									        "source": "doc['order_date'].value+'hello'"
									      }
									    }
									  },
									  "query": {
									    "match_all": {}
									  }
									}
							5.and
							POST movies/_search
									{
									  "query": {
									    "match": {
									      "title": {
									        "query": "last christmas",
									        "operator": "and"
									      }
									    }
									  }
									}

							6.or
							POST users/_search
									{
									  "query": {
									    "query_string": {
									      "fields":["name","about"],
									      "query": "(Ruan AND Yiming) OR (Java AND Elasticsearch)"
									    }
									  }
									}


									- `q` 指定查询语句, 使用 Query String Syntax
									- `df` 默认字段, 不指定时会对所有字段进行查询
									- `sort` 排序
									- `from` 和 `size` 用于分页
									- `profile: true`: 输出查询的执行计划


							操作符号含义
									- 指定字段 vs 泛查询
									// 泛查询(对所有字段应用各种类型的查询)
									GET movies/_search?q=2012

									// 指定字段
									GET movies/_search?q=title:2012


									- Term vs Phrase
									// 其中的 "Mind" 部分是泛查询(即此时是对文档中所有字段查询 "Mind")
									GET movies/_search?q=title:Beautiful Mind

									// Phrase查询: 使用引号
									GET movies/_search?q=title:"Beautiful Mind"


									- 分组与引号
									// 分组, Bool 查询
									GET movies/_search?q=title:(Beautiful Mind)

									// 引号, Phrase
									GET movies/_search?q=title:"Beautiful Mind"


									- 布尔操作

									AND / OR / NOT 或者 `&&` / `||` / `!`

									> 注意必须大写
									// 布尔 AND
									// Title 必须同时包含 Beautiful 和 Mind
									GET movies/_search?q=title:(Beautiful AND Mind)

									// 布尔 NOT
									// Title 必须包含 Beautiful, 但不能包含 Mind
									GET movies/_search?q=title:(Beautiful NOT Mind)


									- 分组

									- `+` 表示 must
									- `-` 表示 must_not
									// 布尔 must
									// Title 必须包含 Mind
									// %2B 是加号的 urlencode
									GET movies/_search?q=title:(Beautiful %2BMind)&sort=_score:asc


									- 范围查询

									- `[]` 闭区间
									- `{}` 开区间
									GET movies/_search?q=year:[* TO 2018]


									- 算术符号
									GET movies/_search?q=year:>=1980

									GET movies/_search?q=year:(>=1980 AND <=2018)

									GET movies/_search?q=year:(%2B>1980 %2B<=2018)


									- 通配符查询

									> 查询效率低, 占用内存大, 不建议使用(特别是放在最前面)

									- `?` 表示1个字符
									- `*` 表示任意个字符
									// 通配符匹配
									GET movies/_search?q=title:b*


									- 正则表达式
									GET movies/_search?q=title:/[abc]eautifu.?/


									- 模糊匹配与近似查询
									// 模糊匹配&近似度匹配
									// 这里输入 beautifl 是一个错误的单词, 但通过近似度匹配能够匹配到
									// 这里的 ~1 表示允许有一个字母有差别, 即 match_phrase 的 slop:1
									GET movies/_search?q=title:beautifl~1

									// 模糊匹配&近似度匹配
									// ~2 即 match 的 minimum_should_match: 2
									GET movies/_search?q=title:"Lord Rings"~2




				#URI Query
					GET kibana_sample_data_ecommerce/_search?q=customer_first_name:Eddie
					GET kibana*/_search?q=customer_first_name:Eddie
					GET /_all/_search?q=customer_first_name:Eddie


					#REQUEST Body
					POST kibana_sample_data_ecommerce/_search
					{
						"profile": true,
						"query": {
							"match_all": {}
						}
					}


	3.深入
			1.基于词项和基于全文的搜索

			2.
























	TODO:
			1.数据导入到es中怎样使其结构化，比如db中都是通过mapper...insert
			2.各个关键字
