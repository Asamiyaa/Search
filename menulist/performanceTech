性能优化
	
	1.总述
			1.背景
					1.大规模（大数据、用户多等）和高要求（低延迟、高吞吐等）。这 样的特点也就要求服务的高性能和容量的高效率。
					2.程序员在开发软件时会写出高性能的代码；运维人员会懂得如何监测和提 高系统的性能；软件测试人员会通览软件测试的分类和方法；管理人员可以了解如何进行容 量管理，提升服务效率并降低运营成本
					3.性能和容量领域的工作特点是需要多方面 的知识和技能，以及实际的经验积累。这种学习和积累需要相当长的时间，不太可能一蹴而 就。
					4.帮助公司提升业务 性能和容量效率，*** 节省运营成本 ***
					5.性能分析1.png
					6.靠谱的程序 - 靠谱的人 - 不要挖坑埋雷
					7.一门艺术，它需要知识，需要经验，也需要天分

			2.指标
					服务延迟（Service latency）：客户请求的处理时间。											(快)
					吞吐量（Throughput）：单位时间处理请求的数量。												(多)
					可靠性 （Reliability）可靠性注重的是在极端情况下能不能持续处理正常的服务请求
					扩展性（Scalability）：系统在高压的情况下能不能正常处理请求。								   （好）
					资源使用效率（Resource Utilization）：单位请求处理所需要的资源量（比如 CPU，内存等）。		   （省）

			3.示例
					1.新手: 判断两个日期之间,每天进行循环判断是否满足节假日   ===> 判断两个日期间有几个星期 * 5 + 特殊处理边界问题
					2.普通： 嵌套循环 x[j][i] ==> x[i][j]		因为计算机通常都会有数量不大的缓存。数组在内存里是连续存放的，所以，如果访问数组 元素的时候能够按照顺序来，缓存可以起到极大的加速作用。
					3.资深： std::unordered_map ==> Google：：unordered_map 通过修改合适的数据结构和算法
					4.专家:  提高cpu利用率 线程池线程数等于逻辑cpu数 ==> 逻辑cpu一半 逻辑cpu底层共享了物理资源
					5.架构师：指令级别的提前获取优化。具体来讲，就是用 GCC 的 __builtin_prefetch指令来预先提取关键指令，从而降低缓存的缺失比例，也就提高了 CPU 的使用效率。

					整个的代码改进只需要几行代码的改动，真真切切是"一字万金"。 性能优化的方法和最终解决方案或许看起来很简单直白，但 是要知道在哪里做优化和做什么样的优化，却需要很多的测试和分析的工作经验
					逻辑= 清晰与精准。

			4.整体
					1.模块 - 系统 - 硬件 -机房 - 部门 - 组织结构 - 公司战略
					2.协调、配合、沟通、唇亡齿寒
					3.更进一步到设计层面来讲，从我们负责的模块和应用程序角度来看，对下层的软硬件构件越 是了解，就越有可能设计出性能优越的模块和应用程序。
					4.能科学地管理容量，准确地预 测未来需求，并逐步提升容量的效率，就能把公司这方面的成本管理和节省好


			5.技能要求
					1.知识面要广，并且软硬结合
					2.理论联系实际  性能分析的过程需要做些 实验来使真正的问题暴露出来，也就是需要进行验证，所以对动手能力的要求也比较高 , 多维度确认
					3.不但要会性能测试，还要会性能分析和性能优化

					性能分析技能树.png

					跨层交互，更会产生各种复杂的性能测试；并且实际场景是复杂的，当前的上下文，业务模型

			6.法律法规
					1.帕累托法则
							1.在很多场景下，大约 20% 的因素操控着 80% 的局面。
							2.八二原则.png
					2.阿姆达尔定律
							1.改进程序本身的串行算法可能比用多核处理器并行更有效
							2.优先加速占用时间最多的模块，因为这样可以最大限度地提升加速比
							  对一个性能优化的计划可以做出准确的效果预估和整个系统的性能预测
							同一个要素增加到一定程度后，效果就越来越不明显，所以你要拆分、分别优化使其最大化
					3.利特尔法则
							1. N=XW  从不能参数上变量要进行优化
			7.数理基础
					1.概率统计
							1.概率
								对两个事件 A 和 B 而言，“发生事件 A”在“事件 B 发生”的条件下的概率， 与“发生事件 B”在“事件 A 发生”的条件下的概率是不一样的。

							2.置信度
								对产生样本的 总体参数分布（Parametric Distribution）中的某一个未知参数值，以区间形式给出的估计。置信区间蕴含了估计精 确度的信息。
								置信区间是对分布（尤其是正态分布）的一种深入研究。通过对样本的计算，得到对某个总 体参数的区间估计，展现为总体参数的真实值有多少概率落在所计算的区间里
							3.不同指标
								平均值：（Mean，或称均值，平均数）是最常用测度值，它的目的是确定一组数据的均衡 点。
										但不足之处是它容易受极端值影响。
								中位数（Median，又称中值），将数值集合划分为相等的上下两部分，一般是把数据以升 序或降序排列后，处于最中间的数。它的优点是不受极端值的影响，但是如果数据呈现一些 特殊的分布，比如二向分布，中位数的表达会受很大的负面影响		
								四分位数
								百分位数 ： 将一组数据从小到大排序，某一百 分位所对应数据的值就称为这一百分位的百分位数，以 Pk 表示第 k 个百分位数
										   几个特殊的百分位数也很有意思，比如 P50 其实就是中位数，P0 其实就是最小值，P100 其实就是最大值。
								方差 / 标准差: 变量的离散程度，也就是该变 量离其期望值的距离。

								分布：
										1.泊松分布（Poisson distribution）适合于描述单位时间内随机事件发生的次数的概率分 布。如某一服务设施在一定时间内收到的服务请求的次数等
										2.二项分布（Binomial distribution），是 n 个独立的是 / 非试验中成功的次数的离散概率 分布。
										3.正态分布（Normal distribution），也叫高斯分布（Gaussian distribution）。经常用来 代表一个不明的随机变量。


					2.排队论
							1.网络数据发送和接收、CPU 的调度、存储 IO、数据库查询 处理等等，都是用队列来缓冲请求的，因此排队理论经常被用来做各种性能的建模分析。
							2.主要的输入参数是到达速度、顾客到达分布、排队的规则、服务机构处理速度和处理模型 等。
							  排队系统的输出也有很多的参数，比较重要的是排队长度、等待时间、系统负载水平和空闲 率等。所有这些输入、输出参数和我们进行的性能测试和优化都息息相关

				8.复杂度
							常数时间，O(1)：判断一个数字是奇数还是偶数。1.
							对数时间，O(Log(N))：你很熟悉的对排序数组的二分查找。2.
							线性时间，O(N)：对一个无序数组的搜索来查找某个值。3.
							线性对数时间，O(N Log(N))：最快的排序算法，比如希尔排序，就是这个复杂度。4.
							二次时间，O(N^2)：最直观也最慢的排序算法，比如冒泡，就是这个复杂度。5.
							指数时间，O(2^N)： 比如使用动态规划解决旅行推销员问题。这种复杂度的解决方案 一般不好

				9.性能数据展示
							1.每一种场景下，数 据展示要根据你的具体目的、听众的特点和内容的特点而采用合适的图表。然后用这些图表 做支持，把一个精美的数据分析的“故事”讲出来。
							2.挑战
									1.数据量大		 多个指标
									2.数据复杂 		 几十或几百的性能指标互相交互和影响，需要考虑它们之间的复杂关系
									3.性能问题复杂	 性能分析不是简单地死扣数据，还需要考虑互联网系统的协议，计算机 的设计，和软硬知识的结合
									4.牵扯的模块多    很多性能问题都不是孤立的，都和其他模块和子系统，甚至客户请求有 关系。

							3.如何讲好故事
									1.有啥菜吃啥饭：  要根据手头数据的特点来决定展示方法。比如你有好几组相关的数据， 有很有趣的趋势，那就要用线图来展示趋势和相关性。
									2.给足上下文：    很多工程人员容易犯的错误，就是想当然地以为别人也都了解问题的背 景，然后一上来就展示很多细节。其实除了你自己，没有第二个人更了解你要解决的问 题和要展示的数据，所以一定要给足背景介绍和上下文信息。
									3.用图讲故事：    人人喜欢听故事，而且是有趣的故事。如果你能把整个分析和推理的过 程，变成一个引人入胜和图文并茂的故事来讲，那我保证你的展示会非常地成功。
									4.和听众交互：    尽量鼓励听众参与讲故事的过程。有两种类型的数据叙事：叙述型和探索 型。叙述型是通过描述告诉观众具体结论；而探索型是鼓励观众探索数据以得出结论。 虽然探索型叙事稍微多花一点时间，但若能成功运用，听众更容易被说服；因为结论是 他们自己得出的，而不是你告诉他们的。
									5.总结重要点：    在数据展示的最后，一定要简洁明了地总结你的展示。比如你希望听众最 后只记住三句话，是哪三句呢？根据你的展示目的，这三句或是相关数字，或是趋势， 或是问题本质，或是解决方案。

							4.形式
									1.表格（Table）
									2.线图（Line Chart)
									3.PDF 和 CDF 图
										pdf: 概率密度函数（Probability Density Function）是连续型随机变量的概率密度函数 （或简称为密度函数）。就是一个描述这个随机变量的输出值，在某个确定的取值点附近的 可能性的函数。
										cdf：累积分布函数 (Cumulative Distribution Function)，又叫分布函数，是概率密度函 数 PDF 的积分。换句话说，PDF 就是 CDF 的导数。CDF 能完整描述一个实随机变量 X 的 概率分布。
									4.面积图（Area Charts）
									5.柱状图和条形图（Bar Charts）
									6.散点图（Scatter Plots）和气泡图（Bubble Charts）
									7.饼图和圆环图（Pie Charts， Donut Charts
									8.树形图（Treemaps
									9.热图（Heatmaps）

									性能数据展示.png

				10.容量设计
						1.根据业务、市场、硬件、扩展、数据中心进行总体设计

				11.情商、沟通能力


	2.设计

			1.服务重试机制 vs 指数退避机制（Exponential Backoff ）vs 熔断
						1.串联的服务模块中，上游模块必须摒弃这样雪上加霜的服务异常 通过快速地降低请求速度来帮助 下游模块恢复（上游模块对下游资源进行重试请求的时间间隔，要随着失败次数的增加而指 数加长）。

	3.实现
			1.优化原则
					1.查最大性能瓶颈  - 四个方面 / 最大性能提升 / 
					2.确诊问题根因	 - 考虑工作的投入与产出比例，即考虑成本，也考虑带来的好处 / 多方面的性能测试、假设分析并验证 / 特殊场景 边缘场景
					3.考虑多种情况	 - 不要苛求所有场景下的最优解决方案，不现实的。 / 在不同的性能指标间权衡，以找到一个最优解能达到  ‘ 总体和整体 ’ 最优
				
				综合以上因素，在实际的优化过程中，我们经常会反复权衡利弊和取舍来做最终决定

					1.过度反常态优化
								1.增加系统复杂度和维护成本，开发测试周期加长
								2.预估产品的要求并按照其进行测试
								3.类似于过度设计

					2.过早不成熟优化
								1.不要过早优化 ，但这并不代表写程序时不考虑性能，应该在性能基础上不要去扣研究，最求极致
								2.快速 “ 推出产品 ”，并小步快跑进行迭代
					3.表面的肤浅优化
								1.不要头痛医头，脚痛医脚，从 “底层”“全局”把握，达到最好的解决
								2.你发现一个应用程序的 CPU 使用率并不高，但是吞吐率上不去，表面的优化方 式可能是增大线程池来提升 CPU 使用率。
								  需不需要根据底层硬件和上层请求的变化而对线程池 的大小调优呢？
								  对这样的场景，正确的优化方式，是彻底了解线程的特性，以优化线程为主。
								  至于线程池的 大小，最好能够 “ 自动调整 " ==> 自适应。千万别动不动就手工调优。如果这样手工调整的参数多了，就会 做出一个有很多可调参数的复杂系统，类似jvm

					针锋相对的提出最好的解决方案，而这个最优解，往往是考虑各种情况之后权衡取舍决定，所以说脱离场景没有意义

			2.优化策略	== 有层次的优化性能问题		
					1.时空转换
								1.空间换时间
										1.缓存
										2.数据copy，使其离用户更近，cdn转发
										3.集群/负载均衡

								2.时间换空间
										1.改变应用程序本身的数据结构或者数据格式，减少需要存储的数据的大小；
										2.想方设法压缩存在内存中的数据，比如采用某种压缩算法，真正使用时再解压缩；
										3.把一些内存数据，存放到外部的、更加便宜的存储系统里面，到需要时再取回来。
										4.降低数据的大小来方便网络传输和外部存储

										ZStandard（ZSTD）和 LZ4。这些算法之间有 空间和时间的取舍。 
										压缩比例、压缩速度以及使用内存 - 如果系统的瓶颈在网络传输速度或者存储空间大小上，那就尽量采取高压缩比的算法

					2.并行/异步
								1.并行操作
										1.并行操作是一种物理上把一条流水线分成好几条的策略。直观上说，一个人干不完的活，那 就多找几个人来干
										2.服务器的粒度（所谓的横向扩展），还是在多线程的粒度，甚至 是在指令级别的粒度。
										3.io阻塞的地方

								2.异步操作
										1.
					3.预先/延后处理
								1.预先/提前处理
										1.页面/文件系统会对数据从磁盘额外读取，为下次上层应用读取竹内
										2.cpu/内存同样预读
										3.硬件预取/软件预取 ，后者需要插入prelocad /prefetch指令


								2.延后/惰性处理
										1.cow 写时复制 多线程间/多cpu间共享数据，只有修改时候需要复制/失效来保证一致性 ；2.并不是一开始就复制，而是写时才复制
												1.unix fork
												2.Java copyonwrite容器 并发问题 
												3.string类 写时复制
										2.同步和异步的区别在于一个函数调用之 后，是否直接返回结果。
					4.缓存/批量合并
								1.缓存数据和结果
										1.CPU、内存、文件系统、存储系统、内容分布、数据库通过缓存加速
										2. Web 的应用服务，前端会有浏览器缓存，有 CDN 存放在边缘服务器上，有 反向代理提供的静态内容缓存；后端则还会有服务器本地缓存
										3.程序设计中，对于可能重复创建和销毁，且创建销毁代价很大的对象（比如套接字和线 程），也可以缓存，对应的缓存形式，就是连接池和线程池等。
										4.对于消耗较大的计算，也可以将计算结果缓存起来，下次可以直接读取结果。比如对递归代 码的一个有效优化手段，就是缓存中间结果。
										   ===> 图搜 向量计算

								2.合并和批处理
										1.在有 IO（比如网络 IO 和磁盘 IO）的时候，合并操作和批量操作往往能提升吞吐量
										2.GFS 写文件的时候，尽量批量写，以减少 IO 开销。
										3.对数据库的读写操作，也可以尽量合并。比如，对键值数据库的查询，最好一次查询多个 键，而不要分成多次
										4.网络请求的时候，网络传输的时间可能远大于请求的处理时间，因此合并网络请求也 很有必要

					5.算法设计和数据结构
								1.更快的算法设计
										1.对每一种具体的场景（包括输入集合大小、时间空间的要求、数据的大小分布等），总会有 一种算法是最适合的。
								2.更优化的数据结构
										1.没有一个数据结构是在所有情况下都是最好的，比如你可能经常用到的 Java 里面列表的各 种实现，包括各种口味的 List、Vector、LinkedList，它们孰优孰劣，取决于很多个指标： 添加元素、删除元素、查询元素、遍历耗时等等。我们同样要权衡取舍，找出实际场合下最 适合的高效的数据结构。




	4.测试

			1.概述
					1.目的是确保软件应用程序在一定的负载流量下运行良好。性能测试是性能分析和性能优化的基础，它的目标是发现和性能相关的各种问题和性能瓶颈，从而进一步去消除错误和性能瓶颈。
			2.概念
					1.测试目的
							测量服务速度（Speed）：确定程序是否能够快速地响应用户的请求，这个服务速度一 般包括延迟和吞吐率两个指标。速度通常是应用程序最重要的属性之一，因为运行缓慢 的应用程序容易丢失用户。
							测量可扩展性（Scalability）：确定应用程序是否可以在用户负载和客户流量增大情况下 还能正常地运行。
							测量稳定性（Stability）：确定在各种极端和恶劣环境下，应用程序是否能稳定运行。3.
							测量性能瓶颈（Performance Bottleneck）：性能瓶颈是应用程序和系统中的最影响整 体性能的因素。瓶颈是指某个资源不足而导致某些负载下的性能降低。一些常见的性能 瓶颈是 CPU、内存、网络、存储等
					2.测试环境
					3.负载流量
							小流量，正常流量，还是超大流量。除了大小，负载变化的速 度也需要考虑。

					4.测试对象
							针对一个代码功能，或是整个代码模块，亦或是整个系统。

					5.负载数据
							真实数据 vs 人工模拟
					6.黑盒白盒
					7.性能测试种类 == 测试种类.png 
							负载测试
							容量测试
									：可调节的流量负载、性能的测量、可以接受的性 能指标。
							压力测试
							断点测试
									断点测试也可以用来确定系统将达到其所需规范或服务水平协议的最大容量，并且自动采取 措施来纠正或者缓解。云计算环境中，我们可以设置某种性能断点，用它们来驱动某种 扩展和伸缩策略。

							瓶颈测试
							尖峰测试
							耐力测试
									用例是暴露某些不易重现的问题，如内存问题、系统故障或其他随机 问题。

							基准测试
									确保新的代码不会对整个模块或系统的性能产生任何不好的影响。
							可扩展性测试
									测试用于确定一个程序和系统的非功能性特征能不能在变化的 环境里合理扩展。这里的环境变化包括系统环境的变化、负载量的大小、请求的多样性、数 据量的大小等。

							冒烟测试
									开发环境里执行的简单测试，以确定新的程序代码不出故
			
			3.步骤	==  性能测试步骤.png 
					1.影响性分析，知道测试的点，代码块/模块/系统/...
					2.响应时间是用户关注的 指标，吞吐量是业务关注的指标，资源利用率是系统关注的指标。

			4.工具
					1.Web 测试场景
							1.JMeter 
							2.LoadRunner
							3.Locust 

					2.系统测试场景
							1.UnixBench 
							2.Perf 
					3.数据库测试场景
							1.SysBench
							2.mysqlslap 
					4.文件 IO 和存储测试场景
							1.ioZone

					5.网络测试场景
							1.Netperf 
							2.Iperf 

					工欲善其事，必先利其器


			3.九条性能测试的经验和教训：如何保证测试结果可靠且可重复？ 
					测试可靠且重复.png 
					1.虽然每次调优建议只调整一个参数，但是这样的话可能错过： 最有搭配 
					2.分析要由易到难：
							1.首先从最常见的几种资源和几个指标查起，比如 CPU 使用率、存储 IO 繁忙度、内存大 小、网络发送和接收速度等。
							2.进一步的分析就可以针对不太明显的资源，比如内存带宽，缓存击中率，线程加锁解锁等； 
								从而过渡到应用程序和系统的一些配置参数。这些配置参数包括应用服务器及中间件，操作 系统瓶颈，数据库、WEB 服务器的配置；
							3.还有应用业务瓶颈，比如 SQL 语句、数据库设 计、业务逻辑、算法、数据等。
					3.几种测试最好互相验证
					4.与生产环境对比：
							1.网络
							2.硬件
							3.用户思考时间 停顿

			4. 性能测试的工程集成：如何与产品开发和运维业务有机集成
					1.devops - jeckins : 为了配合敏捷开发（Agile Developement）的速度和效率而产生的，是把源代码管理、代码检查、程序编译、性能 和功能测试、产品部署等一系列操作整合在一起的概念和工具。
					2.集成
						0.持续集成的价值主要有几点：
								降低代码开发风险，
								及早发现集成错误，
								减少手动测试过程， 
								快速生成测试结果，
								提高程序员和开发团队的安全感。
								频繁的提交代码，也会鼓励和 促使开发人员创建模块化和低复杂性的代码。

						1.与产品开发
							代码版本管理、代码检查、编 译连接、功能测试、性能测试、性能分析、代码覆盖率、结果展示等。有些流行的持续集成 服务器可以整合这些工具或功能，比如 Jenkins、CruiseControl、Travis 
								用 JStyle 来分析 Java 源代码
								用 Ant 构建运行 JUnit 来进行简单的功能测试
								用 DbUnit 执行长时间的数据库组件测试
								用 JUnitPerf 来进行负载和压力测试
								用 JProfiler 来进行全功能的代码性能剖析
								用 Selenium 运行基于 Web 的功能测试
								用 Cobertura 测量代码覆盖率
								用 CruiseControl 作为服务器来管理持续集成
						2.与运维业务
								智能化
								自动
								监控报警



	5.问题定位
			1.如何从大量指标中看到信号
					1.收集系统和模块的运行时间、客户访问延迟、客户滞留时间、服 务吞吐率、程序的 CPU 占用时间等
					2.需要判断它们的值到底是正常还是不正常，这就需要经验 和知识才能判断。
							db查询 ： 几百毫秒
					3.如果观测到针对某个指标的一系列性能数据，那就需要判断这个指标有没有随着时间或者其 他变量的变化而变差（Regression）和变好（Improvement
					4.对某个性能指标做预测。根据 情况做些预测分析，比如根据这个指标的历史数据来进行曲线拟合。
					5.指标之间相互影响
					6.对一个时间序列的分析
							1.性能数据的时间序列往往不是均匀平滑的，反而会有各种有规律的峰 值。需要你根据具体的情况来决定要不 要做特殊的考虑。比如节假日,秒杀、办公时点
							2.线性回归分析（Linear Regression）。线性回归是通过拟合自变量 与因变量之间最佳线性关系，来预测目标变量的方法。线性回归往往可以预测未来的数据点
							3.除了研究数据的趋势和未来预测，分类（Classification）、聚 类（Clustering）以及决策树（Decision Tree）。分类是将类别分配给数据集合，帮助更 准确地预测和分析。聚类是把相似的东西分到一组。决策树也叫分类树或回归树，每个叶节 点存放一个类别，每个非叶节点表示一个特征属性上的测试。
					7.对不同时间序列的分析
							1.数据的相关性是指数据之间存在某种关系，可以是正相关，也可以是负相关。两个数据之间 有很多种不同的相关关系

			2.数据分析的教训和陷阱
					1.数据的相关和因果关系
							1.因果关系 却不能判定
							2.容易武断地犯这样的错误，而导致走弯路。
							3.几个指标互为因果，或者构成环形因果，也就 是互相推波助澜。实际分析起来非常有挑战性，这就需要我们对整个系统和各个性能指标了 如指掌。

					2.数据的大小和趋势
							1.面对性能相关的数据并判断它们是“好”还是“坏”是很难的
							2.在很多情况下，比单纯数 字大小更重要的是数据的趋势，比如某个时期是上升还是下降，变化的幅度有多大等等
					3.数据干净与否
							1.是否需要剔除，否则会干扰结论
					
					4.性能数据内在关系的理解

			3.通常的性能标准  == 每次看到一个性能相关的数据的时候，我们立刻就能知道这个性能 数据有没有问题。 == 常用性能数值.png 

					1.存储 -- 存储性能数值.png  
							1.IO 读写延迟。一般是用 4KB 大小的 IO 做基准来测试；
							2.IO 带宽，一般是针对比较大的 IO 而言；
							3.IOPS，就是每秒钟可以读写多少个小的随机 IO

							

							太大可能有问题，太小可能都没有到达存储而是在上游。
							一般传统硬 盘的随机 IO 读写延迟是 8 毫秒的样子，IO 带宽大约 100MB 每秒，而随机 IO 读写一般就 是每秒 100 出头。


					2.CPU 和内存相关  -- cpu性能数值.png
							1. CPU 的时钟频率，也就是主频。 主频反映了 CPU 工作节拍，也就直接决定了 CPU 周期大小。4GHz， 那么每一个时钟周期（Cycle）大约0.25 纳秒（ns）。						
							2.CPU 运行程序时，最基本的执行单位是指令。而每一条指令的执行都需要经过四步：指令 获取、指令解码、指令执行、数据存入。这些操作都是按照 CPU 周期来进行的，一般需要 好几个周期。
							3.CPI 和 IPC
									CPI（cycles per instruction）衡量平均每条指令的平均时钟周期个数。它的反面是 IPC（instructions per cycle）
							4.MIPS


							1.cpu缓存
									1. L1、L2、L3，按这个顺序越来越慢，也越来越大
									2.多核 CPU 的情况下，一般 L1 和 L2 在核上，而 L3 是各个核共享的。

									

					3.操作系统和应用程序相关	
							1. 指令分支延迟	
									1.CPU 通常会采取提前 提取指令这项优化来提高性能，但是如果是指令分支，那么就可能预测错误，预先提取的指 令分支并没有被执行。

							2. 互斥加锁和解锁
									1.互斥锁 Mutex（也叫 Lock）是在多线程中用来同步的，可以保证没有两个线程同时运行在 受保护的关键区域。使用互斥锁的时候需要加锁和解锁，都是时间很昂贵的操作，每个操作 一般需要几十个时钟周期，10 纳秒以上。

							3. 上下文切换
									1.多个进程或线程共享 CPU 的时候，就需要经常做上下文切换（Context switch）。这种切 换在 CPU 时间和缓存上都很大代价；尤其是进程切换。在时间上，上下文切换可能需要几 千个时钟周期，1 微秒（1us）级别。在缓存代价上，多级 CPU 缓存和 TLB 缓存都需要恢 复，所以可能极大地降低程序线程和进程性能。


					4.网络相关	-- 网络性能数值.png
							1.大致 说每 100 公里就需要一毫秒。北京到深圳约 2,000 公里，RTT 就是 20 毫秒；上海到乌鲁 木齐或者美国的东西海岸之间距离差不多 4,000 公里，所以 RTT 是 40 毫秒左右；中国到 美国（比如北京到美国西海岸旧金山）差不多 10,000 公里，RTT 就是 100 毫秒
							2.在数据中心里面，一般的传输 RTT 不超过半毫秒。如果是同一个机柜里面的两台主机之 间，那么延迟就更小了，小于 0.1 毫秒。
							3.数据是通过骨干网光纤网络传播的。 如果光纤网络绕路的话，那么实际的 RTT 会超过以上估算数值。
							4.传输延迟也取决于传输数据的大小，因为各种网络协议都是按照数据包来 传输的，包的大小会有影响。比如一个 20KB 大小的数据，用 1Gbps 的网络传输，仅仅网 卡发送延迟就是 0.2 毫秒。
            
            4.常见的性能瓶颈和任何找到他们 
            		1.常见的性能瓶颈和任何找到他们.png 

            5.依据数据和剖析（Profiling）来分析 而不是碰运气
            		1.依据数据和剖析分析.png 

            6.常见的性能问题
            		1.cpu
            				1.CPU 的性能决定因素
            						1.包括有多少处理器、多少个 核、时钟主频是多少、有没有 Turbo 模式、处理器内部的运算架构以及和 CPU 紧密交互 的其他部件的性能。
            				2.CPU 的内部结构
            						1.多处理器和多核、逻辑 CPU 和硬件线 程、超线程，以及 L1/L2/L3 三级缓存
            						2.现在的 CPU 普遍采用多处理器（Socket）来提高 CPU 性能，每个处理器都有自己可以直 接访问的本地内存（Local Memory）。每个处理器也都可以访问其他处理器的内存，这些内存就相当于是外地 / 远程 内存（Remote Memory）。
            						  采用多处理器和 NUMA 架构的主要原因，是提高整个 CPU 的并行处理性能。
            						3.多核结构和多级缓存
            								1.L1 和 L2 一般在核的内部,在同一个处理 器内部的核会共享同一个 L3 缓存。
            								2.除了多个核以及 L3 缓存外，处理器上一般还有非核心处理器（Uncore），里面含有和指 令运行不直接相关的组件，包括 QPI 控制器和存储器一致性监测组件

            						4.超线程
            								1.一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程 度，这一技术就叫超线程
            								2.当处理器在运行一个线程，执行指令代码时，很多时候处理器 并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多 线程来进一步“压榨”处理器。

            				==> 一台计算机有两个处理器，每个处理器有 12 个核，而且采用了 HT 超线程，那 么总的 CPU 数目就是 48，就是 2×12×2。这个数字 48，就是我们平时用监控软件和命令 看到的 CPU 的数量。比如，Linux 的 top 或者 vmstat 命令，显示的 CPU 个数就是这样 算出来的。

            				3.分析
            						1. CPU 架构的复杂性，以及和其他部件的交互，CPU 的使用率和负载的 关系往往不是线性的
            						   如果 10% 的 CPU 使用率可以每秒处理 1 千个请求，那么 80% 的 CPU 使用率 能够处理多少请求呢？不太可能处理每秒 8 千个请求，往往会远远小于这个数字
            						2. MIPS（Millions of Instructions Per Second）来衡量，表示每秒能运行多少个百万指令，MIPS 越高，性能 越高。MIPS 的计算很简单，就是时钟频率×IPC。
            						3.CPU 常见的各种中断包括软中断和硬中断 ， 上下文切换； 均衡
            								如果是 CPU 超载，那么就要分析为什么超载。多数情况下都不一定是合理的超载，
            								比如说 多核之间的负载没有平衡好，或者 CPU 干了很多没用的活，或者应用程序本身的设计需要 优化等等。
            								反之，如果是 CPU 空闲，那就需要了解为什么空闲，或许是指令里面太多内存 数据操作，从而造成 CPU 停顿，也或许是太多的分支预测错误等，这就需要具体分析和对 症下药的优化。

            						4.CPU 对多线程的执行顺序是谁定的呢
            								内核的进程调度来决定的。内核进程调度负责管理和分配 CPU 资源，合理决定哪个进 程该使用 CPU，哪个进程该等待。进程调度给不同的线程和任务分配了不同的优先级，优 先级最高的是硬件中断，其次是内核（系统）进程，最后是用户进程。每个逻辑 CPU 都维 护着一个可运行队列，用来存放可运行的线程来调度。



            		2.内存
            				1.因素
            					缓存命中率
            							1.缓存是 CPU 与内存之间的临时数据交换器
            							2.缓存的策略也用在计算机和互联网服务中很多其他的 地方，比如外部存储、文件系统，以及程序设计上。有人甚至开玩笑说，计算机的各种技术 说到底就是三种——Cache（缓存）、Hash（哈希处理）和 Trash（资源回收
            							3.cpu缓存.png

            							4.为什么要采用多级缓存，并逐级增加缓存大小呢
            								是为了提高各级缓存的命中率，从而最大限度地降低直接访问内存的概率。每 一级缓存的命中率都很重要，尤其是 L1 的命中率。

            					缓存一致性
            							1.在访问缓存和写回内存时遵循一些协议，这样的协议就叫缓存一致性协议。常见的缓存一致性协议有 MSI、MESI 


            					内存带宽内存延迟
            							1.内存带宽：单位时间内，可以并行读取或写入内存的数据量，通常以 字节 / 秒为单位表示。
            							2.实际中我们的程序使用只能达到最大带宽利用率的 60％。如果超出这个百分比，内存的访问延迟会急剧上升
            							3.带宽影响因素：
            									DRAM 时 钟频率
            									每时钟的数据传输次数
            									内存总线带宽（一般是 64 字节）
            									内存通道数量。

            					内存的使用 
            					大小及碎片
            					内存的分配
            							内存池，就是提前申请分配一定数量的、大小仔细考虑的内存块留作备用。当线程有 新的内存需求时，就从内存池中分出一部分内存块。如果已分配的内存块不够，那么可以继 续申请新的内存块。同样，线程释放的内存也暂时不返还给操作系统，而是放在内存池内留 着备用。
            							这样做的一个显著优点是尽量避免了内存碎片，使得内存分配效率和系统的总体内存使用效 率得到提升。

            					回收速度



            		3.存储
            				1.狭义上的存储往往是硬件，比如磁盘、磁带还有固态硬盘。而广义上的存储系统除了指硬件 的硬盘，还包括基于网络的存储系统，比如 SAN（Storage Area Network, 存储区域网 络）和 NAS 存储（Network Attached Storage，网络接入存储）。

            				2.三大指标
            						IOPS
            								1.Input/Output Per Second,即每秒钟能处理的读写请求数量，这是衡量存储性 能的主要指标之一。每个 IO 的请求都有自己的特性，比如读还是写，是顺序读写还是随机 读写，IO 的大小是多少等
            								2.顺序读写
            										就是访问存储设备中相邻位置的数据；随机读写呢，则是访问存储设备 中非相邻位置的数据。对随机读写进行性能衡量时，一般假定 IO 大小是 4KB
            								3.影响要素
            										IOPS 的数值会随这样的参数不同而有很大的不同，这些参数的变化，包括读取和写入的比 例、其中顺序读写及随机读写的比例、读写大小、线程数量及读写队列深度等。此外，系统 配置等因素也会影响 IOPS 的结果，例如操作系统的设置、存储设备的驱动程序特点、操作 系统后台运行的作业等。
            										IO 队列深度增加吞吐率会升高，平均访问延迟会降低。： 这是因为存储系统的 IO 队列处理机制，可以对 IO 进行重新排序，从而获 得好的性能。比如，它可以合并几个相邻的 IO，把随机 IO 重新排序为顺序 IO 

            						访问延迟
            						吞吐率 / 带宽
            								1.衡量的是存储系统的实际数据传输速 率，通常以 MB/s 或 GB/s 为单位。
            				3.磁盘响应时间 = io排队延迟 + 寻址时间（几毫秒） + 旋转时间 + 数据传输时间
            								1.随机 IO 读写延迟就是 8 毫 秒左右，IO 带宽大约每秒 100MB，而随机 IOPS 一般是 100 左右

            				4.SSD
            						1.单元（Cell）、页面（Page）、块（Block）
            								1.SSD 的特点是，对 SSD 单元的每次擦除，都会降低单元的寿命，因此每一个单元只能承受 一定数量的擦除。所以，不同的 SSD 就有这几方面的考虑和平衡。单元存储的位数越多， 制造成本就越少，SSD 的容量也就越大。但是耐久性（擦除次数）也会降低。
            								2.不存在页的覆写，而是直接擦除、重置
            				5.I/O 和垃圾回收
            						1.最小读入一页 page 4k
            						2.ssd问题：写入放大
            								实际写入 SSD 的物理数据量，有可能是应用层写入 数据量的多倍
            									一方面页级别的写入需要移动已有的数据来腾空页面来写入。
            									另一方面，GC 的 操作, 也会移动用户数据来进行块级别的擦除
            							   这么设计是因为：因为一 块 SSD 只能进行有限的擦除次数，也称为编程 / 擦除（P/E）周期，所以写入放大效用会 缩短 SSD 的寿命。
            						3.耗损平衡（Wear Leveling）
            				6.NAS（Network Attached Storage）是网络接入存储。在 NAS 存储结构中，存储系统不 再通过 I/O 总线只属于某个特定的服务器，而是通过网络接口直接与网络相连。NAS 提供 的是文件服务器的功能（比如 NFS 和 CIFS），供客户通过网络访问。


            		4.网络
            				1.指标
            						1.可用性
									2.响应时间（response time）
										端到端的物理距离、所经过网络以及负荷、两端主机的负荷、主机网络队列中堆积情况
									3.网络带宽容量（network bandwidth capacity）
										由设备和网络协议决定的，比如网卡、局域网和 TCP/IP 的特性
									4.网络吞吐量（network throughput）
										网络吞吐量取决于当前的网络负载情况，而且是随着时间不同而不断变化的
									5.网络利用率（network utilization）
										因为 数据传输的突发性，所以实际中的网络利用率一般不会太高。否则的话，那么响应时间就不 能保证。

            				2.论端到端的互联网数据传输		
            						1.机柜交换器
            						2.数据中心网络
            						3.cdn:内容分发网络的基本原理是，利用最靠近每位用户的服务器，更快、更可靠地将（音乐、图 片及其他）文件发送给终端用户
            				3.丢包
            						1.网络 发生了拥塞
            						2.硬件问 题
            						3.软件问题
            					对 TCP 层，可以用比如 netstat 来观察是不是套接字 缓存不够；对操作系统，可以观察 softnet_stat，来判断 CPU 的查询队列；对网卡驱动 层，可以用 ethtool 等来进行分析。

            				4.工具
            						1.Netstat 
            						2.Traceroute 


	6.复现和解决验证
            1.如何提高LLC（最后一级缓存）的命中率？  Cache Hit / miss
            	1.影响
            		1.cpu速度 内存访问延迟时LLC延迟的很多倍
            		2.内存带宽 不命中就要从内存中读取，加大内存带宽
            		3.最近几 年计算机和互联网发展的趋势是，后台系统需要对越来越多的数据进行处理，因此内存带宽 越来越成为性能瓶颈。
            	2.发现定位
            		1.perf : pmu收集各种相关 CPU 硬件事件的数据（例如缓存访问和缓存未命中LLC 读写计数、LLC 不命中计数、LLC 预 先提取计数等指标)

            	2.解决
            		1.紧凑化数据结构
            		2.软件预取数据
            				1.在相当多的情况下，程序对内存的访问模式是随机、不规则的，也就是不连续的。硬件 预取器对于这种随机的访问模式，根本无法做出正确的预测，这就需要使用软件预取。
            				2. __builtin_prefetch ,因为预取的内容会占用缓存空间，所以要谨慎
            							1.软件预取最好只针对绝对必要的情况，就是对会实际严重导致 CPU 停顿的数据进行预 取。
										2.对于很长的循环（就是循环次数比较多），尽量提前预取后面的两到三个循环所需要的 数据。
										3.而对于短些的循环（循环次数比较少），可以试试在进入循环之前，就把数据提前预取 到

            		3.去除伪共享缓存
            				1.避免共享变量，因为底层每次写都要加锁 -- 数据分段整合/数据独立最后整合思想
            						举个具体例子，假如我们要写一个多线程的程序来做分布式的统计工作，为了避免线程对于 同一个变量的竞争，我们一般会定义一个数组，让每个线程修改其中一个元素。当需要总体 统计信息时，再将所有元素相加得到结果。
            						解决:
            							让每个元素单独占用一个缓存行，比如 64 字节，也就是按缓存行 的大小来对齐（Cache Line Alignment）。
            							具体方法怎么实现呢？
            							其实就是插入一些无用 的字节（Padding）。这样的好处，是多个线程可以修改各自的元素和对应的缓存行，不会 存在缓存行竞争，也就避免了“伪共享”问题。

            		
            2.如何提高iTLB（指令地址映射）的命中率？ 
            	1.背景：
            		为了能进行快速的虚拟到物理地址转换，TLB（转换后备缓冲区）这种专门的硬件就 被发明出来了，它可以作为内存页表的缓存。TLB 有两种：数据 TLB（Data）和指令 TLB（Instruction），也就是 iTLB 和 dTLB；因为处理器的大小限制，这两者的大小，也 就是条目数，都不是很大，只能存储为数不多的地址条目。

            	2.解决
            		1.地址映射 - 编译
            				1.重新处理编译二进制，使其hot text 
            		2.使用大页面
            				1.x86_64 上分别为 2MB 和 1GB
            						1.使用较大的页面好处是，减少 了覆盖二进制文件的工作集所需的 TLB 条目数，从而用较少的页面表就可以覆盖所有用到 的地址，也就相应地降低了采用页面表地址转换的成本。
            				2.如果使用“大内存页”，页面变大了（比如 2MB 是 4KB 的 512 倍），自然所需 要的页数就变少了，也就大大减少了由内核加载的映射表的数量。这样就提高了内核级别的 性能，最终提升了应用程序的性能。
            				3.使用大页面来装载程序的热文本区域。通过在大页面上放置热 文本，可以进一步提升 iTLB 命中率。使用大页面 iTLB 条目时，单个 TLB 条目覆盖的代码 是标准 4K 页面的 512 倍。
           
            3.如何降低SSD峰值延迟？
            	1.背景
            		1.写入放大
            				1.当写入 SSD 的物理数据量，大于应用程序打算写入 SSD 的逻辑数据 量时，就会造成“写入放大”。
            				原因:
            					HDD 是可以直接写入覆盖的。和 HDD 不同，SSD 里面的页面只能写入一次， 要重写的话，必须先回收擦除，而且只能在“块”这个级别进行擦除。因此呢，SSD 内部 就需要不断地移动所存储的数据，来清空需要回收的块。也就是说，SSD 内部需要进行块 级别的“垃圾回收”。垃圾收集器必须有效地在 SSD 内部不断地回收块，回收以前使用的 页面空间，然后才能在这个块上写入新数据。
            				缺点:
            					会更快地损耗 SSD 的生命。每个 SSD 都有固定数量的擦除周期，如果在很短时间内写到 SSD 太多数据，就会导致 SSD 损耗太快，有可能过早烧坏 SSD。换句话说，很高的写入速率，可能会导致 SSD 在到 达其预期使用寿命之前就发生故障。
            					一个 SSD 预期寿命是 4 年

            	2.解决
            		1.降低写入放大系数
            				1.是保留一定的空闲存储空间，这是因为写入放大系数是和 SSD 存储空闲率相关的 - 预留空间 / 存储量在80% - 85%
            				2.是使用 Trim
            					SSD 收到 Trim 命令后，SSD 内部的控制器会更新其内部数据页面地图，以便在写入新数 据时不去保留无效页面。
            					并且，在垃圾回收期间不会复制无效页面，这样就实现了更有效的垃圾收集，也就减少了写操作和写入放大系数，同时获得了更高的写吞吐量，延长了驱动器 的使用寿命
            					默认情况下，操作系统一般不启用 Trim。因此，当文件系统删除文件时， 它只是将数据块标记为“未使用”。
            				3.好处是可以减少 SSD 的损耗，延长 SSD 的寿命；坏 处是会造成应用程序的 IO 读写延迟变大。

            4.如何优化程序、OS和存储系统的交互？
            	1.储备
		            	1.问题的表象、问题的" *** 重现 *** " 、性能分析的过程和解决方案
		            	2.从最上层的应用程序，到 中间层 JVM 的机制，再到操作系统和文件系统的特性，最后还涉及到硬件存储的特点。
		            	3.200 毫秒延迟，是多数在线用户可以忍受的 最大延迟。因此，确保低于 200 毫秒（甚至更短）的延迟，已经成为定义的 SLA（服务水 平协议）的一部分。

		        2.现象
		        		1.gc日志
		        			JVM 的堆 4GB,垃圾收集基本不会超过 1 秒,但是
		        			用户和系统时间都可以忽略不计，User 和 Sys 的暂停时间分别 是 0.18 秒钟和 0.01 秒钟，但是实际上 JVM 暂停了 11.45 秒钟

		       	3.解决
		       			1.实验环境搭建--预搭的重要性：出于可控制性和可重复性的考虑，我们使用了自己设计的一个简单 Java 程序。为了方便对 比，我们根据有没有后台背景 IO 活动
		       			2.Java 程序的逻辑也很简单直白，就是不断地分配和删除特定大小的对象
		       			3.为了真实地模拟生产环境，我们在第二种场景中注入后台 IO。 这些 IO 由 bash 脚本生 成，该脚本就是不断复制很大的文件。
		       				在实际的生产过程中，IO 负载可能 来自很多地方，比如操作系统、同一机器上的其他应用程序，或者来自同一 Java 应用程序 的各种 IO 活动。

		       			4.跟踪指标：
		       					1.总暂停时间，即所有 STW  暂停的总暂停时间
		       					2.较大的 STW 暂停计数和

		       					工具监测 分析
		       	4.分析
		       			1.io问题导致，那么此时的swt 需要哪些io操作呢？
		       				 GC 日志记录，write() 调用被阻塞导致的
		       				 虽然以缓冲写入模式（即非阻塞 IO）发出，但由于操作系统有“回写”IO 的机制，所 以仍然可能被操作系统的“回写”IO 阻塞。
		       			2.为什么非阻塞 IO，还会被阻塞？！
		       				非阻塞 IO 写入还是可 能会停留，并被阻塞在内核代码执行过程中。具体原因有好几个，其中包括：
		       				页面写入稳定 （Stable Page Writing）
		       				件系统日志提交（Journal Logging）
		       	5.解决
		       			  修改 JVM；将 GC 日志记录活动，与导致 STW 暂停的关键 JVM GC 进程分开 可能造成数据丢失
						  减少后台 IO；
						  将 GC 日志记录与其他 IO 分开	ssd  是JVM的一个参数 -Xloggc:<gc-log-file-path>。

			5.如何在生产环境中进行真实的容量测试？ 
				1.如何保证服务容量足够呢？首先就必须做好服务容量的预测
						一般是先确定每台单独的服务器可以支撑 多少服务流量；然后用这个单台服务器的数据，来决定这个服务整体需要多少台服务器
				2.测试为什么需要在生产环境中进行？
						1.客户需求会随着时间而变化，例如高峰时段与非高峰时段的流量就很不一样；
						2.用户请求的多样性，例如不同国家的查询类型不同；2.
						3.负载流量的规模，基础架构设施的变化，例如服务的软件版本更新，微服务互相调用的 变化等等。
						由于难以在非生产环境中进行准确的容量测 试，我们经常需要转向真正的生产环境，使用实时而真实的客户流量负载来测试
				3.实施
						1.重定向用户数据
						2.挑战
								1.要使用实时流量以确保准确性。
								2.尽量不影响生产流量，这就需要实时的监测和反馈模块。
								3.可以定制重定向等行为规则；对不同的服务和不同的场景的测试，各种性能指标和阈值 都会不同。
								4.能自动终止测试，并把测试环境复位，尽量减少人工干预。
								5.支持基于日历和事件的自动触发和调度
						3.具体方案
							1.线上测试组件.png
							2.LiveRedliner:核心控制组件是整个系统的神经中枢，负责总体调度容量测试的过程。比如何时发起测试， 何时终止测试，何时需要增加更大的流量等等
							  TrafficRedirector:Redliner 是通过客户请求的属性（例如用户 ID、语言或帐户创建日 期），来决定是否对一个客户请求来进行重定向的
							  					重定向流量可以通过另外一个叫做“资源动态发现和负载均衡”的模块来 实现
							  					权重
							  PerfCollector
							  PerfAnalyzer：指标的值来确定 SUT 是否饱和

			6.怎么规划和控制数据库的复制延迟大小？ 
					1.背景:
							1.当今主要的互联网公司，往往部署一整套的数据处理系统。这一整套系 统通常由几个模块构成，包括数据事件的捕获、数据的存储、数据的复制传输、数据的读 取。

					2.分析
							1.按月/周/天统计信息并绘制图表，进行流量特征
					3.数据传输复制模块 -- 数据复制传输系统.png
							1.除了可以提供高扩展性，也可以提供数据的一致。需要一个整合而一致的数据视图。这样一个目 的，是可以通过传输复制实时数据库事件来得到的。
							2.数据的传输复制模块，其实是一个排队系统，是一个有无限缓冲的先进先出队 列。

					4.历史分析预估未来/sla调整 / 动态 自动化


			7.多任务环境中的Java性能问题，怎样才能不让程序互相干扰？   详见：《27：多任务环境中的Java性能问题，怎样才能不让程序互相干扰》
					1.现象及关联性图表分析
					2.解决方案


			8.网络数据传输慢，问题到底出在哪了？ 

					1.原因
							1.客户端应用程序的原因
							2.网络的原因
							3.服务器应用程序的原因
					2.如何对上述原因进行隔离判断
							1.数据传输涉及多个网络实体，包括两台机器（也就是发送者和接收者）和网络路由， 这与仅涉及一台机器的常见性能问题形成鲜明对比。
							2.诊断涉及多层信息，包括应用程序层和网络传输层。为了找出原因，工程师必须 检查各种数据，包括客户端日志、服务器日志、网络统计信息、CPU 使用情况等。这些检 查需要花费很多时间和精力，并且通常需要性能工程师的经验和专业知识。
							3.日志杂乱
							提出了一种当发生数据传输缓慢的问题时，可以自动 隔离原因的解决方案。毕竟，你只有找出了要对“数据传输慢”负责任的那一部分，才可以 进行后续分析工作，最终确定真正的问题。

					3.流控制功 能，可避免过载； 通过操作 系统来监测当前队列大小。 <=== netstat ss
					4.结论：
							1.网络传输慢原因.png
					5.解决：
							1.基于状态机的解决方案，它是一个针对 HTTP 数据传输问题的，完整 而具体的解决方案。
							  对上满原因.png进行自动监控



			9. 如何彻底发挥SSD的潜力？ 
					1.HDD 最大应用程序吞吐量为 142 个查询 / 秒  ==>  SSD  20,000 QPS
						使用多个并发线程来执行 I/O。这利用了 SSD 的内部并 行性。
						在这个系统中，多个 I/O 线程对 HDD 是毫无益处的。因为 HDD 只 有一个磁头，所以用多个 I/O 线程，并不能提高旧系统的吞吐量

					2.更高效的存储 I/O。
						SSD 上的最小内部 I/O 单元是一页，比如 4KB 大小,必须在页面级进
						ssd寿命:	
								1.SSD 大小
								2.最大擦除周期数
								3.写入放大系数
								4.应用程序写入速
						1.文件系统(数据库系统)		 Ext4 和 Btrfs		基本思想是采用日志结构的数据布局（相对于 B 树或 Htree），来容纳 SSD 的“复制 - 修改 - 写入”属性，比如 NVFS（非易失性文件系统）、FFS / JFFS2 和 F2FS。
					3.数据基础架构
						计算机上的本地磁盘或另一 台计算机上的内存？。这两个来源哪个更快更高效呢？
							Memcached  传统的本地 HDD 访问延迟，大约是好几个毫秒；而远程内存访问的延迟，包 括了 RAM 访问延迟和网络传输延迟，也仅处于微秒级
							而 SSD 的出现，使用 SSD 作为存储设备后，本地 SSD 变得比远程内存访问更为高效。
							SSD 的 I/O 延迟降低到了微秒级，而 I/O 带宽可是比 HDD 高一个数量级

					4.应用层
						1.应用层对ssd友好设计 
							1.数据结构 
									1.避免就地更新优化
										1.传统的 HDD 的寻址延迟很大，因此，使用 HDD 的应用程序通常会进行各种优化，以执行 不需要寻址的就地更新，比如只在一个文件后面写入。
										  不过在设计与 SSD 配合使用的应用程序时，这些考虑就没什么意义了。
										2.SSD 上的随机更新，就不会引起读取和修改步骤（即仅仅“写入”），因此速 度更快。
									2.区分热、冷数据
										1.将 SSD 用作存储设备时，应将热数据与冷数据分开。以不同级 别或不同方式来进行分隔。例如，把它们存在不同的文件、文件的不同部分或不同的表格 里。
									3.采取紧凑的数据结构
										1.在 SSD 的世界中，最小的更新单位是页面（4KB），因此，即使是一个字节的更新，也将 导致至少 4KB SSD 写入。由于写入放大的效果，实际写入 SSD 的字节可能远大于 4KB。 


							2.io处理
									1.避免长而繁重的持续写入
										1.由于后台 GC 是异步发生的（即非阻塞），因此它不会影响应用程序的 I/O 延 迟。但是，如果块的请求速率超过了 GC 速率，并且后台 GC 无法跟上，则将触发前台 GC。
									2.避免 SSD 存储太满
										1.SSD 磁盘存储的满存程度，会影响写入放大系数和 GC 导致的写入性能。在 GC 期间，需 要擦除块以创建空闲块。擦除块前，需要移动并保留有效数据才能获得空闲块。有时为了获 得一个空闲块，需要压缩好几个存储块。而每个空闲块的生产需要压缩的块数，取决于磁盘 的空间使用率。

							3.线程使用
									1.使用多个线程执行小的 I/O
										1.SSD 内部大量使用了并行的设计，这种并行表现在多个层面上。一个 I/O 线程无法充分利 用这些并行性，会导致访问时间更长。而使用多个线程，就可以充分利用 SSD 内部的并行 性了。
										2.这里的“小”IO，到底有多小
											只要是不能充分利用内部并行性的任何 I/O 大小，都被视为“小”。例如，SSD 页面大小为 4KB，内部并行度为 16，则阈值应约为 64KB
									2.使用较少的线程来执行大 I/O
										1.对于大型 I/O，SSD 内部已经充分优化使用了 SSD 的内部并行性，因此，应使用更少的线 程（即小于四个）以实现最大的 I/O 吞吐量。从吞吐量的角度来看，用太多线程不会有太 大益处。更重要的是，使用太多线程可能导致线程之间的资源竞争，以及诸如 OS 级的预读 和回写之类的后台活动。
