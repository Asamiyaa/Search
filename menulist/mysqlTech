DB技术本质：在一致性的基础上(事务、断电、并发、因为缓存造成的不一致),通过算法(大数据量写入、搜索)、内存、硬件(顺序独写)加速。充分利用硬件的能力


mysql
			1.一条sql的执行：
				SQL语句执行深入讲解（MySQL架构总览->查询执行流程->SQL解析顺序）																https://www.jb51.net/article/155350.htm\
					1.执行器：操作引擎，返回结果
					2.存储引擎：存储数据，提供读写接口

			2.长连接 vs 短连接
					数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。
					短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。
					建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。
				   但如果一直倡长连接可能导致oom,so :
				   			1.定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
				   			2.如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。


			3.关闭db默认缓存 -- 8.0之后已删除
					1.查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。导致命中率低。
					  除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。


			====> 从连接建立 - 权限校验 - 查询缓存命中 - 分析器 - 优化器 - 执行器      其实这些思考下来就是自己写代码的时候的  校验-缓存-执行 .存储引擎就是封装了io流读写+自己各个引擎特性

			4.更新操作涉及redoLog(重做)/binlog(归档)
					0.只有更新操作才会记录两日志
					1.为什么两个日志对比
							1.性能看，redolog：MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。(酒馆赊账) == 顺序写、组提交.优化
							2.如果未等打烊，粉版写满了(InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写)
							  write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。
							====> 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe
							3.binlog: Server 层，它主要做的是MySQL 功能层面的事情； Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。
							4.两个日志的对比
								1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
								2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
								3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

							5.事务id对应二者


			5.两阶段提交 - 让数据恢复到半月内任意秒  === 事务概念
					1.checkpoint ： redoLog中的位置(要清理缓存的)。 write pos就是写入db的位置

					update流程：
								1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
								2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
								3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
								4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
								5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。更改最终状态 。
					db恢复流程： --- 必须现有redo后有binlog ; 恢复库其实根本在binlog记录了操作过程
								1. 找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
								2. 从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

								1.备份机制取决于业务重要性和存储资源权衡，考虑最短恢复，肯定是越短越好

							====> 如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。
					保证主从一致：常见的做法也是用全量备份加上应用binlog 来实现的
					redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致

			6.两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。

			7.隔离级别 -  创建对应视图
					1. ACID: 原子性、一致性、隔离性、持久性
					2. 你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：
							读未提交是指，	一个事务还没提交时，它做的变更就能被别的事务看到。
							读提交是指，		一个事务提交之后，它做的变更才会被其他事务看到。		oracle 默认
							可重复读是指，	一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。  mysql 默认
							串行化			顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行



			8.长事务 - 尽量不要使用  ==
					1.事务可能访问任意一段数据，而数据库保留对应的回滚记录，占用空间 - checkpoint回滚点
					2.占用锁资源


							导读：MySQL运维中长事务和锁等待排查  ： https://dbalife.info/2018/06/28/MySQL%E8%BF%90%E7%BB%B4%E4%B8%AD%E9%95%BF%E4%BA%8B%E5%8A%A1%E5%92%8C%E9%94%81%E7%AD%89%E5%BE%85%E6%8E%92%E6%9F%A5/	
							查询长事务：查看长事务  ： select t.*,to_seconds(now())-to_seconds(t.trx_started) idle_time from INFORMATION_SCHEMA.INNODB_TRX t;
									   获得长事务id   :  select t.*,to_seconds(now())-to_seconds(t.trx_started) idle_time ,concat('kill ',trx_mysql_thread_id,';') kill_sql from INFORMATION_SCHEMA.INNODB_TRX t ； 
									   获取长事务用户  :  select * from information_schema.INNODB_TRX i, information_schema.processlist p where i.trx_mysql_thread_id=p.id and p.time > 60;
									   查看长事务对应sql : SELECT ps.id 'PROCESS ID',ps.user,ps.host, esh.EVENT_ID, trx.trx_started,esh.event_name 'EVENT NAME',esh.sql_text 'SQL' ,ps.time from performance_schema.events_statements_history esh   join performance_schema.threads th on esh.thread_id = th.thread_id       join information_schema.processlist ps on ps.id = th.processlist_id left join information_schema.innodb_trx trx on trx.trx_mysql_thread_id = ps.id      where ps.time > 60 AND trx.trx_id is not null and ps.USER != 'SYSTEM_USER'  order by   esh.EVENT_ID;
					分类：
						 读长事务
								比如研发同事连接查询机（从库）查询，没有启用autocommit, 执行了一个查询SQL，没有commit(一般执行查询都不会再执行commit)，连接就这样长时间挂起。这就造成了一个读长事务。这个事务持有一个share_read DML锁，它会影响对该表的DDL操作。如果这时DBA在主库对这个表执行了DDL操作，这个DDL操作复制到从库的时候，会因等待MDL锁而无法执行。这会造成从库复制大量延迟！

								还有一种读长事务的情况就是研发人员执行了一个复杂的统计类SQL，这个SQL执行完本身就耗时很长，这也会造成长时间占用DML锁，即使启用了autocommit也没用。而且还有可能大量数据文件排序造成磁盘空间耗尽。

								更有甚者，程序的连接执行了查询，没有commit,而程序用的是连接池，连接又不关闭。这个问题就很严重了。

						 写长事务
								写长事务比较好理解，就是批量更新、插入，造成事务长时间执行。还有就是事务本身逻辑比较复杂，存在锁竞争、锁等待。锁等待都会有超时，超时后应用端应该回滚或者重试。

								面对复杂的应用场景，DBA要以不变应万变，这个法宝就是监控。监控的目的是即时发现，发现了就能即时处理。对于读长事务，一旦超过一定阈值（比如10m）可立马kill,对于写操作则不能这么任性，需要结合业务来分析。

								INFORMATION_SCHEMA.INNODB_TRX表中包含了当前innodb内部正在运行的事务信息，包括只读事务。这个表中给出了事务的开始时间，我们可以稍加运算即可得到事务的运行时间。



				查询锁等待：锁等待与长事务由密切关系，不管哪种长事务都会造成MDL锁等待。 
							5.6之前：SELECT r.trx_state wating_trx_state, r.trx_id waiting_trx_id, r.trx_mysql_thread_Id waiting_thread,r.trx_query waiting_query,b.trx_state blocking_trx_state,b.trx_id blocking_trx_id,b.trx_mysql_thread_id blocking_thread,b.trx_query blocking_query FROM information_schema.innodb_lock_waits w INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id

							5.6之后： SELECT waiting_trx_id, waiting_pid, waiting_query, blocking_trx_id,blocking_pid,blocking_query FROM sys.innodb_lock_waits;


				如何避免长事务：

						首先，从应用开发端来看：
								1. 确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。
								2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。
								3. 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）
						其次，从数据库端来看：
								1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
								2. Percona 的 pt-kill 这个工具不错，推荐使用；
								3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
								4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。




			9.索引：
					0.每一张表其实就是多个个B+树，树结点的key值就是某一行的主键，value是该行的其他数据。新建索引就是新增一个B+树，查询不走索引就是遍历主B+树。非主键索引关联主键索引，主键索引的值就是整行数据值

					1.实现方式
							1.hash表  
									优点：增加新的 User 时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。
									缺点：如果你现在要找身份证号在 [ID_card_X, ID_card_Y] 这个区间的所有用户，就必须全部扫描一遍了。
									哈希表这种结构适用于只有等值查询的场景 
							2.有序数组
									优点：仅仅看查询效率，有序数组就是最好的数据结构。 更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。
									缺点：更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。
									有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。
							3.树
									优点：查询更新都是O(log(n))
									缺点：

								分类:
									二叉树
									多叉树：= B+树  为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。
											以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了

										问题：N叉树的N由什么决定？
											1.5.6以后可以通过page大小来间接控制
											2.计算方法、前缀索引

									红黑树
									LSM树

					2.主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。
					  非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。
					  没有主键的表，innodb默认rowId主键


					3.索引维护
							1.旋转
							2.追加 - 如果中间值则需要挪动后面数据 - 如果改页满了则页分裂(性能和利用率降低)(页合并)

							手动创建、删除顺序：
								1.
								2.

					4.从索引维护角度分析为什么使用自增主键  === 每个非主键索引的叶子节点上都是主键的值
							1.递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。
							2.由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。
							从性能和存储空间方面考量，自增主键往往是更合理的选择

							适合业务主键：
								1. 只有一个索引；2. 该索引必须是唯一索引。你一定看出来了，这就是典型的 KV    ===>为什么用树的思维去考虑呢？KV不一般都是hash散列吗？

								由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。
								这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。
					5.覆盖索引
							1. select ID from T where k between 3 and 5，查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。无需回表

					6.复合(联合索引)索引 来满足覆盖索引
							1.有一个 ‘ 高频请求 ’，要根据市民的身份证号查询他的姓名，这个身份证号和姓名联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

							====>  当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。

							2.最左前缀原则
								0.不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。意思是：查询不必须全部使用name/age两个条件，对于符合索引之只要按照name进行条件查询就可以匹配到最左前缀。使用索引
								  这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。
								1.索引项是按照索引定义里面出现的字段顺序排序的。比如(name,age) 则对应索引就是（zhangsan,23）...
								2.由索引组织的数据是 ‘ 有序 ’的。 比如 先按name排，再按age排，所以就是zhang后面都是zhang开头....匹配后，往后遍历直到不满足 。 
								3.为了最左前缀，在建立联合索引的时候，如何安排索引内的字段顺序。
							3.索引的复用
								1.高频请求创建 (身份证号，姓名）这个联合索引，并用这个索引支持 “ 根据身份证号查询地址 ”的需求==最左前缀 + 覆盖索引。
								2.(a,b)符合索引满足条件查询 a ； a+b ;两种 （索引：zhangsan,10 ; zhangsan,20==> 
										那么当我们使用条件where name = 'zhangsan' and age = 10这个时候是完全从左向右匹配的，所以符合索引完全使用了。 
										但当我们使用where name like "zhang%" and age = 10 这个时候由于中断，所以只能匹配到索引的zhang..后面的san,age都没有匹配到。这时age其实就没有使用到联合索引的好处。--索引下推
								） ，单独使用b查询则不能使用联合索引。想要使用则需要再建b索引
								3.所以使用联合索引可以适应更多的场景和复用，减少占用
							 4.索引下推
							 	1.对联合索引中的后半部分没有使用到的，先进行过滤，最后再回表

				10.锁
					1.并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构
					2.全局锁
						1.全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。  
							Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

							VS 用 set global readonly=true 
								一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
								二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

						2.对于支持事务的引擎， mysqldump。当 mysqldump 使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的
							MVCC:https://juejin.cn/post/6844903778026536968

					4.表锁
						1.一种是表锁，一种是元数据锁（meta data lock，MDL) mdl是默认添加的每个sql
						2.读锁 - 写锁 -由于读锁未释放，写锁获取阻塞，导致后面的读写都会阻塞。如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session 再请求的话，这个库的线程很快就会爆满。

					5.如何给表加字段
						1.首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。
						如果你要做DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDLNOWAIT/WAIT n 这个语法。

						online ddl

					6.行锁
						1.MyISAM 引擎就不支持行锁。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。
						2.两阶段锁协议
							在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。
						3.死锁
						   解决方案
							1.直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout 来设置。---- 设置时间长短容易误伤
							2.发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 --- 使用但需要遍历判断所有的依赖是否死锁性能

						   解决方案：
							1.头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
							
							2.控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前 ‘ 排队 ’ 。这样在 InnoDB 内部就不会有大量的死锁检测工作了。

							3.考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

							4.批次执行，减少冲突操作 - 通过queue来暂存数据

						4.锁竞争：
							如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到
								1.直接执行 delete from T limit 10000;
								2.在一个连接中循环执行 20 次 delete from T limit 500;
								3.在 20 个连接中同时执行 delete from T limit 500。

								答案：	--- 其它客户端 
								1.事务相对较长，则占用锁的时间较长，会导致其他客户端等待资源时间较长。且大事务还会导致主从延迟
								2.串行化执行，将相对长的事务分成多次相对短的事务，则每次事务占用锁的时间相对较短，其他客户端在等待相应资源的时间也较短。这样的操作，同时也意味着将资源分片使用（每次执行使用不同片段的资源），可以提高并发性。
								3.人为自己制造锁竞争，加剧并发量。

					
					11.	事务之间既然是隔离的，事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。
						问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候	？ 锁和事务的关系



问题：矛盾点：
				1.可重复读隔离级别，事务 T 启 动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数 据，事务 T 看到的仍然跟在启动时看到的一样
				2.一个事务要更新一行，如果刚好有 另外一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。
				
				begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start	transaction with consistent snapshot 这个命令。

				是默认 autocommit=1。


			  读锁？写锁？两个update事务时等待，一个读一个写则遵循1

		事务和锁关系：
				1.两个视图
					1.它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结 果。创建视图的语法是 create view … ，而它的查询方法与表一样。
					2.InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用 于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离 级别的实现
						
						1.基于整库---InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒 级创建快照”的能力。
									1.每个事务有一个唯一transaction id,递增申请。和每行变化的事务id进行比对，判断是否要找‘上一个’版本
									2.每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本， 并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。 。。excel:行+列(rowid)
									3.同时，旧的数据 版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。只记录update的版本和undolog而不是所有的事务操作都记录
										单向链表定位 <--- undoLog实现 V1、V2、V3 并不是物理上真实存 在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。
						2.如何定义快照
									1.一个事务启动的时候，能够看到所有已经提交的事务结果。但是之 后，这个事务执行期间，其他事务的更新对它不可见 。 如果“上一个版本”也不可见，那就得继续往前找。 
									2.如果是这个事务自己更新 的数据，它自己还是要认的。
									3. InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正 在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。
										a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
									    b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 ？？？ 

						3.读写关系
									1.读默认不加锁  所以当前镜像  如果加了了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。
 lock in share mode  / for update  则会读取最新事务提交的数据

 									2.对于update默认先读后写，读：当前读，读最新数据			<-- 类似 读提交
 									3.如果update操作时，有其他事务未提交改行锁，造成锁等待


 						4.对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
						  对于读提交，查询只承认在语句启动前就已经提交完成的数据；
						  对于当前读，查询该行数据最新版本数据；

		普通索引 vs 唯一索引

				1.id_card是否做主键
					1.不建议，因为id_card较长，主键作为其他索引的引用，消耗太大了
					2.解决方案：倒排id_card  reverse(id_card)并使用前缀   ， 注意索引不是一定能唯一定位到一行(唯一索引)，对于普通索引只能定位范围，在轮询查找对比判断

				2.普通索引 vs 唯一索引 性能对比
						1.查询
								1.对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直 到碰到第一个不满足 k=5 条件的记录。
								2.对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止 继续检索。
								==> 差距不大
								虽然存在db读取页的形式，多一次查询可能正好跨页导致加载，但每页近千个key,所以出现这种概率低
						2.更新	
								1.如果数据在内存中，则直接更新
								2.这个数据页还没有在内 存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了
								3.定期 merge。在数据库正常关闭 （shutdown）的过程中，也会执行 merge 操作 <-- hook


								唯一索引：判断是否违反唯一性约束，所以使用了内存，不使用change buffer 
								普通索引：change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大 小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的 时候，表示 	change buffer 的大小最多只能占用 buffer pool 的 50%。

				3.是否所有的普通索引都走change buffer呢？ -- 适用场景
							no 
								1.对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。
								2.一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新 先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。 这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代

				4.索引选择和实践
						1.在不使用db唯一约束情况下，优先使用普通索引，更好的update性能
						2.普通索引和 change buffer 的配合使用，对于数据量大的表的 更新优化还是很明显的。
						3.在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。
						4.“历史数据”的库，
						5.归档数据已经是确保没有唯一键冲突 了

				5.change buffer 和 redo log
						1. Page 1 在内存中，直接更新内存； 
						2. Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入 一行”这个信息 
						3. 将上述两个动作记入 redo log 中（图中 3 和 4）。

					二者不同：二者维度不同，redolog是为了记录所有操作过程，不管是否读取磁盘。
							 change buffer
							 做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是 写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写 的。

				6.有了change buffer如何保证一致性呢？
						1. 读 Page 1 的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL 之后 如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以 返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但 是这里直接从内存返回结果，结果是正确的。
						
						2. 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里 面的操作日志，生成一个正确的版本并返回结果。可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存。
						
		所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的 是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘 的 IO 消耗。


				问题：如果断电丢失change buffer ,如何保证一致性？
						会导致change buffer丢失，会导致本次未完成的操作数据丢失，但不会导致已完成操作的 数据丢失。 1.change buffer中分两部分，一部分是本次写入未写完的，一部分是已经写入完成的。 2.针对未写完的，此部分操作，还未写入redo log，因此事务还未提交，所以没影响。 2.针对，已经写完成的，可以通过redo log来进行恢复。 … 

						1、changebuffer跟普通数据页一样也是存在磁盘里，区别在于changebuffer是在共享表 空间ibdata1里 2、redolog有两种，一种记录普通数据页的改动，一种记录changebuffer的改动 
						3、只要内存里脏页（innodb buffer pool）里的数据发生了变化，就一定会记录2中前

				问题：有点疑惑: 主键id也是唯一索引吧? 那我们的新增操作如何利用 change buffer呢?
							作者回复: 所以主键索引用不上，都是对于那些二级索引的才有效。  
	 
							一个insert语句要操作所有索引的嘛，收益在二级索引

							insert的时候，写主键是肯定不能用change buffer了，但是同时也会要写其它索引，而其它索引 中的“非唯一索引”是可以用的这个机制的； 

	优化器导致索引失效
			1. 现场准备：批量插入 - 执行计划 -查看扫描行数 - 执行时间 - 设置慢查询set long_query_time=0 - 打印执行信息
			2. MySQL 选错索引依据：扫描的行数越少,是否使用临时表、是否排序等
					select * from t  where a between 10000 and 20000; 使用索引  -- 扫描:10001
					开启事务A ; 事务B :delete table ; 重新插入(由于事务A导致其实没有真正删除，每行有两个版本)
					select * from t  where a between 10000 and 20000; 没有使用索引		-- 扫描:全量
					select * from t force index(a) where a between 10000 and 20000; 强制使用索引 		-- 扫描:3w多
		问题：上面的查询语句中，mysql优化器如何判断错了行数?
				1.索引的“区分度 : 一个索引上不同的值越多，这个索引的区分度 就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说， 这个基数越大，索引的区分度越好。
				2.到精确的结果，但是代 价太高了，所以只能选择“采样统计”。
				3.InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个 平均值，然后乘以这个索引的页面数，就得到了这个索引的基
				4.当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。
				5.错误一：它认为使用索引 b 可以避免排序（b 本身是索引，已 经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行 数多，也判定为代价更小。
				  错误二：总体上因为综合了多个指标，所以导致最终没有走向我们期望的索引  <-== MySQL 优化器 bug
				  错误三：为什么删除索引数据快，建立却慢？
				  		删除的时候是标记删除，所以很快。 建索引是要扫描数据和真正生成索引树，是会慢些。
				  		所以2中的例子可能由于真正的索引并没有删除，不是真实态导致选择索引有误

				6.解决
				1.analyze table t 修正表  <== 发现 explain 的结果预估的 rows 值跟实际情况差距比较大，可以采 用这个方法来处理。
				2. select * from t force index(a) where a between 10000 and 20000 == 不推荐
				3. 引导选择：“order by b,a limit 1 解决错误一让mysql认为b,a都要排序，所以行数就作为重要依据。不具有通用性
				4.以新建一个更合适的索引，来提供给优化器做选择
				5.删掉误用的索引。

	如何给字符串加索引
				1.前缀索引，减少占用 。  alter table SUser add index index2(email(6)); 邮箱都是xxxxqq.com
				2.效果 -- 索引区分度评估 -- 评估占用和扫描(因为普通前缀索引会造成想相同索引对应不同数据，需要找到索引后
				    到主键到数据进行遍历判断) -- 只有唯一索引才会找到直接拿出，而不用遍历

				 		1.select count(distinct email) as L from SUser;
				 		2.select   count(distinct left(email,4)）as L4,     <=== 函数的使用
				 				   count(distinct left(email,5)）as L5,  
				 				   count(distinct left(email,6)）as L6,  
				 				   count(distinct left(email,7)）as L7,
				 			 from SUser;
				 		3.Ln / L >= 0.95
				 3.前缀索引会因为前缀影响到覆盖索引，如果
				 select id,email from SUser where email='zhangssxyz@xxx.com'  <---- 不使用前缀。可以不回表
				 select id,name,email from SUser where email='zhangssxyz@xxx.com' <---- 使用，因为都要回表 

				 4.为了增加区分度又减少占用
				 		1.倒序存储(身份证)  select field_list from t where id_card = reverse('input_id_card_string')
				 		2.使用 hash 字段。你可以在表上再创建一个整数字段，来保存身份证的校验 码，同时在这个字段上创建索引
				 				 alter table t add id_card_crc int unsigned, add index(id_card_crc);
				 				 由于 校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相 同的，所以你的查询语句 where 部分要判断 id_card 的值是否精确相同。

				 	==> 对比：
				 		1.都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符 串的方式排序的，已经没有办法利用索引方式查出身份证号码在 [ID_X, ID_Y] 的所有市民了。
				 		同样地，hash 字段的方式也只能支持等值查询。
				 				1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是 不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。 
				 				2. 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来 看的话，reverse 函数额外消耗的 CPU 资源会更小些。 
				 				3. 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来 的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。 而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。

	mysql抖动
					1.当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写 入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。，都在内存

					2.引发数据库的 flush 场景
							1.粉板满了，记不下了；危险 ，因为此时不能执行update.，这时候更新数会跌为 0
							2(通常).系统内存不足 新的内存页，而内存不够用的时候，就要淘   汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到 磁盘。 
									1. innodb_io_capacity = 磁盘的 IOPS；充分利用磁盘
									2. 不能全力刷脏页，因为还要处理用户请求 -- 控制策略
											1.一个是脏页比例，参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75
											2.一个是 redo log 写 盘速度。  SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。避免连坐刷周边脏页。机械盘的话可以考虑
											MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 


							3.生意不忙的时候，或者打烊之后
							4.是 MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度 会很快。





				数据库是：数据结构、算法、一致性、性能、取舍多角度的解决方案
				

				问题
							1、什么情况下创建索引才有意义？有哪些限制？比如字段长度
									有这个索引带来的查询收益，大于维护索引的代价，就该建😄对于可能变成大表的表，实际上如果不建索引会导致全表扫描，这个索引就是必须的
							2、如何查看索引占用多少空间？
									可以估算出来的，根据表的行数和索引的定义。
							3、查看索引数的结构，比如多少个层，多少节点？
									跟2一样。 如果要精确的，就要解数据文件，这个工具可以看看https://github.com/jeremycole/innodb_diagrams
							4、如何查看索引的利用率。比如我创建了一个索引，是否可以有记录这个索引被调用了
									performance_schema.table_io_waits_summary_by_index_usage能看到一些信息
							5、“N叉树”的N值在MySQL中是可以被人工调整吗？
									1， 通过改变key值来调整
											N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。如果B+树中页大小为16K，那么一个页就可以存储1024个索引，此时N就等于1024。我们通过改变Key的大小，就可以改变N的值
									2， 改变页的大小
											页越大，一页存放的索引就越多，N就越大。

										数据页调整后，如果数据页太小层数会太深，数据页太大，加载到内存的时间和单个数据页查询时间会提高，需要达到平衡才行。
							6、一个innoDB引擎的表，数据量非常大，根据二级索引搜索会比主键搜索快，文章阐述的原因是主键索引和数据行在一起，非常大搜索慢，我的疑惑是：通过普通索引找到主键ID后，同样要跑一边主键索引？
									1.覆盖索引。
							7.什么情况下会重建索引
									1.索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。否则就造成了浪费和性能下降


							8、如果你要重建索引 k，你的两个 SQL 语句可以这么写：

									alter table T drop index k;
									alter table T add index(k);
									如果你要重建主键索引，也可以这么写：
									alter table T drop primary key;
									alter table T add primary key(id);

									重建索引 k 的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做了。这两个语句，你可以用这个语句代替 ： alter table T engine=InnoDB 

									所以说索引的构建是在主键的基础上加强的


							9、只要用户定义的索引字段中包含了主键中的字段，那么这个字段就不会再被InnoDB自动加到索引中了，如果用户的索引字段中没有完全包含主键字段，InnoDB就会把剩下的主键字段加到索引末尾。
	 

							10、innodb行级锁是通过锁索引记录实现的。如果update的列没建索引，即使只update一条记录也会锁定整张表吗？比如update t set t.name='abc' where t.name='cde'; name字段无索引

										是的。但是你可以再往前考虑一下，如果是 你的update 语句后面加个limit 1, 会怎么锁？
										1.可以自己实践一下,当加上limit1之后 更新语句的执行流程是先去查询再去更新,也就是查询sql为 select * from t where name = "abc" limit 1 for  update,相当于扫描主键索引找到第一个满足name="abc"的条件为止,此时锁的区间为(负无穷,当前行的id],如果在这个id之后的更新和插入时都不会锁住的,在这个id之前的更新和插入会阻塞,之后则不会阻塞 **** 悲观锁 vs 乐观锁  

										2.如果不加limit 1的话,因为此时是整个主键索引全表扫描则整个表锁住了

										3.你说的回表的行锁,比如字段name有普通索引,在更新操作时普通索引会锁住的同时,如果更新操作需要回表的话对应的主键索引也会存在锁(主键索引锁临界锁会退化为行锁),普通索引(间隙锁和行锁)
						
							11.索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。
										rename +新建表

							12.where 过滤条件的顺序会影响sql的执行顺序吗?









		参考大神：https://dbalife.info/categories/MySQL/




		参数参考：
				innodb_flush_log_at_trx_commit   这个参数设置成1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
				sync_binlog 					 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。
				transaction-isolation 			 show variables like '%transaction-isolation%';  / show variables like '%tx_isolation%';
				set autocommit=1     			 通过显式语句的方式来启动事务，避免查询开启事务，需要commit来完成 并不会因为多一步交互 。 执行 commit work and 	chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。
				show variables like '%char%' ;	 注意事项：修改编码 只对“之后”创建的数据库生效，因此 我们建议 在mysql安装完毕后，第一时间 统一编码。
				show variables like '%storage_engine%' ;
				show variables like '%innodb_lock_wait_timeout%';  锁等待
				show variables like '%innodb_deadlock_detect%';    死锁监测，出现死锁则回滚部分事务使其解锁
				show engine innodb status 








问题：	
	0.sql对应的函数 - sql写法 <== sql部分
			update T set c=c+1 where id = 1;
			update T set replace(xxx,x222) where id = 1;
			update T set contact(c,‘xxxx’) where id = 1;



	1.binlog/ relog 
	2.慢日志
	3.行列转换
	4.分页
			1.offset和rows的正负带来的影响；
				当偏移量很大时效率是很低的，可以这么做：
				采用子查询的方式优化，在子查询里先从索引获取到最大id，然后倒序排，再取N行结果集
				采用INNER JOIN优化，JOIN子句里也优先从索引获取ID列表，然后直接关联查询获得最终结果
    5.存储引擎区别对比  memory引擎 vs redis
    6.优化器是怎么选择索引的，有没有可能选择错等等




